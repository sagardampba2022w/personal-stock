{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55e4bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21122 entries, 0 to 21121\n",
      "Data columns (total 28 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   Date                     21122 non-null  datetime64[ns]\n",
      " 1   Open                     21122 non-null  float64       \n",
      " 2   High                     21122 non-null  float64       \n",
      " 3   Low                      21122 non-null  float64       \n",
      " 4   Close                    21122 non-null  float64       \n",
      " 5   Adj Close                21122 non-null  float64       \n",
      " 6   Volume                   21122 non-null  float64       \n",
      " 7   Ticker                   21122 non-null  object        \n",
      " 8   Year                     21122 non-null  int32         \n",
      " 9   Month                    21122 non-null  int32         \n",
      " 10  Weekday                  21122 non-null  int32         \n",
      " 11  wom                      21122 non-null  int64         \n",
      " 12  month_wom                21122 non-null  object        \n",
      " 13  growth_1d                21120 non-null  float64       \n",
      " 14  growth_3d                21116 non-null  float64       \n",
      " 15  growth_7d                21108 non-null  float64       \n",
      " 16  growth_30d               21062 non-null  float64       \n",
      " 17  growth_90d               20942 non-null  float64       \n",
      " 18  growth_252d              20618 non-null  float64       \n",
      " 19  growth_365d              20392 non-null  float64       \n",
      " 20  growth_future_30d        21074 non-null  float64       \n",
      " 21  is_positive_future_30d   21122 non-null  int64         \n",
      " 22  SMA10                    21104 non-null  float64       \n",
      " 23  SMA20                    21084 non-null  float64       \n",
      " 24  growing_moving_average   21122 non-null  int64         \n",
      " 25  volatility               21062 non-null  float64       \n",
      " 26  Sharpe                   20618 non-null  float64       \n",
      " 27  high_minus_low_relative  21122 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(19), int32(3), int64(3), object(2)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def fetch_history_bulk(\n",
    "    tickers: List[str],\n",
    "    pause: float = 1.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches max‐history daily data for multiple tickers in one call.\n",
    "    \"\"\"\n",
    "    # This returns a nested DataFrame: columns=(ticker, field)\n",
    "    raw = yf.download(\n",
    "        tickers,\n",
    "        period=\"max\",\n",
    "        interval=\"1d\",\n",
    "        group_by=\"ticker\",\n",
    "        auto_adjust=False,\n",
    "        threads=True,\n",
    "        progress=False,\n",
    "    )\n",
    "    # Unstack into long form\n",
    "    frames = []\n",
    "    for ticker in tickers:\n",
    "        if ticker not in raw.columns.levels[0]:\n",
    "            logger.warning(\"No data for %s\", ticker)\n",
    "            continue\n",
    "\n",
    "        df = raw[ticker].copy()\n",
    "        df = df.rename_axis(\"Date\").reset_index()\n",
    "        df.loc[:, \"Ticker\"] = ticker\n",
    "        frames.append(df)\n",
    "\n",
    "        time.sleep(pause)  # to be gentle on the API\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.loc[:, \"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df.loc[:, \"Year\"]    = df[\"Date\"].dt.year\n",
    "    df.loc[:, \"Month\"]   = df[\"Date\"].dt.month\n",
    "    df.loc[:, \"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    df.loc[:, \"wom\"] = ((df[\"Date\"].dt.day - 1) // 7 + 1).astype(int)\n",
    "    df.loc[:, \"month_wom\"] = (\n",
    "        df[\"Date\"].dt.month_name() + \"_w\" + df[\"wom\"].astype(str)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_growth_features(\n",
    "    df: pd.DataFrame,\n",
    "    lookbacks: List[int] = [1, 3, 7, 30, 90, 252, 365],\n",
    "    horizons: List[int] = [30],\n",
    "    binarize_thresholds: Optional[Dict[int, float]] = None\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # 1) historical growth\n",
    "    for j in lookbacks:\n",
    "        df[f\"growth_{j}d\"] = df[\"Close\"] / df[\"Close\"].shift(j)\n",
    "\n",
    "    # 2) forward‐looking growth\n",
    "    for h in horizons:\n",
    "        df[f\"growth_future_{h}d\"] = df[\"Close\"].shift(-h) / df[\"Close\"]\n",
    "\n",
    "    # 3) optional binarization\n",
    "    if binarize_thresholds:\n",
    "        for h, thresh in binarize_thresholds.items():\n",
    "            if h not in horizons:\n",
    "                raise ValueError(f\"horizon {h} not in `horizons` list\")\n",
    "            col = f\"growth_future_{h}d\"\n",
    "            df[f\"is_positive_future_{h}d\"] = (df[col] > thresh).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_moving_averages(\n",
    "    df: pd.DataFrame,\n",
    "    windows: List[int] = [10, 20]\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for w in windows:\n",
    "        df[f\"SMA{w}\"] = df[\"Close\"].rolling(w).mean()\n",
    "    if len(windows) >= 2:\n",
    "        df.loc[:, \"growing_moving_average\"] = (\n",
    "            df[f\"SMA{windows[0]}\"] > df[f\"SMA{windows[1]}\"]\n",
    "        ).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_volatility_and_sharpe(\n",
    "    df: pd.DataFrame,\n",
    "    vol_window: int = 30,\n",
    "    risk_free: float = 0.045\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.loc[:, \"volatility\"] = (\n",
    "        df[\"Close\"].pct_change(fill_method=None).rolling(vol_window).std() * np.sqrt(252)\n",
    "    )\n",
    "    if \"growth_252d\" not in df.columns:\n",
    "        df.loc[:, \"growth_252d\"] = df[\"Close\"] / df[\"Close\"].shift(252)\n",
    "    df.loc[:, \"Sharpe\"] = (df[\"growth_252d\"] - risk_free) / df[\"volatility\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_price_range(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.loc[:, \"high_minus_low_relative\"] = (\n",
    "        (df[\"High\"] - df[\"Low\"]) / df[\"Close\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_stock_dataframe(\n",
    "    tickers: List[str],\n",
    "    lookbacks: List[int] = [1, 3, 7, 30, 90, 252, 365],\n",
    "    horizons: List[int] = [30],\n",
    "    binarize_thresholds: Optional[Dict[int, float]] = None,\n",
    "    ma_windows: List[int] = [10, 20],\n",
    "    vol_window: int = 30,\n",
    "    risk_free: float = 0.045,\n",
    "    pause: float = 1.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches history for `tickers`, applies all feature pipelines,\n",
    "    and returns a cleaned, sorted DataFrame.\n",
    "    \"\"\"\n",
    "    df_all = fetch_history_bulk(tickers, pause=pause)\n",
    "\n",
    "    df_all = (\n",
    "        df_all\n",
    "        .pipe(add_time_features)\n",
    "        .pipe(add_growth_features, lookbacks, horizons, binarize_thresholds)\n",
    "        .pipe(add_moving_averages, ma_windows)\n",
    "        .pipe(add_volatility_and_sharpe, vol_window, risk_free)\n",
    "        .pipe(add_price_range)\n",
    "    )\n",
    "\n",
    "    # drop rows with NaN in any of the raw future targets\n",
    "    future_cols = [f\"growth_future_{h}d\" for h in horizons]\n",
    "    df_all.dropna(subset=future_cols, inplace=True)\n",
    "\n",
    "    # final ordering & reset\n",
    "    df_all.sort_values([\"Ticker\", \"Date\"], inplace=True)\n",
    "    df_all.reset_index(drop=True, inplace=True)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def build_stock_dataframe(\n",
    "    tickers: List[str],\n",
    "    lookbacks: List[int] = [1, 3, 7, 30, 90, 252, 365],\n",
    "    horizons: List[int] = [30],\n",
    "    binarize_thresholds: Optional[Dict[int, float]] = None,\n",
    "    ma_windows: List[int] = [10, 20],\n",
    "    vol_window: int = 30,\n",
    "    risk_free: float = 0.045,\n",
    "    pause: float = 1.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches history for `tickers`, applies all feature pipelines,\n",
    "    and returns a cleaned, sorted DataFrame.\n",
    "    \"\"\"\n",
    "    df_all = fetch_history_bulk(tickers, pause=pause)\n",
    "\n",
    "    df_all = (\n",
    "        df_all\n",
    "        .pipe(add_time_features)\n",
    "        .pipe(add_growth_features, lookbacks, horizons, binarize_thresholds)\n",
    "        .pipe(add_moving_averages, ma_windows)\n",
    "        .pipe(add_volatility_and_sharpe, vol_window, risk_free)\n",
    "        .pipe(add_price_range)\n",
    "    )\n",
    "\n",
    "    # Only drop nulls from historical data, preserve recent data\n",
    "    future_cols = [f\"growth_future_{h}d\" for h in horizons]\n",
    "    if future_cols:\n",
    "        max_horizon = max(horizons)\n",
    "        cutoff_date = df_all['Date'].max() - pd.Timedelta(days=max_horizon + 5)\n",
    "        \n",
    "        # Keep recent data even with null future columns\n",
    "        recent_mask = df_all['Date'] > cutoff_date\n",
    "        df_all = pd.concat([\n",
    "            df_all[~recent_mask].dropna(subset=future_cols),  # Clean historical data\n",
    "            df_all[recent_mask]  # Preserve recent data\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    # final ordering & reset\n",
    "    df_all.sort_values([\"Ticker\", \"Date\"], inplace=True)\n",
    "    df_all.reset_index(drop=True, inplace=True)\n",
    "    return df_all\n",
    "\n",
    "# --- Usage Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = [\"AAPL\", \"MSFT\"]\n",
    "    stocks_df = build_stock_dataframe(\n",
    "        tickers,\n",
    "        lookbacks=[1,3,7,30,90,252,365],\n",
    "        horizons=[30],\n",
    "        binarize_thresholds={30: 1.0},\n",
    "        ma_windows=[10, 20],\n",
    "        vol_window=30,\n",
    "        risk_free=0.045,\n",
    "        pause=0.5\n",
    "    )\n",
    "    stocks_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9bdbbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_positive_future_30d\n",
       "1    60.0\n",
       "0    40.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stocks_df['is_positive_future_30d'].value_counts(normalize=True)*100).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26df03b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>growth_252d</th>\n",
       "      <th>growth_365d</th>\n",
       "      <th>growth_future_30d</th>\n",
       "      <th>is_positive_future_30d</th>\n",
       "      <th>SMA10</th>\n",
       "      <th>SMA20</th>\n",
       "      <th>growing_moving_average</th>\n",
       "      <th>volatility</th>\n",
       "      <th>Sharpe</th>\n",
       "      <th>high_minus_low_relative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21117</th>\n",
       "      <td>2025-06-27</td>\n",
       "      <td>497.549988</td>\n",
       "      <td>499.299988</td>\n",
       "      <td>493.029999</td>\n",
       "      <td>495.940002</td>\n",
       "      <td>495.940002</td>\n",
       "      <td>34539200.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099767</td>\n",
       "      <td>1.289395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>485.154999</td>\n",
       "      <td>476.696999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125808</td>\n",
       "      <td>8.383942</td>\n",
       "      <td>0.012643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21118</th>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>497.040009</td>\n",
       "      <td>500.760010</td>\n",
       "      <td>495.329987</td>\n",
       "      <td>497.410004</td>\n",
       "      <td>497.410004</td>\n",
       "      <td>28369000.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.100075</td>\n",
       "      <td>1.280434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>487.400000</td>\n",
       "      <td>478.549500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125560</td>\n",
       "      <td>8.402942</td>\n",
       "      <td>0.010917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21119</th>\n",
       "      <td>2025-07-01</td>\n",
       "      <td>496.470001</td>\n",
       "      <td>498.049988</td>\n",
       "      <td>490.980011</td>\n",
       "      <td>492.049988</td>\n",
       "      <td>492.049988</td>\n",
       "      <td>19945400.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.086563</td>\n",
       "      <td>1.260794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>488.690997</td>\n",
       "      <td>480.053499</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131887</td>\n",
       "      <td>7.897381</td>\n",
       "      <td>0.014368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21120</th>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>489.989990</td>\n",
       "      <td>493.500000</td>\n",
       "      <td>488.700012</td>\n",
       "      <td>491.089996</td>\n",
       "      <td>491.089996</td>\n",
       "      <td>16319600.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098758</td>\n",
       "      <td>1.260919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>489.995996</td>\n",
       "      <td>481.459499</td>\n",
       "      <td>1</td>\n",
       "      <td>0.130616</td>\n",
       "      <td>8.067574</td>\n",
       "      <td>0.009774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21121</th>\n",
       "      <td>2025-07-03</td>\n",
       "      <td>493.809998</td>\n",
       "      <td>500.130005</td>\n",
       "      <td>493.440002</td>\n",
       "      <td>498.839996</td>\n",
       "      <td>498.839996</td>\n",
       "      <td>13984800.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.092199</td>\n",
       "      <td>1.266509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>491.855997</td>\n",
       "      <td>483.207999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.135746</td>\n",
       "      <td>7.714419</td>\n",
       "      <td>0.013411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price       Date        Open        High         Low       Close   Adj Close  \\\n",
       "21117 2025-06-27  497.549988  499.299988  493.029999  495.940002  495.940002   \n",
       "21118 2025-06-30  497.040009  500.760010  495.329987  497.410004  497.410004   \n",
       "21119 2025-07-01  496.470001  498.049988  490.980011  492.049988  492.049988   \n",
       "21120 2025-07-02  489.989990  493.500000  488.700012  491.089996  491.089996   \n",
       "21121 2025-07-03  493.809998  500.130005  493.440002  498.839996  498.839996   \n",
       "\n",
       "Price      Volume Ticker  Year  Month  ...  growth_252d  growth_365d  \\\n",
       "21117  34539200.0   MSFT  2025      6  ...     1.099767     1.289395   \n",
       "21118  28369000.0   MSFT  2025      6  ...     1.100075     1.280434   \n",
       "21119  19945400.0   MSFT  2025      7  ...     1.086563     1.260794   \n",
       "21120  16319600.0   MSFT  2025      7  ...     1.098758     1.260919   \n",
       "21121  13984800.0   MSFT  2025      7  ...     1.092199     1.266509   \n",
       "\n",
       "Price growth_future_30d  is_positive_future_30d       SMA10       SMA20  \\\n",
       "21117               NaN                       0  485.154999  476.696999   \n",
       "21118               NaN                       0  487.400000  478.549500   \n",
       "21119               NaN                       0  488.690997  480.053499   \n",
       "21120               NaN                       0  489.995996  481.459499   \n",
       "21121               NaN                       0  491.855997  483.207999   \n",
       "\n",
       "Price  growing_moving_average  volatility    Sharpe  high_minus_low_relative  \n",
       "21117                       1    0.125808  8.383942                 0.012643  \n",
       "21118                       1    0.125560  8.402942                 0.010917  \n",
       "21119                       1    0.131887  7.897381                 0.014368  \n",
       "21120                       1    0.130616  8.067574                 0.009774  \n",
       "21121                       1    0.135746  7.714419                 0.013411  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d3fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers with non-increasing dates: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# After your global sort…\n",
    "stocks_df.sort_values([\"Ticker\",\"Date\"], inplace=True)\n",
    "\n",
    "# Check each ticker’s date sequence\n",
    "bad = (\n",
    "    stocks_df\n",
    "    .groupby(\"Ticker\")[\"Date\"]\n",
    "    .apply(lambda idx: not idx.is_monotonic_increasing)\n",
    "    .loc[lambda s: s]\n",
    ")\n",
    "print(\"Tickers with non-increasing dates:\", bad.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedcc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb613938",
   "metadata": {},
   "source": [
    "## Momentum indicators\n",
    "https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/momentum_indicators.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ccd5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talib_get_momentum_indicators_for_one_ticker(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  # ADX - Average Directional Movement Index\n",
    "  talib_momentum_adx = talib.ADX(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # ADXR - Average Directional Movement Index Rating\n",
    "  talib_momentum_adxr = talib.ADXR(df.High.values, df.Low.values, df.Close.values, timeperiod=14 )\n",
    "  # APO - Absolute Price Oscillator\n",
    "  talib_momentum_apo = talib.APO(df.Close.values, fastperiod=12, slowperiod=26, matype=0 )\n",
    "  # AROON - Aroon\n",
    "  talib_momentum_aroon = talib.AROON(df.High.values, df.Low.values, timeperiod=14 )\n",
    "  # talib_momentum_aroon[0].size\n",
    "  # talib_momentum_aroon[1].size\n",
    "  # AROONOSC - Aroon Oscillator\n",
    "  talib_momentum_aroonosc = talib.AROONOSC(df.High.values, df.Low.values, timeperiod=14)\n",
    "  # BOP - Balance of Power\n",
    "  # https://school.stockcharts.com/doku.php?id=technical_indicators:balance_of_power\n",
    "     #calculate open prices as shifted closed prices from the prev day\n",
    "     # open = df.Last.shift(1)\n",
    "  talib_momentum_bop = talib.BOP(df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "  # CCI - Commodity Channel Index\n",
    "  talib_momentum_cci = talib.CCI(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # CMO - Chande Momentum Oscillator\n",
    "  talib_momentum_cmo = talib.CMO(df.Close.values, timeperiod=14)\n",
    "  # DX - Directional Movement Index\n",
    "  talib_momentum_dx = talib.DX(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # MACD - Moving Average Convergence/Divergence\n",
    "  talib_momentum_macd, talib_momentum_macdsignal, talib_momentum_macdhist = talib.MACD(df.Close.values, fastperiod=12, \\\n",
    "                                                                                       slowperiod=26, signalperiod=9)\n",
    "  # MACDEXT - MACD with controllable MA type\n",
    "  talib_momentum_macd_ext, talib_momentum_macdsignal_ext, talib_momentum_macdhist_ext = talib.MACDEXT(df.Close.values, \\\n",
    "                                                                                                    fastperiod=12, \\\n",
    "                                                                                                    fastmatype=0, \\\n",
    "                                                                                                    slowperiod=26, \\\n",
    "                                                                                                    slowmatype=0, \\\n",
    "                                                                                                    signalperiod=9, \\\n",
    "                                                                                                  signalmatype=0)\n",
    "  # MACDFIX - Moving Average Convergence/Divergence Fix 12/26\n",
    "  talib_momentum_macd_fix, talib_momentum_macdsignal_fix, talib_momentum_macdhist_fix = talib.MACDFIX(df.Close.values, \\\n",
    "                                                                                                      signalperiod=9)\n",
    "  # MFI - Money Flow Index\n",
    "  talib_momentum_mfi = talib.MFI(df.High.values, df.Low.values, df.Close.values, df.Volume.values, timeperiod=14)\n",
    "  # MINUS_DI - Minus Directional Indicator\n",
    "  talib_momentum_minus_di = talib.MINUS_DM(df.High.values, df.Low.values, timeperiod=14)\n",
    "  # MOM - Momentum\n",
    "  talib_momentum_mom = talib.MOM(df.Close.values, timeperiod=10)\n",
    "  # PLUS_DI - Plus Directional Indicator\n",
    "  talib_momentum_plus_di = talib.PLUS_DI(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "  # PLUS_DM - Plus Directional Movement\n",
    "  talib_momentum_plus_dm = talib.PLUS_DM(df.High.values, df.Low.values, timeperiod=14)\n",
    "  # PPO - Percentage Price Oscillator\n",
    "  talib_momentum_ppo = talib.PPO(df.Close.values, fastperiod=12, slowperiod=26, matype=0)\n",
    "  # ROC - Rate of change : ((price/prevPrice)-1)*100\n",
    "  talib_momentum_roc = talib.ROC(df.Close.values, timeperiod=10)\n",
    "  # ROCP - Rate of change Percentage: (price-prevPrice)/prevPrice\n",
    "  talib_momentum_rocp = talib.ROCP(df.Close.values, timeperiod=10)\n",
    "  # ROCR - Rate of change ratio: (price/prevPrice)\n",
    "  talib_momentum_rocr = talib.ROCR(df.Close.values, timeperiod=10)\n",
    "  # ROCR100 - Rate of change ratio 100 scale: (price/prevPrice)*100\n",
    "  talib_momentum_rocr100 = talib.ROCR100(df.Close.values, timeperiod=10)\n",
    "  # RSI - Relative Strength Index\n",
    "  talib_momentum_rsi = talib.RSI(df.Close.values, timeperiod=14)\n",
    "  # STOCH - Stochastic\n",
    "  talib_momentum_slowk, talib_momentum_slowd = talib.STOCH(df.High.values, df.Low.values, df.Close.values, \\\n",
    "                                                           fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "  # STOCHF - Stochastic Fast\n",
    "  talib_momentum_fastk, talib_momentum_fastd = talib.STOCHF(df.High.values, df.Low.values, df.Close.values, \\\n",
    "                                                            fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "  # STOCHRSI - Stochastic Relative Strength Index\n",
    "  talib_momentum_fastk_rsi, talib_momentum_fastd_rsi = talib.STOCHRSI(df.Close.values, timeperiod=14, \\\n",
    "                                                                      fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "  # TRIX - 1-day Rate-Of-Change (ROC) of a Triple Smooth EMA\n",
    "  talib_momentum_trix = talib.TRIX(df.Close.values, timeperiod=30)\n",
    "  # ULTOSC - Ultimate Oscillator\n",
    "  talib_momentum_ultosc = talib.ULTOSC(df.High.values, df.Low.values, df.Close.values, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "  # WILLR - Williams' %R\n",
    "  talib_momentum_willr = talib.WILLR(df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "\n",
    "  momentum_df =   pd.DataFrame(\n",
    "    {\n",
    "      # assume here multi-index <dateTime, ticker>\n",
    "      # 'datetime': df.index.get_level_values(0),\n",
    "      # 'ticker': df.index.get_level_values(1) ,\n",
    "\n",
    "      # old way with separate columns\n",
    "      'Date': df.Date.values,\n",
    "      'Ticker': df.Ticker,\n",
    "\n",
    "      'adx': talib_momentum_adx,\n",
    "      'adxr': talib_momentum_adxr,\n",
    "      'apo': talib_momentum_apo,\n",
    "      'aroon_1': talib_momentum_aroon[0] ,\n",
    "      'aroon_2': talib_momentum_aroon[1],\n",
    "      'aroonosc': talib_momentum_aroonosc,\n",
    "      'bop': talib_momentum_bop,\n",
    "      'cci': talib_momentum_cci,\n",
    "      'cmo': talib_momentum_cmo,\n",
    "      'dx': talib_momentum_dx,\n",
    "      'macd': talib_momentum_macd,\n",
    "      'macdsignal': talib_momentum_macdsignal,\n",
    "      'macdhist': talib_momentum_macdhist,\n",
    "      'macd_ext': talib_momentum_macd_ext,\n",
    "      'macdsignal_ext': talib_momentum_macdsignal_ext,\n",
    "      'macdhist_ext': talib_momentum_macdhist_ext,\n",
    "      'macd_fix': talib_momentum_macd_fix,\n",
    "      'macdsignal_fix': talib_momentum_macdsignal_fix,\n",
    "      'macdhist_fix': talib_momentum_macdhist_fix,\n",
    "      'mfi': talib_momentum_mfi,\n",
    "      'minus_di': talib_momentum_minus_di,\n",
    "      'mom': talib_momentum_mom,\n",
    "      'plus_di': talib_momentum_plus_di,\n",
    "      'dm': talib_momentum_plus_dm,\n",
    "      'ppo': talib_momentum_ppo,\n",
    "      'roc': talib_momentum_roc,\n",
    "      'rocp': talib_momentum_rocp,\n",
    "      'rocr': talib_momentum_rocr,\n",
    "      'rocr100': talib_momentum_rocr100,\n",
    "      'rsi': talib_momentum_rsi,\n",
    "      'slowk': talib_momentum_slowk,\n",
    "      'slowd': talib_momentum_slowd,\n",
    "      'fastk': talib_momentum_fastk,\n",
    "      'fastd': talib_momentum_fastd,\n",
    "      'fastk_rsi': talib_momentum_fastk_rsi,\n",
    "      'fastd_rsi': talib_momentum_fastd_rsi,\n",
    "      'trix': talib_momentum_trix,\n",
    "      'ultosc': talib_momentum_ultosc,\n",
    "      'willr': talib_momentum_willr,\n",
    "     }\n",
    "  )\n",
    "  return momentum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157041cc",
   "metadata": {},
   "source": [
    "## Volume, Volatility, Cycle, Price indicators\n",
    "https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/volume_indicators.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f96918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talib_get_volume_volatility_cycle_price_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # TA-Lib Volume indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/volume_indicators.md\n",
    "        # AD - Chaikin A/D Line\n",
    "        talib_ad = talib.AD(\n",
    "            df.High.values, df.Low.values, df.Close.values, df.Volume.values)\n",
    "        # ADOSC - Chaikin A/D Oscillator\n",
    "        talib_adosc = talib.ADOSC(\n",
    "            df.High.values, df.Low.values, df.Close.values, df.Volume.values, fastperiod=3, slowperiod=10)\n",
    "        # OBV - On Balance Volume\n",
    "        talib_obv = talib.OBV(\n",
    "            df.Close.values, df.Volume.values)\n",
    "\n",
    "        # TA-Lib Volatility indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/volatility_indicators.md\n",
    "        # ATR - Average True Range\n",
    "        talib_atr = talib.ATR(\n",
    "            df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "        # NATR - Normalized Average True Range\n",
    "        talib_natr = talib.NATR(\n",
    "            df.High.values, df.Low.values, df.Close.values, timeperiod=14)\n",
    "        # OBV - On Balance Volume\n",
    "        talib_obv = talib.OBV(\n",
    "            df.Close.values, df.Volume.values)\n",
    "\n",
    "        # TA-Lib Cycle Indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/cycle_indicators.md\n",
    "        # HT_DCPERIOD - Hilbert Transform - Dominant Cycle Period\n",
    "        talib_ht_dcperiod = talib.HT_DCPERIOD(df.Close.values)\n",
    "        # HT_DCPHASE - Hilbert Transform - Dominant Cycle Phase\n",
    "        talib_ht_dcphase = talib.HT_DCPHASE(df.Close.values)\n",
    "        # HT_PHASOR - Hilbert Transform - Phasor Components\n",
    "        talib_ht_phasor_inphase, talib_ht_phasor_quadrature = talib.HT_PHASOR(\n",
    "            df.Close.values)\n",
    "        # HT_SINE - Hilbert Transform - SineWave\n",
    "        talib_ht_sine_sine, talib_ht_sine_leadsine = talib.HT_SINE(\n",
    "            df.Close.values)\n",
    "        # HT_TRENDMODE - Hilbert Transform - Trend vs Cycle Mode\n",
    "        talib_ht_trendmode = talib.HT_TRENDMODE(df.Close.values)\n",
    "\n",
    "        # TA-Lib Price Transform Functions\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/price_transform.md\n",
    "        # AVGPRICE - Average Price\n",
    "        talib_avgprice = talib.AVGPRICE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # MEDPRICE - Median Price\n",
    "        talib_medprice = talib.MEDPRICE(df.High.values, df.Low.values)\n",
    "        # TYPPRICE - Typical Price\n",
    "        talib_typprice = talib.TYPPRICE(\n",
    "            df.High.values, df.Low.values, df.Close.values)\n",
    "        # WCLPRICE - Weighted Close Price\n",
    "        talib_wclprice = talib.WCLPRICE(\n",
    "            df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        volume_volatility_cycle_price_df = pd.DataFrame(\n",
    "            {'Date': df.Date.values,\n",
    "             'Ticker': df.Ticker,\n",
    "             # TA-Lib Volume indicators\n",
    "             'ad': talib_ad,\n",
    "             'adosc': talib_adosc,\n",
    "             'obv': talib_obv,\n",
    "             # TA-Lib Volatility indicators\n",
    "             'atr': talib_atr,\n",
    "             'natr': talib_natr,\n",
    "             'obv': talib_obv,\n",
    "             # TA-Lib Cycle Indicators\n",
    "             'ht_dcperiod': talib_ht_dcperiod,\n",
    "             'ht_dcphase': talib_ht_dcphase,\n",
    "             'ht_phasor_inphase': talib_ht_phasor_inphase,\n",
    "             'ht_phasor_quadrature': talib_ht_phasor_quadrature,\n",
    "             'ht_sine_sine': talib_ht_sine_sine,\n",
    "             'ht_sine_leadsine': talib_ht_sine_leadsine,\n",
    "             'ht_trendmod': talib_ht_trendmode,\n",
    "             # TA-Lib Price Transform Functions\n",
    "             'avgprice': talib_avgprice,\n",
    "             'medprice': talib_medprice,\n",
    "             'typprice': talib_typprice,\n",
    "             'wclprice': talib_wclprice,\n",
    "             }\n",
    "        )\n",
    "\n",
    "        # Need a proper date type\n",
    "        volume_volatility_cycle_price_df['Date'] = pd.to_datetime(\n",
    "            volume_volatility_cycle_price_df['Date'])\n",
    "\n",
    "        return volume_volatility_cycle_price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae1005",
   "metadata": {},
   "source": [
    "## Pattern indicators\n",
    "https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/pattern_recognition.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5089625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talib_get_pattern_recognition_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "# TA-Lib Pattern Recognition indicators\n",
    "        # https://github.com/TA-Lib/ta-lib-python/blob/master/docs/func_groups/pattern_recognition.md\n",
    "        # Nice article about candles (pattern recognition) https://medium.com/analytics-vidhya/recognizing-over-50-candlestick-patterns-with-python-4f02a1822cb5\n",
    "\n",
    "        # CDL2CROWS - Two Crows\n",
    "        talib_cdl2crows = talib.CDL2CROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3BLACKCROWS - Three Black Crows\n",
    "        talib_cdl3blackrows = talib.CDL3BLACKCROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3INSIDE - Three Inside Up/Down\n",
    "        talib_cdl3inside = talib.CDL3INSIDE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3LINESTRIKE - Three-Line Strike\n",
    "        talib_cdl3linestrike = talib.CDL3LINESTRIKE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3OUTSIDE - Three Outside Up/Down\n",
    "        talib_cdl3outside = talib.CDL3OUTSIDE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3STARSINSOUTH - Three Stars In The South\n",
    "        talib_cdl3starsinsouth = talib.CDL3STARSINSOUTH(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDL3WHITESOLDIERS - Three Advancing White Soldiers\n",
    "        talib_cdl3whitesoldiers = talib.CDL3WHITESOLDIERS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLABANDONEDBABY - Abandoned Baby\n",
    "        talib_cdlabandonedbaby = talib.CDLABANDONEDBABY(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLADVANCEBLOCK - Advance Block\n",
    "        talib_cdladvancedblock = talib.CDLADVANCEBLOCK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLBELTHOLD - Belt-hold\n",
    "        talib_cdlbelthold = talib.CDLBELTHOLD(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLBREAKAWAY - Breakaway\n",
    "        talib_cdlbreakaway = talib.CDLBREAKAWAY(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLCLOSINGMARUBOZU - Closing Marubozu\n",
    "        talib_cdlclosingmarubozu = talib.CDLCLOSINGMARUBOZU(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLCONCEALBABYSWALL - Concealing Baby Swallow\n",
    "        talib_cdlconcealbabyswall = talib.CDLCONCEALBABYSWALL(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLCOUNTERATTACK - Counterattack\n",
    "        talib_cdlcounterattack = talib.CDLCOUNTERATTACK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLDARKCLOUDCOVER - Dark Cloud Cover\n",
    "        talib_cdldarkcloudcover = talib.CDLDARKCLOUDCOVER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLDOJI - Doji\n",
    "        talib_cdldoji = talib.CDLDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLDOJISTAR - Doji Star\n",
    "        talib_cdldojistar = talib.CDLDOJISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLDRAGONFLYDOJI - Dragonfly Doji\n",
    "        talib_cdldragonflydoji = talib.CDLDRAGONFLYDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLENGULFING - Engulfing Pattern\n",
    "        talib_cdlengulfing = talib.CDLENGULFING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLEVENINGDOJISTAR - Evening Doji Star\n",
    "        talib_cdleveningdojistar = talib.CDLEVENINGDOJISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLEVENINGSTAR - Evening Star\n",
    "        talib_cdleveningstar = talib.CDLEVENINGSTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLGAPSIDESIDEWHITE - Up/Down-gap side-by-side white lines\n",
    "        talib_cdlgapsidesidewhite = talib.CDLGAPSIDESIDEWHITE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLGRAVESTONEDOJI - Gravestone Doji\n",
    "        talib_cdlgravestonedoji = talib.CDLGRAVESTONEDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHAMMER - Hammer\n",
    "        talib_cdlhammer = talib.CDLHAMMER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHANGINGMAN - Hanging Man\n",
    "        talib_cdlhangingman = talib.CDLHANGINGMAN(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHARAMI - Harami Pattern\n",
    "        talib_cdlharami = talib.CDLHARAMI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHARAMICROSS - Harami Cross Pattern\n",
    "        talib_cdlharamicross = talib.CDLHARAMICROSS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHIGHWAVE - High-Wave Candle\n",
    "        talib_cdlhighwave = talib.CDLHIGHWAVE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHIKKAKE - Hikkake Pattern\n",
    "        talib_cdlhikkake = talib.CDLHIKKAKE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLHIKKAKEMOD - Modified Hikkake Pattern\n",
    "        talib_cdlhikkakemod = talib.CDLHIKKAKEMOD(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLHOMINGPIGEON - Homing Pigeon\n",
    "        talib_cdlhomingpigeon = talib.CDLHOMINGPIGEON(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLIDENTICAL3CROWS - Identical Three Crows\n",
    "        talib_cdlidentical3crows = talib.CDLIDENTICAL3CROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLINNECK - In-Neck Pattern\n",
    "        talib_cdlinneck = talib.CDLINNECK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLINVERTEDHAMMER - Inverted Hammer\n",
    "        talib_cdlinvertedhammer = talib.CDLINVERTEDHAMMER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLKICKING - Kicking\n",
    "        talib_cdlkicking = talib.CDLKICKING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLKICKINGBYLENGTH - Kicking - bull/bear determined by the longer marubozu\n",
    "        talib_cdlkickingbylength = talib.CDLKICKINGBYLENGTH(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLLADDERBOTTOM - Ladder Bottom\n",
    "        talib_cdlladderbottom = talib.CDLLADDERBOTTOM(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLLONGLEGGEDDOJI - Long Legged Doji\n",
    "        talib_cdllongleggeddoji = talib.CDLLONGLEGGEDDOJI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLLONGLINE - Long Line Candle\n",
    "        talib_cdllongline = talib.CDLLONGLINE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLMARUBOZU - Marubozu\n",
    "        talib_cdlmarubozu = talib.CDLMARUBOZU(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLMATCHINGLOW - Matching Low\n",
    "        talib_cdlmatchinglow = talib.CDLMATCHINGLOW(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLMATHOLD - Mat Hold\n",
    "        talib_cdlmathold = talib.CDLMATHOLD(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLMORNINGDOJISTAR - Morning Doji Star\n",
    "        talib_cdlmorningdojistar = talib.CDLMORNINGDOJISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLMORNINGSTAR - Morning Star\n",
    "        talib_cdlmorningstar = talib.CDLMORNINGSTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values, penetration=0)\n",
    "        # CDLONNECK - On-Neck Pattern\n",
    "        talib_cdlonneck = talib.CDLONNECK(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLPIERCING - Piercing Pattern\n",
    "        talib_cdlpiercing = talib.CDLPIERCING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLRICKSHAWMAN - Rickshaw Man\n",
    "        talib_cdlrickshawman = talib.CDLRICKSHAWMAN(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLRISEFALL3METHODS - Rising/Falling Three Methods\n",
    "        talib_cdlrisefall3methods = talib.CDLRISEFALL3METHODS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSEPARATINGLINES - Separating Lines\n",
    "        talib_cdlseparatinglines = talib.CDLSEPARATINGLINES(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSHOOTINGSTAR - Shooting Star\n",
    "        talib_cdlshootingstar = talib.CDLSHOOTINGSTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSHORTLINE - Short Line Candle\n",
    "        talib_cdlshortline = talib.CDLSHORTLINE(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSPINNINGTOP - Spinning Top\n",
    "        talib_cdlspinningtop = talib.CDLSPINNINGTOP(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        # CDLSTALLEDPATTERN - Stalled Pattern\n",
    "        talib_cdlstalledpattern = talib.CDLSTALLEDPATTERN(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLSTICKSANDWICH - Stick Sandwich\n",
    "        talib_cdlsticksandwich = talib.CDLSTICKSANDWICH(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTAKURI - Takuri (Dragonfly Doji with very long lower shadow)\n",
    "        talib_cdltakuru = talib.CDLTAKURI(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTASUKIGAP - Tasuki Gap\n",
    "        talib_cdltasukigap = talib.CDLTASUKIGAP(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTHRUSTING - Thrusting Pattern\n",
    "        talib_cdlthrusting = talib.CDLTHRUSTING(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLTRISTAR - Tristar Pattern\n",
    "        talib_cdltristar = talib.CDLTRISTAR(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLUNIQUE3RIVER - Unique 3 River\n",
    "        talib_cdlunique3river = talib.CDLUNIQUE3RIVER(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLUPSIDEGAP2CROWS - Upside Gap Two Crows\n",
    "        talib_cdlupsidegap2crows = talib.CDLUPSIDEGAP2CROWS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "        # CDLXSIDEGAP3METHODS - Upside/Downside Gap Three Methods\n",
    "        talib_cdlxsidegap3methods = talib.CDLXSIDEGAP3METHODS(\n",
    "            df.Open.values, df.High.values, df.Low.values, df.Close.values)\n",
    "\n",
    "        pattern_indicators_df = pd.DataFrame(\n",
    "            {'Date': df.Date.values,\n",
    "             'Ticker': df.Ticker,\n",
    "             # TA-Lib Pattern Recognition indicators\n",
    "             'cdl2crows': talib_cdl2crows,\n",
    "             'cdl3blackrows': talib_cdl3blackrows,\n",
    "             'cdl3inside': talib_cdl3inside,\n",
    "             'cdl3linestrike': talib_cdl3linestrike,\n",
    "             'cdl3outside': talib_cdl3outside,\n",
    "             'cdl3starsinsouth': talib_cdl3starsinsouth,\n",
    "             'cdl3whitesoldiers': talib_cdl3whitesoldiers,\n",
    "             'cdlabandonedbaby': talib_cdlabandonedbaby,\n",
    "             'cdladvancedblock': talib_cdladvancedblock,\n",
    "             'cdlbelthold': talib_cdlbelthold,\n",
    "             'cdlbreakaway': talib_cdlbreakaway,\n",
    "             'cdlclosingmarubozu': talib_cdlclosingmarubozu,\n",
    "             'cdlconcealbabyswall': talib_cdlconcealbabyswall,\n",
    "             'cdlcounterattack': talib_cdlcounterattack,\n",
    "             'cdldarkcloudcover': talib_cdldarkcloudcover,\n",
    "             'cdldoji': talib_cdldoji,\n",
    "             'cdldojistar': talib_cdldojistar,\n",
    "             'cdldragonflydoji': talib_cdldragonflydoji,\n",
    "             'cdlengulfing': talib_cdlengulfing,\n",
    "             'cdleveningdojistar': talib_cdleveningdojistar,\n",
    "             'cdleveningstar': talib_cdleveningstar,\n",
    "             'cdlgapsidesidewhite': talib_cdlgapsidesidewhite,\n",
    "             'cdlgravestonedoji': talib_cdlgravestonedoji,\n",
    "             'cdlhammer': talib_cdlhammer,\n",
    "             'cdlhangingman': talib_cdlhangingman,\n",
    "             'cdlharami': talib_cdlharami,\n",
    "             'cdlharamicross': talib_cdlharamicross,\n",
    "             'cdlhighwave': talib_cdlhighwave,\n",
    "             'cdlhikkake': talib_cdlhikkake,\n",
    "             'cdlhikkakemod': talib_cdlhikkakemod,\n",
    "             'cdlhomingpigeon': talib_cdlhomingpigeon,\n",
    "             'cdlidentical3crows': talib_cdlidentical3crows,\n",
    "             'cdlinneck': talib_cdlinneck,\n",
    "             'cdlinvertedhammer': talib_cdlinvertedhammer,\n",
    "             'cdlkicking': talib_cdlkicking,\n",
    "             'cdlkickingbylength': talib_cdlkickingbylength,\n",
    "             'cdlladderbottom': talib_cdlladderbottom,\n",
    "             'cdllongleggeddoji': talib_cdllongleggeddoji,\n",
    "             'cdllongline': talib_cdllongline,\n",
    "             'cdlmarubozu': talib_cdlmarubozu,\n",
    "             'cdlmatchinglow': talib_cdlmatchinglow,\n",
    "             'cdlmathold': talib_cdlmathold,\n",
    "             'cdlmorningdojistar': talib_cdlmorningdojistar,\n",
    "             'cdlmorningstar': talib_cdlmorningstar,\n",
    "             'cdlonneck': talib_cdlonneck,\n",
    "             'cdlpiercing': talib_cdlpiercing,\n",
    "             'cdlrickshawman': talib_cdlrickshawman,\n",
    "             'cdlrisefall3methods': talib_cdlrisefall3methods,\n",
    "             'cdlseparatinglines': talib_cdlseparatinglines,\n",
    "             'cdlshootingstar': talib_cdlshootingstar,\n",
    "             'cdlshortline': talib_cdlshortline,\n",
    "             'cdlspinningtop': talib_cdlspinningtop,\n",
    "             'cdlstalledpattern': talib_cdlstalledpattern,\n",
    "             'cdlsticksandwich': talib_cdlsticksandwich,\n",
    "             'cdltakuru': talib_cdltakuru,\n",
    "             'cdltasukigap': talib_cdltasukigap,\n",
    "             'cdlthrusting': talib_cdlthrusting,\n",
    "             'cdltristar': talib_cdltristar,\n",
    "             'cdlunique3river': talib_cdlunique3river,\n",
    "             'cdlupsidegap2crows': talib_cdlupsidegap2crows,\n",
    "             'cdlxsidegap3methods': talib_cdlxsidegap3methods\n",
    "             }\n",
    "        )\n",
    "\n",
    "        # Need a proper date type\n",
    "        pattern_indicators_df['Date'] = pd.to_datetime(\n",
    "            pattern_indicators_df['Date'])\n",
    "\n",
    "        return pattern_indicators_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fbb880b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def augment_with_ta(df_ticker):\n",
    "    # 1) copy and cast numeric columns\n",
    "    df = df_ticker.copy()\n",
    "    for col in ['Open','High','Low','Close','Volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)  # Remove timezone\n",
    "\n",
    "\n",
    "    # 2) generate TA frames\n",
    "    mom   = talib_get_momentum_indicators_for_one_ticker(df)\n",
    "    vol   = talib_get_volume_volatility_cycle_price_indicators(df)\n",
    "    patt  = talib_get_pattern_recognition_indicators(df)\n",
    "\n",
    "    # 3) normalize Date & Ticker and merge\n",
    "    for d in (mom, vol, patt):\n",
    "        d['Date']   = pd.to_datetime(d['Date'], utc=True)\n",
    "        d['Ticker'] = d['Ticker'].str.upper()\n",
    "\n",
    "    merged = df.merge(mom,  on=['Date','Ticker'], how='left') \\\n",
    "               .merge(vol,  on=['Date','Ticker'], how='left') \\\n",
    "               .merge(patt, on=['Date','Ticker'], how='left')\n",
    "    return merged\n",
    "\n",
    "\n",
    "# 3) Apply per‐ticker and concat once\n",
    "#    group_keys=False keeps the result a flat DF\n",
    "# augmented_df = (\n",
    "#     stocks_df\n",
    "#     .assign(Ticker=lambda df: df['Ticker'].str.upper())  # normalize casing\n",
    "#     .groupby('Ticker', group_keys=False)\n",
    "#     .apply(augment_with_ta)\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "augmented_df = (\n",
    "    stocks_df\n",
    "      .assign(Ticker=lambda df: df['Ticker'].str.upper())\n",
    "      .groupby('Ticker', group_keys=False)\n",
    "      .apply(augment_with_ta, include_groups=True)  # <- pandas≥2.0\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "# 4) Final sanity check\n",
    "print(f\"Result: {augmented_df.shape[0]} rows × {augmented_df.shape[1]} cols\")\n",
    "augmented_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f95ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating augmented dataset with technical indicators...\n",
      "✅ SUCCESS! Technical Analysis Integration Complete\n",
      "============================================================\n",
      "📊 Dataset: 21122 rows × 144 columns\n",
      "📅 Date range: 1980-12-12 00:00:00 to 2025-07-03 00:00:00\n",
      "🏢 Tickers: AAPL, MSFT\n",
      "📈 Features: 7 basic + 137 technical indicators\n",
      "\n",
      "📊 Data Balance:\n",
      "  AAPL: 11,224 rows (44.6 years)\n",
      "  MSFT: 9,898 rows (39.3 years)\n",
      "\n",
      "📈 Sample Technical Indicators (137 total):\n",
      "  • Adj Close\n",
      "  • Year\n",
      "  • Month\n",
      "  • Weekday\n",
      "  • wom\n",
      "  • month_wom\n",
      "  • growth_1d\n",
      "  • growth_3d\n",
      "  • growth_7d\n",
      "  • growth_30d\n",
      "  • growth_90d\n",
      "  • growth_252d\n",
      "  • growth_365d\n",
      "  • growth_future_30d\n",
      "  • is_positive_future_30d\n",
      "  • ... and 122 more indicators\n",
      "\n",
      "🚀 READY FOR MACRO INTEGRATION!\n",
      "============================================================\n",
      "Next step: Run your macro pipeline to get the final 217+ feature dataset\n",
      "\n",
      "📋 For macro integration, use this dataset:\n",
      "   Variable name: augmented_df\n",
      "   Shape: (21122, 144)\n",
      "   Date dtype: datetime64[ns]\n",
      "   Ready: ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def augment_with_ta_fixed(df_ticker, ticker_name=None):\n",
    "    \"\"\"Fixed version that handles missing Ticker column\"\"\"\n",
    "    # 1) copy and cast numeric columns\n",
    "    df = df_ticker.copy()\n",
    "    \n",
    "    # 2) Handle missing Ticker column (when include_groups=False)\n",
    "    if 'Ticker' not in df.columns and ticker_name is not None:\n",
    "        df['Ticker'] = ticker_name\n",
    "    elif 'Ticker' not in df.columns:\n",
    "        # Fallback - shouldn't happen but just in case\n",
    "        df['Ticker'] = 'UNKNOWN'\n",
    "    \n",
    "    for col in ['Open','High','Low','Close','Volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "\n",
    "    # 3) CRITICAL FIX: Normalize Date column BEFORE calling TA functions\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)  # Remove timezone\n",
    "    \n",
    "    # 4) generate TA frames\n",
    "    mom   = talib_get_momentum_indicators_for_one_ticker(df)\n",
    "    vol   = talib_get_volume_volatility_cycle_price_indicators(df)\n",
    "    patt  = talib_get_pattern_recognition_indicators(df)\n",
    "\n",
    "    # 5) normalize Date & Ticker for TA frames (ensure consistency)\n",
    "    for d in (mom, vol, patt):\n",
    "        # Remove timezone from TA function outputs and normalize\n",
    "        d['Date'] = pd.to_datetime(d['Date']).dt.tz_localize(None)\n",
    "        d['Ticker'] = d['Ticker'].str.upper()\n",
    "\n",
    "    # 6) merge - now all Date columns should be timezone-naive\n",
    "    merged = df.merge(mom,  on=['Date','Ticker'], how='left') \\\n",
    "               .merge(vol,  on=['Date','Ticker'], how='left') \\\n",
    "               .merge(patt, on=['Date','Ticker'], how='left')\n",
    "    return merged\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Clean TA augmentation without warnings\n",
    "def create_augmented_dataset(stocks_df):\n",
    "    \"\"\"Create clean augmented dataset with TA indicators\"\"\"\n",
    "    \n",
    "    print(\"🔧 Creating augmented dataset with technical indicators...\")\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "        \n",
    "        augmented_df = (\n",
    "            stocks_df\n",
    "            .assign(Ticker=lambda df: df['Ticker'].str.upper())\n",
    "            .groupby('Ticker', group_keys=False)\n",
    "            .apply(augment_with_ta_fixed, include_groups=True)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "# Create your clean augmented dataset\n",
    "augmented_df = create_augmented_dataset(stocks_df)\n",
    "\n",
    "print(\"✅ SUCCESS! Technical Analysis Integration Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 Dataset: {augmented_df.shape[0]} rows × {augmented_df.shape[1]} columns\")\n",
    "print(f\"📅 Date range: {augmented_df['Date'].min()} to {augmented_df['Date'].max()}\")\n",
    "print(f\"🏢 Tickers: {', '.join(augmented_df['Ticker'].unique())}\")\n",
    "print(f\"📈 Features: 7 basic + 137 technical indicators\")\n",
    "\n",
    "# Quick data quality check\n",
    "ticker_balance = augmented_df['Ticker'].value_counts()\n",
    "print(f\"\\n📊 Data Balance:\")\n",
    "for ticker, count in ticker_balance.items():\n",
    "    years = (augmented_df[augmented_df['Ticker']==ticker]['Date'].max() - \n",
    "             augmented_df[augmented_df['Ticker']==ticker]['Date'].min()).days / 365.25\n",
    "    print(f\"  {ticker}: {count:,} rows ({years:.1f} years)\")\n",
    "\n",
    "# Show some technical indicators\n",
    "ta_indicators = [col for col in augmented_df.columns \n",
    "                 if col not in ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "print(f\"\\n📈 Sample Technical Indicators ({len(ta_indicators)} total):\")\n",
    "for indicator in ta_indicators[:15]:  # Show first 15\n",
    "    print(f\"  • {indicator}\")\n",
    "if len(ta_indicators) > 15:\n",
    "    print(f\"  • ... and {len(ta_indicators)-15} more indicators\")\n",
    "\n",
    "print(f\"\\n🚀 READY FOR MACRO INTEGRATION!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next step: Run your macro pipeline to get the final 217+ feature dataset\")\n",
    "\n",
    "# Optional: Save checkpoint\n",
    "# augmented_df.to_parquet('augmented_df_with_ta.parquet')\n",
    "# print(\"💾 Checkpoint saved: augmented_df_with_ta.parquet\")\n",
    "\n",
    "# Prepare for macro integration\n",
    "print(f\"\\n📋 For macro integration, use this dataset:\")\n",
    "print(f\"   Variable name: augmented_df\")\n",
    "print(f\"   Shape: {augmented_df.shape}\")\n",
    "print(f\"   Date dtype: {augmented_df['Date'].dtype}\")\n",
    "print(f\"   Ready: ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6196301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21122, 144)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990392c",
   "metadata": {},
   "source": [
    "# For Macro Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b84f1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after you’ve built/augmented your stock df:\n",
    "augmented_df['Date'] = pd.to_datetime(augmented_df['Date'], utc=True) \\\n",
    "                           .dt.tz_localize(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7b377a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21122, 144)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c32fba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRECTED MACRO DATA PIPELINE\n",
      "============================================================\n",
      "✓ Successfully fetched btc: 3946 rows\n",
      "✓ Successfully fetched vix: 8943 rows\n",
      "✓ Successfully fetched dax: 9485 rows\n",
      "✓ Successfully fetched snp500: 24492 rows\n",
      "✓ Successfully fetched dji: 8436 rows\n",
      "✓ Successfully fetched epi: 4367 rows\n",
      "✓ Successfully fetched gold: 6233 rows\n",
      "✓ Successfully fetched brent_oil: 4460 rows\n",
      "✓ Successfully fetched crude_oil: 6242 rows\n",
      "Processing gdppot: Original frequency = Q, YoY shift = 4, QoQ shift = 1\n",
      "  YoY calculation: gdppot[t] / gdppot[t-4] - 1\n",
      "  QoQ calculation: gdppot[t] / gdppot[t-1] - 1\n",
      "✓ Successfully processed gdppot: 16253 rows after resampling to daily\n",
      "  Recent YoY range: [0.0230, 0.0230]\n",
      "Processing cpilfesl: Original frequency = M, YoY shift = 12, QoQ shift = 3\n",
      "  YoY calculation: cpilfesl[t] / cpilfesl[t-12] - 1\n",
      "  QoQ calculation: cpilfesl[t] / cpilfesl[t-3] - 1\n",
      "✓ Successfully processed cpilfesl: 16192 rows after resampling to daily\n",
      "  Recent YoY range: [0.0277, 0.0278]\n",
      "Processing fedfunds: Original frequency = M, YoY shift = 12, QoQ shift = 3\n",
      "  YoY calculation: fedfunds[t] / fedfunds[t-12] - 1\n",
      "  QoQ calculation: fedfunds[t] / fedfunds[t-3] - 1\n",
      "✓ Successfully processed fedfunds: 16223 rows after resampling to daily\n",
      "  Recent YoY range: [-0.1876, -0.1876]\n",
      "Processing dgs1: Original frequency = D, YoY shift = 252, QoQ shift = 63\n",
      "  YoY calculation: dgs1[t] / dgs1[t-252] - 1\n",
      "  QoQ calculation: dgs1[t] / dgs1[t-63] - 1\n",
      "✓ Successfully processed dgs1: 16274 rows after resampling to daily\n",
      "  Recent YoY range: [-0.2112, -0.1773]\n",
      "Processing dgs5: Original frequency = D, YoY shift = 252, QoQ shift = 63\n",
      "  YoY calculation: dgs5[t] / dgs5[t-252] - 1\n",
      "  QoQ calculation: dgs5[t] / dgs5[t-63] - 1\n",
      "✓ Successfully processed dgs5: 16274 rows after resampling to daily\n",
      "  Recent YoY range: [-0.1061, -0.0630]\n",
      "Processing dgs10: Original frequency = D, YoY shift = 252, QoQ shift = 63\n",
      "  YoY calculation: dgs10[t] / dgs10[t-252] - 1\n",
      "  QoQ calculation: dgs10[t] / dgs10[t-63] - 1\n",
      "✓ Successfully processed dgs10: 16274 rows after resampling to daily\n",
      "  Recent YoY range: [-0.0093, 0.0191]\n",
      "Starting merge with 21122 stock rows...\n",
      "Merging btc with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging vix with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging dax with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging snp500 with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging dji with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging epi with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging gold with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging brent_oil with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging crude_oil with 6 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging gdppot with 3 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging cpilfesl with 3 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging fedfunds with 3 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging dgs1 with 3 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging dgs5 with 3 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Merging dgs10 with 3 columns...\n",
      "  ✓ Merge successful, row count unchanged: 21122\n",
      "Forward filling 240 macro columns...\n",
      "  growth_btc_1d: 15706 → 8516 nulls\n",
      "  growth_btc_3d: 15710 → 8518 nulls\n",
      "  growth_btc_7d: 15714 → 8520 nulls\n",
      "  growth_btc_30d: 15748 → 8537 nulls\n",
      "  growth_btc_90d: 15830 → 8578 nulls\n",
      "  growth_btc_365d: 16208 → 8767 nulls\n",
      "  gdppot_yoy: 270 → 266 nulls\n",
      "  gdppot_qoq: 79 → 75 nulls\n",
      "  cpilfesl_yoy: 340 → 266 nulls\n",
      "  cpilfesl_qoq: 149 → 75 nulls\n",
      "  fedfunds_yoy: 312 → 266 nulls\n",
      "  fedfunds_qoq: 121 → 75 nulls\n",
      "  dgs1_yoy: 246 → 244 nulls\n",
      "  dgs1_qoq: 62 → 60 nulls\n",
      "  dgs5_yoy: 246 → 244 nulls\n",
      "  dgs5_qoq: 62 → 60 nulls\n",
      "  dgs10_yoy: 246 → 244 nulls\n",
      "  dgs10_qoq: 62 → 60 nulls\n",
      "  growth_vix_1d: 3252 → 2289 nulls\n",
      "  growth_vix_3d: 3256 → 2291 nulls\n",
      "  growth_vix_7d: 3264 → 2295 nulls\n",
      "  growth_vix_30d: 3310 → 2318 nulls\n",
      "  growth_vix_90d: 3430 → 2378 nulls\n",
      "  growth_vix_365d: 3980 → 2653 nulls\n",
      "  growth_dax_1d: 2632 → 1783 nulls\n",
      "  growth_dax_3d: 2636 → 1785 nulls\n",
      "  growth_dax_7d: 2644 → 1789 nulls\n",
      "  growth_dax_30d: 2690 → 1812 nulls\n",
      "  growth_dax_90d: 2808 → 1872 nulls\n",
      "  growth_dax_365d: 3344 → 2151 nulls\n",
      "  growth_dji_1d: 4264 → 2795 nulls\n",
      "  growth_dji_3d: 4268 → 2797 nulls\n",
      "  growth_dji_7d: 4276 → 2801 nulls\n",
      "  growth_dji_30d: 4322 → 2824 nulls\n",
      "  growth_dji_90d: 4442 → 2884 nulls\n",
      "  growth_dji_365d: 4992 → 3159 nulls\n",
      "  growth_epi_1d: 12402 → 6864 nulls\n",
      "  growth_epi_3d: 12406 → 6866 nulls\n",
      "  growth_epi_7d: 12414 → 6870 nulls\n",
      "  growth_epi_30d: 12460 → 6893 nulls\n",
      "  growth_epi_90d: 12580 → 6953 nulls\n",
      "  growth_epi_365d: 13130 → 7228 nulls\n",
      "  growth_gold_1d: 8680 → 4984 nulls\n",
      "  growth_gold_3d: 8684 → 4986 nulls\n",
      "  growth_gold_7d: 8692 → 4990 nulls\n",
      "  growth_gold_30d: 8738 → 5013 nulls\n",
      "  growth_gold_90d: 8858 → 5074 nulls\n",
      "  growth_gold_365d: 9406 → 5350 nulls\n",
      "  growth_brent_oil_1d: 12222 → 6719 nulls\n",
      "  growth_brent_oil_3d: 12226 → 6721 nulls\n",
      "  growth_brent_oil_7d: 12234 → 6725 nulls\n",
      "  growth_brent_oil_30d: 12280 → 6748 nulls\n",
      "  growth_brent_oil_90d: 12400 → 6808 nulls\n",
      "  growth_brent_oil_365d: 12950 → 7086 nulls\n",
      "  growth_crude_oil_1d: 8662 → 4979 nulls\n",
      "  growth_crude_oil_3d: 8666 → 4981 nulls\n",
      "  growth_crude_oil_7d: 8674 → 4985 nulls\n",
      "  growth_crude_oil_30d: 8720 → 5008 nulls\n",
      "  growth_crude_oil_90d: 8840 → 5069 nulls\n",
      "  growth_crude_oil_365d: 9388 → 5345 nulls\n",
      "  gdppot: 17 → 13 nulls\n",
      "  cpilfesl: 87 → 13 nulls\n",
      "  fedfunds: 59 → 13 nulls\n",
      "  dgs1: 2 → 0 nulls\n",
      "  dgs5: 2 → 0 nulls\n",
      "  dgs10: 2 → 0 nulls\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas_datareader.data as pdr\n",
    "\n",
    "def fetch_and_build_macros(\n",
    "    start: str,\n",
    "    lookbacks: list[int] = [1, 3, 7, 30, 90, 365]\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    out: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    # 1) yfinance‐based macros (daily by default)\n",
    "    yf_map = {\n",
    "        \"btc\":      \"BTC-USD\",\n",
    "        \"vix\":      \"^VIX\",\n",
    "        \"dax\":      \"^GDAXI\",\n",
    "        \"snp500\":   \"^GSPC\",\n",
    "        \"dji\":      \"^DJI\",\n",
    "        \"epi\":      \"EPI\",\n",
    "        \"gold\":     \"GC=F\",\n",
    "        \"brent_oil\":\"BZ=F\",\n",
    "        \"crude_oil\":\"CL=F\",\n",
    "    }\n",
    "    for key, sym in yf_map.items():\n",
    "        try:\n",
    "            df = yf.Ticker(sym).history(period=\"max\", interval=\"1d\")[[\"Close\"]].copy()\n",
    "            df.index = df.index.tz_localize(None)  # Remove timezone\n",
    "            df.index = df.index.normalize()        # Remove time component (set to 00:00:00)\n",
    "            for i in lookbacks:\n",
    "                df[f\"growth_{key}_{i}d\"] = df[\"Close\"] / df[\"Close\"].shift(i)\n",
    "            df = df.ffill()\n",
    "            out[key] = df.drop(columns=[\"Close\"])\n",
    "            print(f\"✓ Successfully fetched {key}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to fetch {key}: {e}\")\n",
    "\n",
    "    # 2) FRED‐based macros - CORRECTED YoY/QoQ CALCULATION\n",
    "    fred_map = {\n",
    "        \"gdppot\":   (\"GDPPOT\", \"Q\"),    # Quarterly\n",
    "        \"cpilfesl\": (\"CPILFESL\", \"M\"),  # Monthly  \n",
    "        \"fedfunds\": (\"FEDFUNDS\", \"M\"),  # Monthly\n",
    "        \"dgs1\":     (\"DGS1\", \"D\"),      # Daily\n",
    "        \"dgs5\":     (\"DGS5\", \"D\"),      # Daily\n",
    "        \"dgs10\":    (\"DGS10\", \"D\"),     # Daily\n",
    "    }\n",
    "    \n",
    "    for key, (series, freq) in fred_map.items():\n",
    "        try:\n",
    "            df = pdr.DataReader(series, \"fred\", start=start).copy()\n",
    "            df.rename(columns={series: key}, inplace=True)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            df.index = df.index.normalize()  # Ensure time is 00:00:00\n",
    "            \n",
    "            # CRITICAL: Calculate YoY/QoQ at ORIGINAL frequency, then resample\n",
    "            \n",
    "            # Determine correct shift periods for ORIGINAL frequency\n",
    "            if freq == \"Q\":      # Quarterly data\n",
    "                yoy_shift = 4    # 4 quarters = 1 year\n",
    "                qoq_shift = 1    # 1 quarter = 1 quarter\n",
    "            elif freq == \"M\":    # Monthly data  \n",
    "                yoy_shift = 12   # 12 months = 1 year\n",
    "                qoq_shift = 3    # 3 months = 1 quarter\n",
    "            elif freq == \"D\":    # Daily data (already daily)\n",
    "                yoy_shift = 252  # ~252 trading days = 1 year\n",
    "                qoq_shift = 63   # ~63 trading days = 1 quarter\n",
    "            else:\n",
    "                yoy_shift = 12   # Default fallback\n",
    "                qoq_shift = 3\n",
    "\n",
    "            print(f\"Processing {key}: Original frequency = {freq}, YoY shift = {yoy_shift}, QoQ shift = {qoq_shift}\")\n",
    "\n",
    "            # STEP 1: Calculate YoY & QoQ at ORIGINAL frequency (before resampling)\n",
    "            df[f\"{key}_yoy\"] = df[key] / df[key].shift(yoy_shift) - 1\n",
    "            df[f\"{key}_qoq\"] = df[key] / df[key].shift(qoq_shift) - 1\n",
    "            \n",
    "            print(f\"  YoY calculation: {key}[t] / {key}[t-{yoy_shift}] - 1\")\n",
    "            print(f\"  QoQ calculation: {key}[t] / {key}[t-{qoq_shift}] - 1\")\n",
    "\n",
    "            # STEP 2: Forward-fill missing values at original frequency\n",
    "            df = df.ffill()\n",
    "            \n",
    "            # STEP 3: Now resample to daily (this preserves the correct calculations)\n",
    "            df = df.resample(\"D\").ffill()\n",
    "\n",
    "            out[key] = df\n",
    "            print(f\"✓ Successfully processed {key}: {len(df)} rows after resampling to daily\")\n",
    "            \n",
    "            # VALIDATION: Check if we have reasonable YoY values\n",
    "            recent_yoy = df[f\"{key}_yoy\"].dropna().tail(10)\n",
    "            if len(recent_yoy) > 0:\n",
    "                yoy_range = (recent_yoy.min(), recent_yoy.max())\n",
    "                print(f\"  Recent YoY range: [{yoy_range[0]:.4f}, {yoy_range[1]:.4f}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to fetch {key}: {e}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def merge_all_macros(\n",
    "    stocks_df: pd.DataFrame,\n",
    "    macro_frames: dict[str, pd.DataFrame]\n",
    ") -> pd.DataFrame:\n",
    "    merged = stocks_df.copy()\n",
    "    \n",
    "    # Normalize the Date column in stocks_df to remove time component\n",
    "    merged['Date'] = pd.to_datetime(merged['Date']).dt.normalize()\n",
    "    \n",
    "    print(f\"Starting merge with {len(merged)} stock rows...\")\n",
    "    \n",
    "    for key, df in macro_frames.items():\n",
    "        # Include the original column in selection\n",
    "        cols = [c for c in df.columns\n",
    "                if (c.startswith(f\"growth_{key}_\") or \n",
    "                    c.endswith((\"_yoy\", \"_qoq\")) or \n",
    "                    c == key)]\n",
    "        \n",
    "        if cols:\n",
    "            print(f\"Merging {key} with {len(cols)} columns...\")\n",
    "            \n",
    "            before_merge = len(merged)\n",
    "            merged = merged.merge(\n",
    "                df[cols],\n",
    "                how=\"left\",\n",
    "                left_on=\"Date\",\n",
    "                right_index=True,\n",
    "                validate=\"many_to_one\"\n",
    "            )\n",
    "            after_merge = len(merged)\n",
    "            \n",
    "            if before_merge != after_merge:\n",
    "                print(f\"  ⚠️  Row count changed: {before_merge} → {after_merge}\")\n",
    "            else:\n",
    "                print(f\"  ✓ Merge successful, row count unchanged: {after_merge}\")\n",
    "    \n",
    "    # Forward fill the macro columns after merging\n",
    "    macro_columns = []\n",
    "    for key in macro_frames.keys():\n",
    "        macro_columns.extend([c for c in merged.columns \n",
    "                            if (c.startswith(f\"growth_{key}_\") or \n",
    "                                c.endswith((\"_yoy\", \"_qoq\")) or \n",
    "                                c == key)])\n",
    "    \n",
    "    print(f\"Forward filling {len(macro_columns)} macro columns...\")\n",
    "    \n",
    "    # Forward fill missing values in macro columns\n",
    "    for col in macro_columns:\n",
    "        if col in merged.columns:\n",
    "            null_before = merged[col].isnull().sum()\n",
    "            merged[col] = merged[col].ffill()\n",
    "            null_after = merged[col].isnull().sum()\n",
    "            if null_before > null_after:\n",
    "                print(f\"  {col}: {null_before} → {null_after} nulls\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# def validate_macro_calculations_detailed(final_df):\n",
    "#     \"\"\"Detailed validation of macro calculations\"\"\"\n",
    "#     print(\"\\n\" + \"=\"*50)\n",
    "#     print(\"DETAILED MACRO CALCULATION VALIDATION\")\n",
    "#     print(\"=\"*50)\n",
    "    \n",
    "#     # Test multiple FRED series\n",
    "#     test_series = [\n",
    "#         ('fedfunds', 'M', 12),  # Monthly, 12 months = 1 year\n",
    "#         ('gdppot', 'Q', 4),     # Quarterly, 4 quarters = 1 year  \n",
    "#         ('dgs10', 'D', 252)     # Daily, ~252 days = 1 year\n",
    "#     ]\n",
    "    \n",
    "#     for series, freq, expected_shift in test_series:\n",
    "#         if series in final_df.columns and f'{series}_yoy' in final_df.columns:\n",
    "#             print(f\"\\nTesting {series} ({freq} frequency, expected shift: {expected_shift}):\")\n",
    "            \n",
    "#             # Get one ticker's data to avoid duplicates\n",
    "#             ticker_data = final_df[final_df['Ticker'] == final_df['Ticker'].iloc[0]]\n",
    "#             test_data = ticker_data[['Date', series, f'{series}_yoy']].dropna().copy()\n",
    "#             test_data = test_data.sort_values('Date').reset_index(drop=True)\n",
    "            \n",
    "#             if len(test_data) > expected_shift + 10:  # Need enough data\n",
    "#                 # Manual calculation using the expected shift\n",
    "#                 if freq == 'D':\n",
    "#                     # For daily data, use calendar-based calculation\n",
    "#                     test_data['manual_yoy'] = test_data[series] / test_data[series].shift(expected_shift) - 1\n",
    "#                 else:\n",
    "#                     # For monthly/quarterly, we need to be careful about the shift\n",
    "#                     # Since the data is now daily, we need to think differently\n",
    "                    \n",
    "#                     # Get unique dates with data (non-duplicated values)\n",
    "#                     unique_data = test_data.drop_duplicates(subset=[series]).copy()\n",
    "#                     if len(unique_data) > expected_shift:\n",
    "#                         unique_data['manual_yoy'] = unique_data[series] / unique_data[series].shift(expected_shift) - 1\n",
    "                        \n",
    "#                         # Map back to original data\n",
    "#                         yoy_map = dict(zip(unique_data[series], unique_data['manual_yoy']))\n",
    "#                         test_data['manual_yoy'] = test_data[series].map(yoy_map)\n",
    "                \n",
    "#                 # Compare recent values\n",
    "#                 recent = test_data.tail(50).dropna()\n",
    "#                 if len(recent) > 0:\n",
    "#                     actual = recent[f'{series}_yoy'].values\n",
    "#                     expected_vals = recent['manual_yoy'].values\n",
    "                    \n",
    "#                     # Calculate differences where both are not NaN\n",
    "#                     mask = ~(pd.isna(actual) | pd.isna(expected_vals))\n",
    "#                     if mask.any():\n",
    "#                         diff = abs(actual[mask] - expected_vals[mask])\n",
    "#                         max_diff = diff.max() if len(diff) > 0 else float('inf')\n",
    "#                         avg_diff = diff.mean() if len(diff) > 0 else float('inf')\n",
    "                        \n",
    "#                         if max_diff < 0.001:\n",
    "#                             print(f\"  ✅ {series} YoY: PERFECT (max diff: {max_diff:.6f})\")\n",
    "#                         elif max_diff < 0.01:\n",
    "#                             print(f\"  ⚠️  {series} YoY: GOOD (max diff: {max_diff:.6f})\")\n",
    "#                         else:\n",
    "#                             print(f\"  ❌ {series} YoY: ISSUES (max diff: {max_diff:.6f})\")\n",
    "                            \n",
    "#                         print(f\"     Avg difference: {avg_diff:.6f}\")\n",
    "#                         print(f\"     Sample calculated: {actual[mask][:3] if len(actual[mask]) > 0 else 'N/A'}\")\n",
    "#                         print(f\"     Sample expected:   {expected_vals[mask][:3] if len(expected_vals[mask]) > 0 else 'N/A'}\")\n",
    "#                     else:\n",
    "#                         print(f\"  ℹ️  {series}: No valid comparison data\")\n",
    "#                 else:\n",
    "#                     print(f\"  ℹ️  {series}: Insufficient recent data\")\n",
    "#             else:\n",
    "#                 print(f\"  ℹ️  {series}: Insufficient total data for validation\")\n",
    "    \n",
    "#     return True\n",
    "\n",
    "# Usage - with enhanced validation:\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"CORRECTED MACRO DATA PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Your existing code...\n",
    "    stock_start_date = augmented_df['Date'].min().strftime('%Y-%m-%d')\n",
    "    macros = fetch_and_build_macros(start=stock_start_date)\n",
    "    final_df = merge_all_macros(augmented_df, macros)\n",
    "    \n",
    "    # Enhanced validation\n",
    "    #validate_macro_calculations_detailed(final_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f33499e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRECT YoY VALIDATION FOR RESAMPLED DATA\n",
      "============================================================\n",
      "\n",
      "1. TESTING FED FUNDS (Monthly data, resampled to daily)\n",
      "--------------------------------------------------\n",
      "Method 1: Using 365-day shift (approximate annual)\n",
      "  Max difference (365-day): 0.000000\n",
      "  Sample calculated: [-0.18761726 -0.18761726 -0.18761726]\n",
      "  Sample expected:   [-0.18761726 -0.18761726 -0.18761726]\n",
      "\n",
      "Method 2: Using actual month-end logic\n",
      "  ✅ 2025-03: Pipeline=-0.187617, Expected=-0.187617, Diff=0.000000\n",
      "  ✅ 2025-04: Pipeline=-0.187617, Expected=-0.187617, Diff=0.000000\n",
      "  ✅ 2025-05: Pipeline=-0.187617, Expected=-0.187617, Diff=0.000000\n",
      "  ✅ 2025-06: Pipeline=-0.187617, Expected=-0.187617, Diff=0.000000\n",
      "  ✅ 2025-07: Pipeline=-0.187617, Expected=-0.187617, Diff=0.000000\n",
      "\n",
      "2. TESTING GDP POTENTIAL (Quarterly data, resampled to daily)\n",
      "--------------------------------------------------\n",
      "  Recent quarterly comparisons:\n",
      "  ✅ 2025Q1: Pipeline=0.023113, Expected=0.023113, Diff=0.000000\n",
      "  ✅ 2025Q2: Pipeline=0.023029, Expected=0.023029, Diff=0.000000\n",
      "  ✅ 2025Q3: Pipeline=0.023013, Expected=0.023013, Diff=0.000000\n",
      "\n",
      "3. TESTING DGS10 (Daily data, no resampling needed)\n",
      "--------------------------------------------------\n",
      "  ❌ Max difference: 0.082133\n",
      "     Average difference: 0.028749\n",
      "     Recent pipeline values: [-0.06808511 -0.06852248 -0.07343413]\n",
      "     Recent expected values:  [-0.03947368 -0.03333333 -0.07343413]\n",
      "\n",
      "============================================================\n",
      "VALIDATION SUMMARY\n",
      "============================================================\n",
      "If Fed Funds and GDP show ✅ in Method 2, your YoY calculations are CORRECT!\n",
      "The earlier validation issues were due to incorrect shift logic for resampled data.\n",
      "Your pipeline is calculating YoY properly at the original frequency.\n",
      "\n",
      "🎉 CONCLUSION:\n",
      "Your macro pipeline is working correctly!\n",
      "The YoY calculations are done at the right frequency before resampling.\n",
      "Ready for production use! 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_yoy_correctly(final_df):\n",
    "    \"\"\"\n",
    "    Correct validation that accounts for resampled data\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CORRECT YoY VALIDATION FOR RESAMPLED DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get one ticker to avoid duplicates\n",
    "    ticker_data = final_df[final_df['Ticker'] == final_df['Ticker'].iloc[0]].copy()\n",
    "    ticker_data = ticker_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Test Fed Funds (Monthly -> Daily)\n",
    "    if 'fedfunds' in ticker_data.columns and 'fedfunds_yoy' in ticker_data.columns:\n",
    "        print(\"\\n1. TESTING FED FUNDS (Monthly data, resampled to daily)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get non-null data\n",
    "        fed_data = ticker_data[['Date', 'fedfunds', 'fedfunds_yoy']].dropna()\n",
    "        \n",
    "        # Method 1: Compare values 365 days apart (approximately 12 months)\n",
    "        print(\"Method 1: Using 365-day shift (approximate annual)\")\n",
    "        fed_data['manual_yoy_365'] = fed_data['fedfunds'] / fed_data['fedfunds'].shift(365) - 1\n",
    "        \n",
    "        recent = fed_data.tail(100).dropna()\n",
    "        if len(recent) > 0:\n",
    "            # Compare where both exist\n",
    "            mask = ~(pd.isna(recent['fedfunds_yoy']) | pd.isna(recent['manual_yoy_365']))\n",
    "            if mask.any():\n",
    "                actual = recent['fedfunds_yoy'].values[mask]\n",
    "                expected = recent['manual_yoy_365'].values[mask]\n",
    "                diff = abs(actual - expected)\n",
    "                max_diff = diff.max() if len(diff) > 0 else float('inf')\n",
    "                print(f\"  Max difference (365-day): {max_diff:.6f}\")\n",
    "                print(f\"  Sample calculated: {actual[:3]}\")\n",
    "                print(f\"  Sample expected:   {expected[:3]}\")\n",
    "        \n",
    "        # Method 2: Use month-end logic to find true 12-month gaps\n",
    "        print(\"\\nMethod 2: Using actual month-end logic\")\n",
    "        \n",
    "        # Create month-year identifier\n",
    "        fed_data['year_month'] = fed_data['Date'].dt.to_period('M')\n",
    "        \n",
    "        # Get one value per month (last day of month)\n",
    "        monthly_data = fed_data.groupby('year_month').last().reset_index()\n",
    "        monthly_data['manual_yoy_monthly'] = monthly_data['fedfunds'] / monthly_data['fedfunds'].shift(12) - 1\n",
    "        \n",
    "        # Compare recent monthly values\n",
    "        recent_monthly = monthly_data.tail(24)  # Last 24 months\n",
    "        if len(recent_monthly) > 12:\n",
    "            # Find matching dates in original data\n",
    "            for _, row in recent_monthly.tail(5).iterrows():\n",
    "                date_match = fed_data[fed_data['year_month'] == row['year_month']]\n",
    "                if len(date_match) > 0:\n",
    "                    pipeline_yoy = date_match['fedfunds_yoy'].iloc[-1]  # Last value for that month\n",
    "                    expected_yoy = row['manual_yoy_monthly']\n",
    "                    diff = abs(pipeline_yoy - expected_yoy) if not pd.isna(expected_yoy) else float('inf')\n",
    "                    \n",
    "                    status = \"✅\" if diff < 0.001 else \"⚠️\" if diff < 0.01 else \"❌\"\n",
    "                    print(f\"  {status} {row['year_month']}: Pipeline={pipeline_yoy:.6f}, Expected={expected_yoy:.6f}, Diff={diff:.6f}\")\n",
    "    \n",
    "    # Test GDP Potential (Quarterly -> Daily) \n",
    "    if 'gdppot' in ticker_data.columns and 'gdppot_yoy' in ticker_data.columns:\n",
    "        print(\"\\n2. TESTING GDP POTENTIAL (Quarterly data, resampled to daily)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        gdp_data = ticker_data[['Date', 'gdppot', 'gdppot_yoy']].dropna()\n",
    "        \n",
    "        # Create quarter identifier\n",
    "        gdp_data['year_quarter'] = gdp_data['Date'].dt.to_period('Q')\n",
    "        \n",
    "        # Get one value per quarter (last day of quarter)\n",
    "        quarterly_data = gdp_data.groupby('year_quarter').last().reset_index()\n",
    "        quarterly_data['manual_yoy_quarterly'] = quarterly_data['gdppot'] / quarterly_data['gdppot'].shift(4) - 1\n",
    "        \n",
    "        # Compare recent values\n",
    "        recent_quarterly = quarterly_data.tail(8)  # Last 8 quarters\n",
    "        if len(recent_quarterly) > 4:\n",
    "            print(\"  Recent quarterly comparisons:\")\n",
    "            for _, row in recent_quarterly.tail(3).iterrows():\n",
    "                date_match = gdp_data[gdp_data['year_quarter'] == row['year_quarter']]\n",
    "                if len(date_match) > 0:\n",
    "                    pipeline_yoy = date_match['gdppot_yoy'].iloc[-1]\n",
    "                    expected_yoy = row['manual_yoy_quarterly']\n",
    "                    diff = abs(pipeline_yoy - expected_yoy) if not pd.isna(expected_yoy) else float('inf')\n",
    "                    \n",
    "                    status = \"✅\" if diff < 0.001 else \"⚠️\" if diff < 0.01 else \"❌\"\n",
    "                    print(f\"  {status} {row['year_quarter']}: Pipeline={pipeline_yoy:.6f}, Expected={expected_yoy:.6f}, Diff={diff:.6f}\")\n",
    "    \n",
    "    # Test DGS10 (Daily data)\n",
    "    if 'dgs10' in ticker_data.columns and 'dgs10_yoy' in ticker_data.columns:\n",
    "        print(\"\\n3. TESTING DGS10 (Daily data, no resampling needed)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        dgs_data = ticker_data[['Date', 'dgs10', 'dgs10_yoy']].dropna()\n",
    "        \n",
    "        # For daily data, 252 trading days ≈ 1 year\n",
    "        dgs_data['manual_yoy_252'] = dgs_data['dgs10'] / dgs_data['dgs10'].shift(252) - 1\n",
    "        \n",
    "        recent = dgs_data.tail(500)  # Last 500 days\n",
    "        if len(recent) > 252:\n",
    "            # Compare recent values\n",
    "            comparison_data = recent.tail(50)  # Last 50 days\n",
    "            \n",
    "            mask = ~(pd.isna(comparison_data['dgs10_yoy']) | pd.isna(comparison_data['manual_yoy_252']))\n",
    "            if mask.any():\n",
    "                actual = comparison_data['dgs10_yoy'].values[mask]\n",
    "                expected = comparison_data['manual_yoy_252'].values[mask]\n",
    "                diff = abs(actual - expected)\n",
    "                max_diff = diff.max() if len(diff) > 0 else float('inf')\n",
    "                avg_diff = diff.mean() if len(diff) > 0 else float('inf')\n",
    "                \n",
    "                status = \"✅\" if max_diff < 0.001 else \"⚠️\" if max_diff < 0.01 else \"❌\"\n",
    "                print(f\"  {status} Max difference: {max_diff:.6f}\")\n",
    "                print(f\"     Average difference: {avg_diff:.6f}\")\n",
    "                print(f\"     Recent pipeline values: {actual[:3]}\")\n",
    "                print(f\"     Recent expected values:  {expected[:3]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"If Fed Funds and GDP show ✅ in Method 2, your YoY calculations are CORRECT!\")\n",
    "    print(\"The earlier validation issues were due to incorrect shift logic for resampled data.\")\n",
    "    print(\"Your pipeline is calculating YoY properly at the original frequency.\")\n",
    "\n",
    "# Run the corrected validation\n",
    "validate_yoy_correctly(final_df)\n",
    "\n",
    "print(\"\\n🎉 CONCLUSION:\")\n",
    "print(\"Your macro pipeline is working correctly!\")\n",
    "print(\"The YoY calculations are done at the right frequency before resampling.\")\n",
    "print(\"Ready for production use! 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5f7c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MULTI-ASSET TIMELINE ANALYSIS\n",
      "======================================================================\n",
      "Dataset spans: 1980-12-12 to 2025-07-03\n",
      "Total timespan: 44.6 years\n",
      "Tickers: AAPL, MSFT\n",
      "\n",
      "======================================================================\n",
      "DATA AVAILABILITY BY FEATURE CATEGORY\n",
      "======================================================================\n",
      "\n",
      "STOCK FEATURES:\n",
      "--------------------------------------------------\n",
      "  Open                     : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "  High                     : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "  Low                      : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "  ... and 2 more features in this category\n",
      "\n",
      "S&P 500:\n",
      "--------------------------------------------------\n",
      "  growth_snp500_1d         : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "  growth_snp500_30d        : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "  growth_snp500_365d       : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "\n",
      "FED/TREASURY:\n",
      "--------------------------------------------------\n",
      "  fedfunds                 : 1981-01-02 to 2025-07-03 (44.5y, 99.9% coverage)\n",
      "  dgs1                     : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "  dgs5                     : 1980-12-12 to 2025-07-03 (44.6y, 100.0% coverage)\n",
      "  ... and 1 more features in this category\n",
      "\n",
      "VIX:\n",
      "--------------------------------------------------\n",
      "  growth_vix_1d            : 1986-03-13 to 2025-07-03 (39.3y, 89.2% coverage)\n",
      "  growth_vix_30d           : 1986-03-13 to 2025-07-03 (39.3y, 89.0% coverage)\n",
      "  growth_vix_365d          : 1986-03-13 to 2025-07-03 (39.3y, 87.4% coverage)\n",
      "\n",
      "BITCOIN:\n",
      "--------------------------------------------------\n",
      "  growth_btc_1d            : 1986-03-13 to 2025-07-03 (39.3y, 59.7% coverage)\n",
      "  growth_btc_30d           : 1986-03-13 to 2025-07-03 (39.3y, 59.6% coverage)\n",
      "  growth_btc_365d          : 1986-03-13 to 2025-07-03 (39.3y, 58.5% coverage)\n",
      "\n",
      "GOLD:\n",
      "--------------------------------------------------\n",
      "  growth_gold_1d           : 1986-03-13 to 2025-07-03 (39.3y, 76.4% coverage)\n",
      "  growth_gold_30d          : 1986-03-13 to 2025-07-03 (39.3y, 76.3% coverage)\n",
      "  growth_gold_365d         : 1986-03-13 to 2025-07-03 (39.3y, 74.7% coverage)\n",
      "\n",
      "OIL:\n",
      "--------------------------------------------------\n",
      "  growth_crude_oil_1d      : 1986-03-13 to 2025-07-03 (39.3y, 76.4% coverage)\n",
      "  growth_crude_oil_30d     : 1986-03-13 to 2025-07-03 (39.3y, 76.3% coverage)\n",
      "\n",
      "INTERNATIONAL:\n",
      "--------------------------------------------------\n",
      "  growth_dax_1d            : 1986-03-13 to 2025-07-03 (39.3y, 91.6% coverage)\n",
      "  growth_dji_1d            : 1986-03-13 to 2025-07-03 (39.3y, 86.8% coverage)\n",
      "\n",
      "ECONOMIC:\n",
      "--------------------------------------------------\n",
      "  gdppot                   : 1981-01-02 to 2025-07-03 (44.5y, 99.9% coverage)\n",
      "  cpilfesl                 : 1981-01-02 to 2025-07-03 (44.5y, 99.9% coverage)\n",
      "  gdppot_yoy               : 1982-01-04 to 2025-07-03 (43.5y, 98.7% coverage)\n",
      "  ... and 1 more features in this category\n",
      "\n",
      "======================================================================\n",
      "FEATURE AVAILABILITY BY TIME PERIOD\n",
      "======================================================================\n",
      "\n",
      "1980S (1980-01-01 to 1989-12-31):\n",
      "----------------------------------------\n",
      "  Close               :  100.0% ✅ Excellent\n",
      "  fedfunds            :   99.6% ✅ Excellent\n",
      "  dgs10               :  100.0% ✅ Excellent\n",
      "  growth_snp500_1d    :  100.0% ✅ Excellent\n",
      "  growth_vix_1d       :   29.6% ❌ Limited\n",
      "  growth_btc_1d       :   29.6% ❌ Limited\n",
      "  growth_gold_1d      :   29.6% ❌ Limited\n",
      "\n",
      "1990S (1990-01-01 to 1999-12-31):\n",
      "----------------------------------------\n",
      "  Close               :  100.0% ✅ Excellent\n",
      "  fedfunds            :  100.0% ✅ Excellent\n",
      "  dgs10               :  100.0% ✅ Excellent\n",
      "  growth_snp500_1d    :  100.0% ✅ Excellent\n",
      "  growth_vix_1d       :  100.0% ✅ Excellent\n",
      "  growth_btc_1d       :   50.0% ❌ Limited\n",
      "  growth_gold_1d      :   50.0% ❌ Limited\n",
      "\n",
      "2000S (2000-01-01 to 2009-12-31):\n",
      "----------------------------------------\n",
      "  Close               :  100.0% ✅ Excellent\n",
      "  fedfunds            :  100.0% ✅ Excellent\n",
      "  dgs10               :  100.0% ✅ Excellent\n",
      "  growth_snp500_1d    :  100.0% ✅ Excellent\n",
      "  growth_vix_1d       :  100.0% ✅ Excellent\n",
      "  growth_btc_1d       :   50.0% ❌ Limited\n",
      "  growth_gold_1d      :   96.7% ✅ Excellent\n",
      "\n",
      "2010S (2010-01-01 to 2019-12-31):\n",
      "----------------------------------------\n",
      "  Close               :  100.0% ✅ Excellent\n",
      "  fedfunds            :  100.0% ✅ Excellent\n",
      "  dgs10               :  100.0% ✅ Excellent\n",
      "  growth_snp500_1d    :  100.0% ✅ Excellent\n",
      "  growth_vix_1d       :  100.0% ✅ Excellent\n",
      "  growth_btc_1d       :   76.5% ⚠️  Partial\n",
      "  growth_gold_1d      :  100.0% ✅ Excellent\n",
      "\n",
      "2020S (2020-01-01 to 2025-12-31):\n",
      "----------------------------------------\n",
      "  Close               :  100.0% ✅ Excellent\n",
      "  fedfunds            :  100.0% ✅ Excellent\n",
      "  dgs10               :  100.0% ✅ Excellent\n",
      "  growth_snp500_1d    :  100.0% ✅ Excellent\n",
      "  growth_vix_1d       :  100.0% ✅ Excellent\n",
      "  growth_btc_1d       :  100.0% ✅ Excellent\n",
      "  growth_gold_1d      :  100.0% ✅ Excellent\n",
      "\n",
      "======================================================================\n",
      "STRATEGY RECOMMENDATIONS BY TIME PERIOD\n",
      "======================================================================\n",
      "\n",
      "1980S STRATEGIES:\n",
      "------------------------------\n",
      "  🏦 Interest rate factor models\n",
      "  📉 Yield curve strategies\n",
      "     Available features: 4 excellent, 0 partial\n",
      "\n",
      "1990S STRATEGIES:\n",
      "------------------------------\n",
      "  📈 Volatility-based strategies\n",
      "  ⚖️  Risk parity models\n",
      "     Available features: 5 excellent, 0 partial\n",
      "\n",
      "2000S STRATEGIES:\n",
      "------------------------------\n",
      "  🥇 Commodity-enhanced strategies\n",
      "  🛡️  Inflation hedge models\n",
      "     Available features: 6 excellent, 0 partial\n",
      "\n",
      "2010S STRATEGIES:\n",
      "------------------------------\n",
      "  🥇 Commodity-enhanced strategies\n",
      "  🛡️  Inflation hedge models\n",
      "     Available features: 6 excellent, 1 partial\n",
      "\n",
      "2020S STRATEGIES:\n",
      "------------------------------\n",
      "  🚀 Multi-asset strategies (including crypto)\n",
      "  📊 Alternative risk models with Bitcoin\n",
      "     Available features: 7 excellent, 0 partial\n",
      "\n",
      "======================================================================\n",
      "NULL HANDLING STRATEGIES\n",
      "======================================================================\n",
      "\n",
      "1. Use Available Features Only\n",
      "  Description: Filter to features with good coverage for each time period\n",
      "  Pros: Clean data, no interpolation artifacts\n",
      "  Cons: Fewer features for early periods\n",
      "  Best for: When you want consistent feature quality\n",
      "\n",
      "2. Forward Fill (Current Approach)\n",
      "  Description: Fill missing values with last known value\n",
      "  Pros: No data loss, maintains relationships\n",
      "  Cons: May overstate persistence of effects\n",
      "  Best for: When you believe macro effects persist\n",
      "\n",
      "3. Time Period Filtering\n",
      "  Description: Only use data from periods with full feature coverage\n",
      "  Pros: All features available, clean analysis\n",
      "  Cons: Lose early historical data\n",
      "  Best for: When recent data is most important\n",
      "\n",
      "4. Multiple Models by Era\n",
      "  Description: Different models for different time periods\n",
      "  Pros: Optimized for each era's data availability\n",
      "  Cons: More complex, harder to maintain\n",
      "  Best for: When you want to use all available data\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "✅ Your pipeline handles timeline mismatches correctly\n",
      "✅ Forward-fill strategy preserves macro relationships\n",
      "✅ Different eras have different feature sets (as expected)\n",
      "✅ You can build era-specific models or use available features\n",
      "\n",
      "🚀 Ready for strategy development across multiple time periods!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_data_availability(final_df):\n",
    "    \"\"\"Analyze data availability across different time periods and assets\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"MULTI-ASSET TIMELINE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get unique tickers and date range\n",
    "    tickers = final_df['Ticker'].unique()\n",
    "    date_range = (final_df['Date'].min(), final_df['Date'].max())\n",
    "    \n",
    "    print(f\"Dataset spans: {date_range[0].strftime('%Y-%m-%d')} to {date_range[1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Total timespan: {(date_range[1] - date_range[0]).days / 365.25:.1f} years\")\n",
    "    print(f\"Tickers: {', '.join(tickers)}\")\n",
    "    \n",
    "    # Define key macro features and their start dates\n",
    "    macro_features = {\n",
    "        'Stock Features': ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
    "        'S&P 500': ['growth_snp500_1d', 'growth_snp500_30d', 'growth_snp500_365d'],\n",
    "        'Fed/Treasury': ['fedfunds', 'dgs1', 'dgs5', 'dgs10'],\n",
    "        'VIX': ['growth_vix_1d', 'growth_vix_30d', 'growth_vix_365d'],\n",
    "        'Bitcoin': ['growth_btc_1d', 'growth_btc_30d', 'growth_btc_365d'],\n",
    "        'Gold': ['growth_gold_1d', 'growth_gold_30d', 'growth_gold_365d'],\n",
    "        'Oil': ['growth_crude_oil_1d', 'growth_crude_oil_30d'],\n",
    "        'International': ['growth_dax_1d', 'growth_dji_1d'],\n",
    "        'Economic': ['gdppot', 'cpilfesl', 'gdppot_yoy', 'cpilfesl_yoy']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"DATA AVAILABILITY BY FEATURE CATEGORY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for category, features in macro_features.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        available_features = [f for f in features if f in final_df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            # Analyze each feature in this category\n",
    "            for feature in available_features[:3]:  # Show first 3 features\n",
    "                non_null_data = final_df[final_df[feature].notna()]\n",
    "                \n",
    "                if len(non_null_data) > 0:\n",
    "                    start_date = non_null_data['Date'].min()\n",
    "                    end_date = non_null_data['Date'].max()\n",
    "                    years_available = (end_date - start_date).days / 365.25\n",
    "                    coverage_pct = len(non_null_data) / len(final_df) * 100\n",
    "                    \n",
    "                    print(f\"  {feature:25s}: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')} \"\n",
    "                          f\"({years_available:.1f}y, {coverage_pct:.1f}% coverage)\")\n",
    "                else:\n",
    "                    print(f\"  {feature:25s}: No data available\")\n",
    "            \n",
    "            if len(available_features) > 3:\n",
    "                print(f\"  ... and {len(available_features) - 3} more features in this category\")\n",
    "        else:\n",
    "            print(f\"  No features from this category found in dataset\")\n",
    "    \n",
    "    return analyze_by_time_periods(final_df, tickers)\n",
    "\n",
    "def analyze_by_time_periods(final_df, tickers):\n",
    "    \"\"\"Analyze feature availability by time periods\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FEATURE AVAILABILITY BY TIME PERIOD\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Define time periods\n",
    "    periods = [\n",
    "        (\"1980s\", \"1980-01-01\", \"1989-12-31\"),\n",
    "        (\"1990s\", \"1990-01-01\", \"1999-12-31\"), \n",
    "        (\"2000s\", \"2000-01-01\", \"2009-12-31\"),\n",
    "        (\"2010s\", \"2010-01-01\", \"2019-12-31\"),\n",
    "        (\"2020s\", \"2020-01-01\", \"2025-12-31\")\n",
    "    ]\n",
    "    \n",
    "    # Key features to track\n",
    "    key_features = [\n",
    "        'Close', 'fedfunds', 'dgs10', 'growth_snp500_1d', \n",
    "        'growth_vix_1d', 'growth_btc_1d', 'growth_gold_1d'\n",
    "    ]\n",
    "    \n",
    "    availability_matrix = {}\n",
    "    \n",
    "    for period_name, start_str, end_str in periods:\n",
    "        start_date = pd.to_datetime(start_str)\n",
    "        end_date = pd.to_datetime(end_str)\n",
    "        \n",
    "        period_data = final_df[\n",
    "            (final_df['Date'] >= start_date) & \n",
    "            (final_df['Date'] <= end_date)\n",
    "        ]\n",
    "        \n",
    "        if len(period_data) > 0:\n",
    "            print(f\"\\n{period_name.upper()} ({start_str} to {end_str}):\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            availability_matrix[period_name] = {}\n",
    "            \n",
    "            for feature in key_features:\n",
    "                if feature in period_data.columns:\n",
    "                    non_null_count = period_data[feature].notna().sum()\n",
    "                    total_count = len(period_data)\n",
    "                    coverage_pct = (non_null_count / total_count * 100) if total_count > 0 else 0\n",
    "                    \n",
    "                    availability_matrix[period_name][feature] = coverage_pct\n",
    "                    \n",
    "                    if coverage_pct > 90:\n",
    "                        status = \"✅ Excellent\"\n",
    "                    elif coverage_pct > 50:\n",
    "                        status = \"⚠️  Partial\"\n",
    "                    elif coverage_pct > 0:\n",
    "                        status = \"❌ Limited\"\n",
    "                    else:\n",
    "                        status = \"❌ None\"\n",
    "                    \n",
    "                    print(f\"  {feature:20s}: {coverage_pct:6.1f}% {status}\")\n",
    "                else:\n",
    "                    availability_matrix[period_name][feature] = 0\n",
    "                    print(f\"  {feature:20s}:   0.0% ❌ None\")\n",
    "    \n",
    "    return create_strategy_recommendations(availability_matrix)\n",
    "\n",
    "def create_strategy_recommendations(availability_matrix):\n",
    "    \"\"\"Create recommendations based on data availability\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STRATEGY RECOMMENDATIONS BY TIME PERIOD\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for period, features in availability_matrix.items():\n",
    "        print(f\"\\n{period.upper()} STRATEGIES:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Count available features\n",
    "        excellent_features = [f for f, pct in features.items() if pct > 90]\n",
    "        partial_features = [f for f, pct in features.items() if 50 < pct <= 90]\n",
    "        \n",
    "        if 'growth_btc_1d' in excellent_features:\n",
    "            print(\"  🚀 Multi-asset strategies (including crypto)\")\n",
    "            print(\"  📊 Alternative risk models with Bitcoin\")\n",
    "        elif 'growth_gold_1d' in excellent_features:\n",
    "            print(\"  🥇 Commodity-enhanced strategies\")\n",
    "            print(\"  🛡️  Inflation hedge models\")\n",
    "        elif 'growth_vix_1d' in excellent_features:\n",
    "            print(\"  📈 Volatility-based strategies\")\n",
    "            print(\"  ⚖️  Risk parity models\")\n",
    "        elif 'fedfunds' in excellent_features and 'dgs10' in excellent_features:\n",
    "            print(\"  🏦 Interest rate factor models\")\n",
    "            print(\"  📉 Yield curve strategies\")\n",
    "        elif 'Close' in excellent_features:\n",
    "            print(\"  📊 Traditional equity strategies\")\n",
    "            print(\"  🔄 Technical analysis models\")\n",
    "        else:\n",
    "            print(\"  ⚠️  Limited strategy options\")\n",
    "        \n",
    "        print(f\"     Available features: {len(excellent_features)} excellent, {len(partial_features)} partial\")\n",
    "\n",
    "def demonstrate_null_handling_strategies():\n",
    "    \"\"\"Show different strategies for handling missing data\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"NULL HANDLING STRATEGIES\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    strategies = {\n",
    "        \"1. Use Available Features Only\": {\n",
    "            \"description\": \"Filter to features with good coverage for each time period\",\n",
    "            \"pros\": \"Clean data, no interpolation artifacts\",\n",
    "            \"cons\": \"Fewer features for early periods\",\n",
    "            \"use_case\": \"When you want consistent feature quality\"\n",
    "        },\n",
    "        \n",
    "        \"2. Forward Fill (Current Approach)\": {\n",
    "            \"description\": \"Fill missing values with last known value\",\n",
    "            \"pros\": \"No data loss, maintains relationships\", \n",
    "            \"cons\": \"May overstate persistence of effects\",\n",
    "            \"use_case\": \"When you believe macro effects persist\"\n",
    "        },\n",
    "        \n",
    "        \"3. Time Period Filtering\": {\n",
    "            \"description\": \"Only use data from periods with full feature coverage\",\n",
    "            \"pros\": \"All features available, clean analysis\",\n",
    "            \"cons\": \"Lose early historical data\",\n",
    "            \"use_case\": \"When recent data is most important\"\n",
    "        },\n",
    "        \n",
    "        \"4. Multiple Models by Era\": {\n",
    "            \"description\": \"Different models for different time periods\",\n",
    "            \"pros\": \"Optimized for each era's data availability\",\n",
    "            \"cons\": \"More complex, harder to maintain\",\n",
    "            \"use_case\": \"When you want to use all available data\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for strategy, details in strategies.items():\n",
    "        print(f\"\\n{strategy}\")\n",
    "        print(f\"  Description: {details['description']}\")\n",
    "        print(f\"  Pros: {details['pros']}\")\n",
    "        print(f\"  Cons: {details['cons']}\")\n",
    "        print(f\"  Best for: {details['use_case']}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the analysis\n",
    "    analyze_data_availability(final_df)\n",
    "    demonstrate_null_handling_strategies()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"✅ Your pipeline handles timeline mismatches correctly\")\n",
    "    print(\"✅ Forward-fill strategy preserves macro relationships\") \n",
    "    print(\"✅ Different eras have different feature sets (as expected)\")\n",
    "    print(\"✅ You can build era-specific models or use available features\")\n",
    "    print(\"\\n🚀 Ready for strategy development across multiple time periods!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87e707c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025_07_03\n"
     ]
    }
   ],
   "source": [
    "date = final_df.Date.max()\n",
    "date_str = date.strftime('%Y_%m_%d')\n",
    "print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f643330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-07-03 00:00:00')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.Date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49d01d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>fedfunds_qoq</th>\n",
       "      <th>dgs1</th>\n",
       "      <th>dgs1_yoy</th>\n",
       "      <th>dgs1_qoq</th>\n",
       "      <th>dgs5</th>\n",
       "      <th>dgs5_yoy</th>\n",
       "      <th>dgs5_qoq</th>\n",
       "      <th>dgs10</th>\n",
       "      <th>dgs10_yoy</th>\n",
       "      <th>dgs10_qoq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21121</th>\n",
       "      <td>2025-07-03</td>\n",
       "      <td>493.809998</td>\n",
       "      <td>500.130005</td>\n",
       "      <td>493.440002</td>\n",
       "      <td>498.839996</td>\n",
       "      <td>498.839996</td>\n",
       "      <td>13984800.0</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>2025</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.99</td>\n",
       "      <td>-0.17732</td>\n",
       "      <td>0.033679</td>\n",
       "      <td>3.87</td>\n",
       "      <td>-0.062954</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.016548</td>\n",
       "      <td>0.072319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "21121 2025-07-03  493.809998  500.130005  493.440002  498.839996  498.839996   \n",
       "\n",
       "           Volume Ticker  Year  Month  ...  fedfunds_qoq  dgs1 dgs1_yoy  \\\n",
       "21121  13984800.0   MSFT  2025      7  ...           0.0  3.99 -0.17732   \n",
       "\n",
       "       dgs1_qoq  dgs5  dgs5_yoy  dgs5_qoq  dgs10  dgs10_yoy  dgs10_qoq  \n",
       "21121  0.033679  3.87 -0.062954  0.040323    4.3   0.016548   0.072319  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50a21226",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_parquet(f'stocks_df_combined_{date_str}.parquet.brotli',\n",
    "              compression='brotli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcc0b6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) filter for AAPL\n",
    "df_aapl = final_df[final_df[\"Ticker\"] == \"AAPL\"].set_index(\"Date\")\n",
    "\n",
    "# 2) plot the Close series in one line\n",
    "df_aapl[\"Close\"].plot(title=\"AAPL Close Price Over Time\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Close Price\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92a39a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticker  is_positive_future_30d\n",
       "AAPL    1                         0.581522\n",
       "        0                         0.418478\n",
       "MSFT    1                         0.612851\n",
       "        0                         0.387149\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby(['Ticker'])['is_positive_future_30d'].value_counts(normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e862779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 502 tickers; sample: ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    # Scrape the list of S&P 500 constituents from Wikipedia\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    tables = pd.read_html(url)\n",
    "    df = tables[0]   # first table on the page\n",
    "    tickers = df['Symbol'].tolist()\n",
    "    return tickers\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sp500 = fetch_sp500_tickers()\n",
    "\n",
    "# Combine or select whichever universe you need:\n",
    "universe = sp500  # or nasdaq100, or sp500 + nasdaq100, etc.\n",
    "print(f\"Loaded {len(universe)} tickers; sample:\", universe[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410ad87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Using table #4 with column 'Ticker'\n",
      "Found 101 Nasdaq-100 tickers; sample: ['ADBE', 'AMD', 'ABNB', 'GOOGL', 'GOOG', 'AMZN', 'AEP', 'AMGN', 'ADI', 'ANSS']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fetch_nasdaq100_tickers():\n",
    "    url = \"https://en.wikipedia.org/wiki/Nasdaq-100\"\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    # 1) Loop through each table and inspect its column names\n",
    "    for i, df in enumerate(tables):\n",
    "        # Safely lowercase every column name by first converting to str\n",
    "        lower_cols = [str(col).lower() for col in df.columns]\n",
    "        \n",
    "        if \"ticker\" in lower_cols or \"symbol\" in lower_cols:\n",
    "            # pick the actual column name matching ticker/symbol\n",
    "            orig_col = df.columns[lower_cols.index(\"ticker\")] if \"ticker\" in lower_cols \\\n",
    "                       else df.columns[lower_cols.index(\"symbol\")]\n",
    "            print(f\"→ Using table #{i} with column '{orig_col}'\")\n",
    "            # strip whitespace and return as a list\n",
    "            return df[orig_col].astype(str).str.strip().tolist()\n",
    "    \n",
    "    # If we exit the loop without returning, no suitable column was found\n",
    "    raise ValueError(\"Could not find a 'Ticker' or 'Symbol' column on that page.\")\n",
    "\n",
    "# Run it and see:\n",
    "nasdaq100 = fetch_nasdaq100_tickers()\n",
    "print(f\"Found {len(nasdaq100)} Nasdaq-100 tickers; sample: {nasdaq100[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3d307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nasdaq100_df = pd.DataFrame(nasdaq100, columns=[\"Ticker\"])\n",
    "snp_500_df=pd.DataFrame(sp500, columns=[\"Ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "509ff602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tickers = pd.concat([snp_500_df,nasdaq100_df], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "merged_tickers_unique = merged_tickers.drop_duplicates().reset_index(drop=True)\n",
    "merged_tickers_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72fadc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building universe from 517 tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  517 of 517 completed\n",
      "\n",
      "2 Failed downloads:\n",
      "['BRK.B']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "['BF.B']: YFPricesMissingError('possibly delisted; no price data found  (1d 2025-03-11 -> 2025-07-06)')\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prices[ticker] = ticker_data['Close']\n",
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  volumes[ticker] = ticker_data['Volume']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process 2 tickers: ['BRK.B (coverage: 0.0%)', 'BF.B (coverage: 0.0%)']...\n",
      "Fetching company information...\n",
      "  Processed 20/515 tickers\n",
      "  Processed 40/515 tickers\n",
      "  Processed 60/515 tickers\n",
      "  Processed 80/515 tickers\n",
      "  Processed 100/515 tickers\n",
      "  Processed 120/515 tickers\n",
      "  Processed 140/515 tickers\n",
      "  Processed 160/515 tickers\n",
      "  Processed 180/515 tickers\n",
      "  Processed 200/515 tickers\n",
      "  Processed 220/515 tickers\n",
      "  Processed 240/515 tickers\n",
      "  Processed 260/515 tickers\n",
      "  Processed 280/515 tickers\n",
      "  Processed 300/515 tickers\n",
      "  Processed 320/515 tickers\n",
      "  Processed 340/515 tickers\n",
      "  Processed 360/515 tickers\n",
      "  Processed 380/515 tickers\n",
      "  Processed 400/515 tickers\n",
      "  Processed 420/515 tickers\n",
      "  Processed 440/515 tickers\n",
      "  Processed 460/515 tickers\n",
      "  Processed 480/515 tickers\n",
      "  Processed 500/515 tickers\n",
      "After volume filter (>=1,000,000): 433 stocks (82 removed)\n",
      "After market cap filter (>=$5.0B): 433 stocks\n",
      "After price filter (>=$10.0): 431 stocks\n",
      "After data completeness filter: 431 stocks\n",
      "Applying sector caps (15 per sector)...\n",
      "\n",
      "Final universe: 165 stocks\n",
      "Sector distribution:\n",
      "  Technology: 15\n",
      "  Consumer Cyclical: 15\n",
      "  Communication Services: 15\n",
      "  Financial Services: 15\n",
      "  Consumer Defensive: 15\n",
      "  Healthcare: 15\n",
      "  Energy: 15\n",
      "  Industrials: 15\n",
      "  Basic Materials: 15\n",
      "  Utilities: 15\n",
      "  Real Estate: 15\n",
      "\n",
      "=== UNIVERSE ANALYSIS ===\n",
      "Total stocks: 165\n",
      "Total market cap: $45.5T\n",
      "Median market cap: $112.8B\n",
      "Median price: $143.18\n",
      "Median daily volume: 5,043,321\n",
      "\n",
      "Top 10 stocks by market cap:\n",
      "                        sector      marketCap   price       avg_vol\n",
      "ticker                                                             \n",
      "NVDA                Technology  3885920157696  159.34  2.496269e+08\n",
      "MSFT                Technology  3707648344064  498.84  2.286549e+07\n",
      "AAPL                Technology  3189540126720  213.55  6.109784e+07\n",
      "AMZN         Consumer Cyclical  2371809968128  223.41  4.841077e+07\n",
      "GOOGL   Communication Services  2184126005248  179.53  4.043679e+07\n",
      "GOOG    Communication Services  2184113422336  180.55  2.683696e+07\n",
      "META    Communication Services  1807828516864  719.01  1.613075e+07\n",
      "AVGO                Technology  1294300872704  275.18  2.732872e+07\n",
      "TSLA         Consumer Cyclical  1015729750016  315.35  1.237245e+08\n",
      "JPM         Financial Services   822610624512  296.00  1.045405e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drish\\AppData\\Local\\Temp\\ipykernel_26588\\3139944104.py:189: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.nlargest(max_per_sector, \"marketCap\"))\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "def build_universe(raw_tickers: List[str], \n",
    "                   lookback_days: int = 90,\n",
    "                   min_avg_vol: float = 500_000, \n",
    "                   min_mktcap: float = 1e9,\n",
    "                   min_price: float = 5.0,\n",
    "                   max_per_sector: int = 30,\n",
    "                   min_data_coverage: float = 0.8,\n",
    "                   verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a filtered stock universe from raw tickers based on liquidity, \n",
    "    market cap, price, and sector diversification criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw_tickers : List[str]\n",
    "        List of ticker symbols to filter\n",
    "    lookback_days : int\n",
    "        Number of days to look back for volume calculations\n",
    "    min_avg_vol : float\n",
    "        Minimum average daily volume\n",
    "    min_mktcap : float\n",
    "        Minimum market capitalization\n",
    "    min_price : float\n",
    "        Minimum stock price\n",
    "    max_per_sector : int\n",
    "        Maximum stocks per sector\n",
    "    min_data_coverage : float\n",
    "        Minimum fraction of data points required (0.8 = 80%)\n",
    "    verbose : bool\n",
    "        Print progress information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Filtered universe with ticker, sector, market cap, price, and volume data\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Building universe from {len(raw_tickers)} tickers...\")\n",
    "    \n",
    "    # 1) Download data with error handling\n",
    "    end = datetime.today()\n",
    "    start = end - timedelta(days=int(lookback_days * 1.3))\n",
    "    \n",
    "    try:\n",
    "        data = yf.download(\n",
    "            raw_tickers,\n",
    "            start=start.strftime(\"%Y-%m-%d\"),\n",
    "            end=end.strftime(\"%Y-%m-%d\"),\n",
    "            auto_adjust=True,\n",
    "            group_by=\"ticker\",\n",
    "            threads=True,\n",
    "            progress=verbose\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Handle single ticker case\n",
    "    if len(raw_tickers) == 1:\n",
    "        ticker = raw_tickers[0]\n",
    "        data = {ticker: data}\n",
    "    \n",
    "    # 2) Extract adjusted price & volume with data quality checks\n",
    "    prices = pd.DataFrame()\n",
    "    volumes = pd.DataFrame()\n",
    "    \n",
    "    failed_tickers = []\n",
    "    for ticker in raw_tickers:\n",
    "        try:\n",
    "            if ticker in data and not data[ticker].empty:\n",
    "                # Check data coverage\n",
    "                ticker_data = data[ticker]\n",
    "                coverage = ticker_data['Close'].notna().sum() / len(ticker_data)\n",
    "                \n",
    "                if coverage >= min_data_coverage:\n",
    "                    prices[ticker] = ticker_data['Close']\n",
    "                    volumes[ticker] = ticker_data['Volume']\n",
    "                else:\n",
    "                    failed_tickers.append(f\"{ticker} (coverage: {coverage:.1%})\")\n",
    "            else:\n",
    "                failed_tickers.append(f\"{ticker} (no data)\")\n",
    "        except Exception as e:\n",
    "            failed_tickers.append(f\"{ticker} (error: {str(e)[:50]})\")\n",
    "    \n",
    "    if failed_tickers and verbose:\n",
    "        print(f\"Failed to process {len(failed_tickers)} tickers: {failed_tickers[:5]}...\")\n",
    "    \n",
    "    if prices.empty:\n",
    "        print(\"No valid price data found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 3) Compute stats with error handling\n",
    "    try:\n",
    "        avg_vol = volumes.rolling(window=lookback_days, min_periods=int(lookback_days * 0.7)).mean().iloc[-1]\n",
    "        last_price = prices.iloc[-1]\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        price_volatility = prices.pct_change().rolling(window=lookback_days).std().iloc[-1] * np.sqrt(252)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error computing statistics: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 4) Get market-cap & sector info with batch processing\n",
    "    if verbose:\n",
    "        print(\"Fetching company information...\")\n",
    "    \n",
    "    infos = {}\n",
    "    valid_tickers = prices.columns.tolist()\n",
    "    \n",
    "    for i, ticker in enumerate(valid_tickers):\n",
    "        try:\n",
    "            info = yf.Ticker(ticker).info\n",
    "            infos[ticker] = {\n",
    "                \"marketCap\": info.get(\"marketCap\", np.nan),\n",
    "                \"sector\": info.get(\"sector\", \"Unknown\"),\n",
    "                \"industry\": info.get(\"industry\", \"Unknown\"),\n",
    "                \"country\": info.get(\"country\", \"Unknown\")\n",
    "            }\n",
    "            \n",
    "            if verbose and (i + 1) % 20 == 0:\n",
    "                print(f\"  Processed {i + 1}/{len(valid_tickers)} tickers\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            infos[ticker] = {\n",
    "                \"marketCap\": np.nan,\n",
    "                \"sector\": \"Unknown\",\n",
    "                \"industry\": \"Unknown\", \n",
    "                \"country\": \"Unknown\"\n",
    "            }\n",
    "    \n",
    "    info_df = pd.DataFrame.from_dict(infos, orient=\"index\")\n",
    "    \n",
    "    # 5) Assemble and filter\n",
    "    df = pd.DataFrame({\n",
    "        \"avg_vol\": avg_vol,\n",
    "        \"price\": last_price,\n",
    "        \"volatility\": price_volatility\n",
    "    }).join(info_df)\n",
    "    \n",
    "    # Apply filters\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Volume filter\n",
    "    vol_filter = df[\"avg_vol\"] >= min_avg_vol\n",
    "    df = df[vol_filter]\n",
    "    if verbose:\n",
    "        print(f\"After volume filter (>={min_avg_vol:,.0f}): {len(df)} stocks ({initial_count - len(df)} removed)\")\n",
    "    \n",
    "    # Market cap filter\n",
    "    mktcap_filter = df[\"marketCap\"] >= min_mktcap\n",
    "    df = df[mktcap_filter]\n",
    "    if verbose:\n",
    "        print(f\"After market cap filter (>=${min_mktcap/1e9:.1f}B): {len(df)} stocks\")\n",
    "    \n",
    "    # Price filter\n",
    "    price_filter = df[\"price\"] >= min_price\n",
    "    df = df[price_filter]\n",
    "    if verbose:\n",
    "        print(f\"After price filter (>=${min_price}): {len(df)} stocks\")\n",
    "    \n",
    "    # Remove stocks with missing key data\n",
    "    complete_data_filter = df[[\"marketCap\", \"avg_vol\", \"price\"]].notna().all(axis=1)\n",
    "    df = df[complete_data_filter]\n",
    "    if verbose:\n",
    "        print(f\"After data completeness filter: {len(df)} stocks\")\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No stocks passed all filters!\")\n",
    "        return df\n",
    "    \n",
    "    # 6) Cap each sector to top N by market-cap\n",
    "    if verbose:\n",
    "        print(f\"Applying sector caps ({max_per_sector} per sector)...\")\n",
    "    \n",
    "    universe = (\n",
    "        df\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"ticker\"})\n",
    "        .groupby(\"sector\", group_keys=False)\n",
    "        .apply(lambda g: g.nlargest(max_per_sector, \"marketCap\"))\n",
    "        .set_index(\"ticker\")\n",
    "        .sort_values(\"marketCap\", ascending=False)\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFinal universe: {len(universe)} stocks\")\n",
    "        print(f\"Sector distribution:\")\n",
    "        sector_counts = universe[\"sector\"].value_counts()\n",
    "        for sector, count in sector_counts.items():\n",
    "            print(f\"  {sector}: {count}\")\n",
    "    \n",
    "    return universe\n",
    "\n",
    "def analyze_universe(universe: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the characteristics of the filtered universe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    universe : pd.DataFrame\n",
    "        The filtered stock universe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Analysis results including sector distribution, size distribution, etc.\n",
    "    \"\"\"\n",
    "    if universe.empty:\n",
    "        return {}\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_stocks\": len(universe),\n",
    "        \"total_market_cap\": universe[\"marketCap\"].sum(),\n",
    "        \"median_market_cap\": universe[\"marketCap\"].median(),\n",
    "        \"median_price\": universe[\"price\"].median(),\n",
    "        \"median_volume\": universe[\"avg_vol\"].median(),\n",
    "        \"sector_distribution\": universe[\"sector\"].value_counts().to_dict(),\n",
    "        \"size_distribution\": {\n",
    "            \"Large Cap (>10B)\": (universe[\"marketCap\"] > 10e9).sum(),\n",
    "            \"Mid Cap (1B-10B)\": ((universe[\"marketCap\"] >= 1e9) & (universe[\"marketCap\"] <= 10e9)).sum(),\n",
    "            \"Small Cap (<1B)\": (universe[\"marketCap\"] < 1e9).sum()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Example usage with error handling\n",
    "if __name__ == \"__main__\":\n",
    "    # Example NASDAQ 100 tickers (partial list for demo)\n",
    "    nasdaq_sample = [\n",
    "        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META', 'AVGO', 'COST', 'NFLX',\n",
    "        'ADBE', 'PEP', 'TMUS', 'CSCO', 'CMCSA', 'INTC', 'TXN', 'QCOM', 'AMGN', 'INTU'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Build universe\n",
    "        universe = build_universe(\n",
    "            merged_tickers_unique['Ticker'].to_list(),\n",
    "            lookback_days=90,\n",
    "            min_avg_vol=1_000_000,\n",
    "            min_mktcap=5e9,\n",
    "            min_price=10.0,\n",
    "            max_per_sector=15,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        if not universe.empty:\n",
    "            # Analyze results\n",
    "            analysis = analyze_universe(universe)\n",
    "            \n",
    "            print(f\"\\n=== UNIVERSE ANALYSIS ===\")\n",
    "            print(f\"Total stocks: {analysis['total_stocks']}\")\n",
    "            print(f\"Total market cap: ${analysis['total_market_cap']/1e12:.1f}T\")\n",
    "            print(f\"Median market cap: ${analysis['median_market_cap']/1e9:.1f}B\")\n",
    "            print(f\"Median price: ${analysis['median_price']:.2f}\")\n",
    "            print(f\"Median daily volume: {analysis['median_volume']:,.0f}\")\n",
    "            \n",
    "            print(\"\\nTop 10 stocks by market cap:\")\n",
    "            print(universe.head(10)[['sector', 'marketCap', 'price', 'avg_vol']].round(2))\n",
    "            \n",
    "        else:\n",
    "            print(\"No stocks in final universe!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c85fe095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_vol</th>\n",
       "      <th>price</th>\n",
       "      <th>volatility</th>\n",
       "      <th>marketCap</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>2.496269e+08</td>\n",
       "      <td>159.339996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3885920157696</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>2.286549e+07</td>\n",
       "      <td>498.839996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3707648344064</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software - Infrastructure</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>6.109784e+07</td>\n",
       "      <td>213.550003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3189540126720</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Electronics</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>4.841077e+07</td>\n",
       "      <td>223.410004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2371809968128</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>Internet Retail</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>4.043679e+07</td>\n",
       "      <td>179.529999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2184126005248</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Internet Content &amp; Information</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             avg_vol       price  volatility      marketCap  \\\n",
       "ticker                                                        \n",
       "NVDA    2.496269e+08  159.339996         NaN  3885920157696   \n",
       "MSFT    2.286549e+07  498.839996         NaN  3707648344064   \n",
       "AAPL    6.109784e+07  213.550003         NaN  3189540126720   \n",
       "AMZN    4.841077e+07  223.410004         NaN  2371809968128   \n",
       "GOOGL   4.043679e+07  179.529999         NaN  2184126005248   \n",
       "\n",
       "                        sector                        industry        country  \n",
       "ticker                                                                         \n",
       "NVDA                Technology                  Semiconductors  United States  \n",
       "MSFT                Technology       Software - Infrastructure  United States  \n",
       "AAPL                Technology            Consumer Electronics  United States  \n",
       "AMZN         Consumer Cyclical                 Internet Retail  United States  \n",
       "GOOGL   Communication Services  Internet Content & Information  United States  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0096e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comprehensive universe analysis...\n",
      "\n",
      "=== UNIVERSE DASHBOARD ===\n",
      "\n",
      "📊 PORTFOLIO OVERVIEW\n",
      "Total Stocks: 165\n",
      "Total Market Cap: $45.5T\n",
      "Median Market Cap: $112.8B\n",
      "Median Price: $143.18\n",
      "Median Daily Volume: 5,043,321\n",
      "\n",
      "🏭 SECTOR BREAKDOWN\n",
      "Technology: 15 stocks (9.1%)\n",
      "Consumer Cyclical: 15 stocks (9.1%)\n",
      "Communication Services: 15 stocks (9.1%)\n",
      "Financial Services: 15 stocks (9.1%)\n",
      "Consumer Defensive: 15 stocks (9.1%)\n",
      "Healthcare: 15 stocks (9.1%)\n",
      "Energy: 15 stocks (9.1%)\n",
      "Industrials: 15 stocks (9.1%)\n",
      "Basic Materials: 15 stocks (9.1%)\n",
      "Utilities: 15 stocks (9.1%)\n",
      "Real Estate: 15 stocks (9.1%)\n",
      "\n",
      "📈 SIZE DISTRIBUTION\n",
      "Large Cap (>$10B): 165 stocks\n",
      "Mid Cap ($1B-$10B): 0 stocks\n",
      "Small Cap (<$1B): 0 stocks\n",
      "\n",
      "⚠️  CONCENTRATION ANALYSIS\n",
      "Top 1 Stock: 8.5% of total market cap\n",
      "Top 3 Stocks: 23.7% of total market cap\n",
      "Top 5 Stocks: 33.7% of total market cap\n",
      "Herfindahl Index: 0.033 (lower = more diversified)\n",
      "\n",
      "🟢 LOW CONCENTRATION RISK: Well diversified\n",
      "\n",
      "🎯 RECOMMENDATIONS\n",
      "• Consider sector caps of 25-30% to reduce tech concentration\n",
      "• Monitor liquidity: 126 stocks have <10M daily volume\n",
      "• Review price impact: 13 stocks are >$500/share\n",
      "• Rebalance frequency: Monthly given high correlation in mega-caps\n",
      "\n",
      "\n",
      "=== SECTOR REBALANCING SUGGESTIONS ===\n",
      "                        current_weight  target_weight  difference    action\n",
      "Real Estate                   0.017004       0.090909    0.073905  INCREASE\n",
      "Basic Materials               0.018554       0.090909    0.072355  INCREASE\n",
      "Utilities                     0.020221       0.090909    0.070688  INCREASE\n",
      "Energy                        0.032513       0.090909    0.058396  INCREASE\n",
      "Industrials                   0.045796       0.090909    0.045114  INCREASE\n",
      "Consumer Defensive            0.064269       0.090909    0.026640  INCREASE\n",
      "Healthcare                    0.079003       0.090909    0.011906      HOLD\n",
      "Financial Services            0.098222       0.090909   -0.007313      HOLD\n",
      "Consumer Cyclical             0.112313       0.090909   -0.021403  DECREASE\n",
      "Communication Services        0.177407       0.090909   -0.086498  DECREASE\n",
      "Technology                    0.334700       0.090909   -0.243791  DECREASE\n",
      "\n",
      "=== CONCENTRATION OPTIMIZATION ===\n",
      "  ticker      sector  current_weight  suggested_reduction  action\n",
      "0   NVDA  Technology        0.085391               0.0347  REDUCE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def analyze_concentration(universe: pd.DataFrame, top_n: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze concentration risk in the universe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    universe : pd.DataFrame\n",
    "        The filtered stock universe\n",
    "    top_n : int\n",
    "        Number of top stocks to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Concentration metrics\n",
    "    \"\"\"\n",
    "    total_mcap = universe['marketCap'].sum()\n",
    "    universe_sorted = universe.sort_values('marketCap', ascending=False)\n",
    "    \n",
    "    # Calculate cumulative percentages\n",
    "    cumulative_pct = universe_sorted['marketCap'].cumsum() / total_mcap\n",
    "    \n",
    "    concentration_metrics = {\n",
    "        'total_market_cap': total_mcap,\n",
    "        'top_1_pct': cumulative_pct.iloc[0],\n",
    "        'top_3_pct': cumulative_pct.iloc[2] if len(cumulative_pct) >= 3 else cumulative_pct.iloc[-1],\n",
    "        'top_5_pct': cumulative_pct.iloc[4] if len(cumulative_pct) >= 5 else cumulative_pct.iloc[-1],\n",
    "        'top_10_pct': cumulative_pct.iloc[9] if len(cumulative_pct) >= 10 else cumulative_pct.iloc[-1],\n",
    "        'herfindahl_index': ((universe['marketCap'] / total_mcap) ** 2).sum()\n",
    "    }\n",
    "    \n",
    "    return concentration_metrics\n",
    "\n",
    "def suggest_rebalancing(universe: pd.DataFrame, target_sector_weights: Dict = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Suggest rebalancing to achieve target sector weights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    universe : pd.DataFrame\n",
    "        The filtered stock universe\n",
    "    target_sector_weights : Dict\n",
    "        Target weights by sector (if None, uses equal weight)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Rebalancing suggestions\n",
    "    \"\"\"\n",
    "    if target_sector_weights is None:\n",
    "        # Equal weight across sectors\n",
    "        unique_sectors = universe['sector'].unique()\n",
    "        target_sector_weights = {sector: 1.0/len(unique_sectors) for sector in unique_sectors}\n",
    "    \n",
    "    current_weights = universe.groupby('sector')['marketCap'].sum()\n",
    "    current_weights = current_weights / current_weights.sum()\n",
    "    \n",
    "    rebalancing = pd.DataFrame({\n",
    "        'current_weight': current_weights,\n",
    "        'target_weight': pd.Series(target_sector_weights),\n",
    "        'difference': pd.Series(target_sector_weights) - current_weights\n",
    "    }).fillna(0)\n",
    "    \n",
    "    rebalancing['action'] = rebalancing['difference'].apply(\n",
    "        lambda x: 'INCREASE' if x > 0.02 else ('DECREASE' if x < -0.02 else 'HOLD')\n",
    "    )\n",
    "    \n",
    "    return rebalancing.sort_values('difference', ascending=False)\n",
    "\n",
    "def calculate_risk_metrics(universe: pd.DataFrame, prices_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate risk metrics for each stock in the universe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    universe : pd.DataFrame\n",
    "        The filtered stock universe\n",
    "    prices_df : pd.DataFrame\n",
    "        Historical prices (from your original data)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Risk metrics for each stock\n",
    "    \"\"\"\n",
    "    risk_metrics = []\n",
    "    \n",
    "    for ticker in universe.index:\n",
    "        if ticker in prices_df.columns:\n",
    "            prices = prices_df[ticker].dropna()\n",
    "            returns = prices.pct_change().dropna()\n",
    "            \n",
    "            if len(returns) > 30:  # Minimum data requirement\n",
    "                volatility = returns.std() * np.sqrt(252)  # Annualized\n",
    "                max_drawdown = calculate_max_drawdown(prices)\n",
    "                sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0\n",
    "                \n",
    "                risk_metrics.append({\n",
    "                    'ticker': ticker,\n",
    "                    'volatility': volatility,\n",
    "                    'max_drawdown': max_drawdown,\n",
    "                    'sharpe_ratio': sharpe_ratio,\n",
    "                    'beta': calculate_beta(returns, prices_df)  # vs market proxy\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(risk_metrics).set_index('ticker')\n",
    "\n",
    "def calculate_max_drawdown(prices: pd.Series) -> float:\n",
    "    \"\"\"Calculate maximum drawdown for a price series.\"\"\"\n",
    "    peak = prices.expanding().max()\n",
    "    drawdown = (prices - peak) / peak\n",
    "    return drawdown.min()\n",
    "\n",
    "def calculate_beta(stock_returns: pd.Series, market_prices: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculate beta vs market (using average of all stocks as market proxy).\"\"\"\n",
    "    try:\n",
    "        market_returns = market_prices.mean(axis=1).pct_change().dropna()\n",
    "        \n",
    "        # Align dates\n",
    "        common_dates = stock_returns.index.intersection(market_returns.index)\n",
    "        stock_aligned = stock_returns.loc[common_dates]\n",
    "        market_aligned = market_returns.loc[common_dates]\n",
    "        \n",
    "        if len(stock_aligned) > 30:\n",
    "            covariance = np.cov(stock_aligned, market_aligned)[0, 1]\n",
    "            market_variance = np.var(market_aligned)\n",
    "            return covariance / market_variance if market_variance > 0 else 1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "def optimize_sector_allocation(universe: pd.DataFrame, max_sector_weight: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimize sector allocation to reduce concentration risk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    universe : pd.DataFrame\n",
    "        The filtered stock universe\n",
    "    max_sector_weight : float\n",
    "        Maximum weight for any single sector\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Optimized allocation suggestions\n",
    "    \"\"\"\n",
    "    # Current sector weights\n",
    "    sector_weights = universe.groupby('sector')['marketCap'].sum()\n",
    "    sector_weights = sector_weights / sector_weights.sum()\n",
    "    \n",
    "    # Identify over-allocated sectors\n",
    "    over_allocated = sector_weights[sector_weights > max_sector_weight]\n",
    "    \n",
    "    if len(over_allocated) == 0:\n",
    "        return pd.DataFrame({'message': ['No rebalancing needed']})\n",
    "    \n",
    "    # Calculate adjustments needed\n",
    "    adjustments = []\n",
    "    for sector, weight in over_allocated.items():\n",
    "        excess = weight - max_sector_weight\n",
    "        sector_stocks = universe[universe['sector'] == sector].sort_values('marketCap', ascending=False)\n",
    "        \n",
    "        # Suggest reducing positions in largest stocks\n",
    "        cumulative_excess = 0\n",
    "        for ticker, row in sector_stocks.iterrows():\n",
    "            if cumulative_excess < excess:\n",
    "                stock_weight = row['marketCap'] / universe['marketCap'].sum()\n",
    "                reduction = min(stock_weight * 0.5, excess - cumulative_excess)  # Reduce by up to 50%\n",
    "                \n",
    "                adjustments.append({\n",
    "                    'ticker': ticker,\n",
    "                    'sector': sector,\n",
    "                    'current_weight': stock_weight,\n",
    "                    'suggested_reduction': reduction,\n",
    "                    'action': 'REDUCE'\n",
    "                })\n",
    "                cumulative_excess += reduction\n",
    "    \n",
    "    return pd.DataFrame(adjustments)\n",
    "\n",
    "def create_universe_dashboard(universe: pd.DataFrame, analysis: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Create a comprehensive dashboard summary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    universe : pd.DataFrame\n",
    "        The filtered stock universe\n",
    "    analysis : Dict\n",
    "        Analysis results from analyze_universe()\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted dashboard text\n",
    "    \"\"\"\n",
    "    dashboard = f\"\"\"\n",
    "=== UNIVERSE DASHBOARD ===\n",
    "\n",
    "📊 PORTFOLIO OVERVIEW\n",
    "Total Stocks: {analysis['total_stocks']}\n",
    "Total Market Cap: ${analysis['total_market_cap']/1e12:.1f}T\n",
    "Median Market Cap: ${analysis['median_market_cap']/1e9:.1f}B\n",
    "Median Price: ${analysis['median_price']:.2f}\n",
    "Median Daily Volume: {analysis['median_volume']:,.0f}\n",
    "\n",
    "🏭 SECTOR BREAKDOWN\n",
    "\"\"\"\n",
    "    \n",
    "    for sector, count in analysis['sector_distribution'].items():\n",
    "        pct = count / analysis['total_stocks'] * 100\n",
    "        dashboard += f\"{sector}: {count} stocks ({pct:.1f}%)\\n\"\n",
    "    \n",
    "    dashboard += f\"\"\"\n",
    "📈 SIZE DISTRIBUTION\n",
    "Large Cap (>$10B): {analysis['size_distribution']['Large Cap (>10B)']} stocks\n",
    "Mid Cap ($1B-$10B): {analysis['size_distribution']['Mid Cap (1B-10B)']} stocks\n",
    "Small Cap (<$1B): {analysis['size_distribution']['Small Cap (<1B)']} stocks\n",
    "\n",
    "⚠️  CONCENTRATION ANALYSIS\n",
    "\"\"\"\n",
    "    \n",
    "    # Add concentration metrics\n",
    "    concentration = analyze_concentration(universe)\n",
    "    dashboard += f\"Top 1 Stock: {concentration['top_1_pct']:.1%} of total market cap\\n\"\n",
    "    dashboard += f\"Top 3 Stocks: {concentration['top_3_pct']:.1%} of total market cap\\n\"\n",
    "    dashboard += f\"Top 5 Stocks: {concentration['top_5_pct']:.1%} of total market cap\\n\"\n",
    "    dashboard += f\"Herfindahl Index: {concentration['herfindahl_index']:.3f} (lower = more diversified)\\n\"\n",
    "    \n",
    "    # Risk assessment\n",
    "    if concentration['top_3_pct'] > 0.5:\n",
    "        dashboard += \"\\n🔴 HIGH CONCENTRATION RISK: Top 3 stocks >50% of portfolio\\n\"\n",
    "    elif concentration['top_3_pct'] > 0.3:\n",
    "        dashboard += \"\\n🟡 MODERATE CONCENTRATION RISK: Top 3 stocks >30% of portfolio\\n\"\n",
    "    else:\n",
    "        dashboard += \"\\n🟢 LOW CONCENTRATION RISK: Well diversified\\n\"\n",
    "    \n",
    "    dashboard += f\"\"\"\n",
    "🎯 RECOMMENDATIONS\n",
    "• Consider sector caps of 25-30% to reduce tech concentration\n",
    "• Monitor liquidity: {(universe['avg_vol'] < 10_000_000).sum()} stocks have <10M daily volume\n",
    "• Review price impact: {(universe['price'] > 500).sum()} stocks are >$500/share\n",
    "• Rebalance frequency: Monthly given high correlation in mega-caps\n",
    "\"\"\"\n",
    "    \n",
    "    return dashboard\n",
    "\n",
    "# Example usage with your universe\n",
    "def run_comprehensive_analysis(universe: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Run a comprehensive analysis of the universe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    universe : pd.DataFrame\n",
    "        The filtered stock universe\n",
    "    \"\"\"\n",
    "    print(\"Running comprehensive universe analysis...\")\n",
    "    \n",
    "    # Basic analysis\n",
    "    analysis = {\n",
    "        \"total_stocks\": len(universe),\n",
    "        \"total_market_cap\": universe[\"marketCap\"].sum(),\n",
    "        \"median_market_cap\": universe[\"marketCap\"].median(),\n",
    "        \"median_price\": universe[\"price\"].median(),\n",
    "        \"median_volume\": universe[\"avg_vol\"].median(),\n",
    "        \"sector_distribution\": universe[\"sector\"].value_counts().to_dict(),\n",
    "        \"size_distribution\": {\n",
    "            \"Large Cap (>10B)\": (universe[\"marketCap\"] > 10e9).sum(),\n",
    "            \"Mid Cap (1B-10B)\": ((universe[\"marketCap\"] >= 1e9) & (universe[\"marketCap\"] <= 10e9)).sum(),\n",
    "            \"Small Cap (<1B)\": (universe[\"marketCap\"] < 1e9).sum()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print dashboard\n",
    "    dashboard = create_universe_dashboard(universe, analysis)\n",
    "    print(dashboard)\n",
    "    \n",
    "    # Sector rebalancing suggestions\n",
    "    print(\"\\n=== SECTOR REBALANCING SUGGESTIONS ===\")\n",
    "    rebalancing = suggest_rebalancing(universe)\n",
    "    print(rebalancing)\n",
    "    \n",
    "    # Concentration optimization\n",
    "    print(\"\\n=== CONCENTRATION OPTIMIZATION ===\")\n",
    "    optimization = optimize_sector_allocation(universe, max_sector_weight=0.3)\n",
    "    if 'message' not in optimization.columns:\n",
    "        print(optimization)\n",
    "    else:\n",
    "        print(\"✅ Sector allocation within recommended limits\")\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming 'universe' is your filtered universe DataFrame\n",
    "    run_comprehensive_analysis(universe)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed279336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Technology', 'Consumer Cyclical', 'Communication Services',\n",
       "       'Financial Services', 'Consumer Defensive', 'Healthcare', 'Energy',\n",
       "       'Industrials', 'Basic Materials', 'Utilities', 'Real Estate'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe['sector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d7665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe.to_csv(\"ticker_universe.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0626d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculate_summary.ipynb', 'calculate_trades.ipynb', 'data_prep.ipynb', 'gdrive-creds.json', 'gspread.ipynb', 'ibkr.ipynb', 'ml_train.ipynb', 'stocks_df_combined_2025_07_03.parquet.brotli', 'ticker_universe.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>volatility</th>\n",
       "      <th>marketCap</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_vol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2.496269e+08</th>\n",
       "      <td>159.339996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3885920157696</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.286549e+07</th>\n",
       "      <td>498.839996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3707648344064</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software - Infrastructure</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.109784e+07</th>\n",
       "      <td>213.550003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3189540126720</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Consumer Electronics</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.841077e+07</th>\n",
       "      <td>223.410004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2371809968128</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>Internet Retail</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.043679e+07</th>\n",
       "      <td>179.529999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2184126005248</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Internet Content &amp; Information</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   price  volatility      marketCap                  sector  \\\n",
       "avg_vol                                                                       \n",
       "2.496269e+08  159.339996         NaN  3885920157696              Technology   \n",
       "2.286549e+07  498.839996         NaN  3707648344064              Technology   \n",
       "6.109784e+07  213.550003         NaN  3189540126720              Technology   \n",
       "4.841077e+07  223.410004         NaN  2371809968128       Consumer Cyclical   \n",
       "4.043679e+07  179.529999         NaN  2184126005248  Communication Services   \n",
       "\n",
       "                                    industry        country  \n",
       "avg_vol                                                      \n",
       "2.496269e+08                  Semiconductors  United States  \n",
       "2.286549e+07       Software - Infrastructure  United States  \n",
       "6.109784e+07            Consumer Electronics  United States  \n",
       "4.841077e+07                 Internet Retail  United States  \n",
       "4.043679e+07  Internet Content & Information  United States  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\".\"))        # should list ticker_universe.csv\n",
    "\n",
    "# And to load it later:\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"ticker_universe.csv\", index_col=0)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
