{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20f6a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Stock Prediction Decision Making Notebook\n",
      "==================================================\n",
      "✅ Paths configured:\n",
      "   Project root: /Users/sagardhal/Desktop/Practice/personal-stock\n",
      "   App code: /Users/sagardhal/Desktop/Practice/personal-stock/app\n",
      "   Artifacts: /Users/sagardhal/Desktop/Practice/personal-stock/artifacts\n",
      "   Data: /Users/sagardhal/Desktop/Practice/personal-stock/data\n"
     ]
    }
   ],
   "source": [
    "# Stock Prediction Decision Making Notebook\n",
    "# Interactive notebook for making daily trading decisions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "\n",
    "# Configuration\n",
    "TOP_N_PICKS = 10\n",
    "INVESTMENT_AMOUNT = 1000\n",
    "\n",
    "# Path configuration - adjust for notebook location\n",
    "# Since notebook is in eda-notebooks/, go up one level to project root\n",
    "PROJECT_ROOT = Path(\"..\").resolve()  # Go up one level from eda-notebooks/\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "APP_DIR = PROJECT_ROOT / \"app\"\n",
    "\n",
    "print(\"📊 Stock Prediction Decision Making Notebook\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup paths\n",
    "if str(APP_DIR) not in sys.path:\n",
    "    sys.path.append(str(APP_DIR))\n",
    "\n",
    "print(f\"✅ Paths configured:\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   App code: {APP_DIR}\")\n",
    "print(f\"   Artifacts: {ARTIFACTS_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ef7bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Stock Prediction Decision Making Notebook\n",
      "==================================================\n",
      "✅ Paths configured:\n",
      "   Project root: /Users/sagardhal/Desktop/Practice/personal-stock\n",
      "   App code: /Users/sagardhal/Desktop/Practice/personal-stock/app\n",
      "   Artifacts: /Users/sagardhal/Desktop/Practice/personal-stock/artifacts\n",
      "   Data: /Users/sagardhal/Desktop/Practice/personal-stock/data\n"
     ]
    }
   ],
   "source": [
    "# Stock Prediction Decision Making Notebook\n",
    "# Interactive notebook for making daily trading decisions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "TOP_N_PICKS = 10\n",
    "INVESTMENT_AMOUNT = 1000\n",
    "\n",
    "# Path configuration - adjust for notebook location\n",
    "# Since notebook is in eda-notebooks/, go up one level to project root\n",
    "PROJECT_ROOT = Path(\"..\").resolve()  # Go up one level from eda-notebooks/\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "APP_DIR = PROJECT_ROOT / \"app\"\n",
    "\n",
    "print(\"📊 Stock Prediction Decision Making Notebook\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup paths\n",
    "if str(APP_DIR) not in sys.path:\n",
    "    sys.path.append(str(APP_DIR))\n",
    "\n",
    "print(f\"✅ Paths configured:\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   App code: {APP_DIR}\")\n",
    "print(f\"   Artifacts: {ARTIFACTS_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08407a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# go up one level to the root directory\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "\n",
    "\n",
    "# # add project root (parent of notebooks/) to sys.path\n",
    "# project_root = Path.cwd().parent\n",
    "# sys.path.insert(0, str(project_root))\n",
    "\n",
    "from app.predictions import (\n",
    "    load_latest_data,\n",
    "    load_model_and_features,\n",
    "    PredictionComparator,\n",
    "    _TransformAdapter,\n",
    ")\n",
    "\n",
    "# If you also need TrainModel directly in the notebook:\n",
    "from app.train_model_new import TrainModel   # ✅ absolute package import\n",
    "\n",
    "from app.stock_pipeline import StockDataPipeline\n",
    "\n",
    "\n",
    "# Import modules\n",
    "try:\n",
    "    from app.predictions import load_model_and_features, _TransformAdapter\n",
    "    from app.train_model_new import TrainModel\n",
    "    from app.stock_pipeline import StockDataPipeline\n",
    "    print(\"✅ All modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Make sure your app directory contains all required modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49c429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a87c1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 1: LOAD TICKERS\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load Tickers\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 1: LOAD TICKERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "tickers = (pd.read_csv(\"/Users/sagardhal/Desktop/Practice/personal-stock/ticker/spx_ndx_liq_top250_latest.csv\")['Ticker']\n",
    "        #.head(5)\n",
    "        .tolist())\n",
    "tickers\n",
    "len(tickers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e3379b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 2: CHECK REQUIREMENTS\n",
      "==================================================\n",
      "🤖 Models found: 5\n",
      "   - best_rf_model.joblib\n",
      "   - random_forest_train_valid_20250904_112953.joblib\n",
      "   - random_forest_train_only_20250904_000839.joblib\n",
      "   - random_forest_train_only_20250904_112906.joblib\n",
      "   - random_forest_train_valid_20250904_000856.joblib\n",
      "📊 Data files found: 1\n",
      "   - stock_data_combined_20250904_071145.parquet\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check Requirements\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: CHECK REQUIREMENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for trained models\n",
    "artifacts_files = list(ARTIFACTS_DIR.glob(\"*.joblib\")) + list(ARTIFACTS_DIR.glob(\"*.pkl\"))\n",
    "print(f\"🤖 Models found: {len(artifacts_files)}\")\n",
    "for f in artifacts_files:\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "# Check for data files\n",
    "data_files = list(DATA_DIR.glob(\"*.parquet\")) if DATA_DIR.exists() else []\n",
    "print(f\"📊 Data files found: {len(data_files)}\")\n",
    "for f in data_files[:3]:\n",
    "    print(f\"   - {f.name}\")\n",
    "if len(data_files) > 3:\n",
    "    print(f\"   ... and {len(data_files) - 3} more files\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4af18aa3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def fetch_fresh_ohlcv_fast(tickers, days_back=30):\n",
    "    \"\"\"\n",
    "    Most efficient approach: one batched yfinance call for all tickers,\n",
    "    then reshape to long format. Prints per-ticker status after download.\n",
    "    \"\"\"\n",
    "    # Clean + de-dupe\n",
    "    tickers = [t.strip().upper() for t in tickers if t and str(t).strip()]\n",
    "    tickers = list(dict.fromkeys(tickers))\n",
    "    if not tickers:\n",
    "        raise ValueError(\"No valid tickers provided\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2 (FAST): FETCH FRESH DATA — Single batched call\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📊 Fetching last {days_back} days of data for {len(tickers)} tickers...\")\n",
    "    # Small pad for weekends/holidays; using period is often more robust than start/end\n",
    "    period_days = days_back + 2\n",
    "    print(f\"   Period: {period_days} days\")\n",
    "\n",
    "    # ---- One batched download ----\n",
    "    df = yf.download(\n",
    "        \" \".join(tickers),\n",
    "        period=f\"{period_days}d\",\n",
    "        interval=\"1d\",\n",
    "        auto_adjust=True,\n",
    "        actions=False,          # no dividends/splits to keep it lean\n",
    "        group_by=\"column\",      # flat column groups (field, ticker) MultiIndex\n",
    "        threads=True,\n",
    "        progress=False,\n",
    "        #show_errors=True,\n",
    "    )\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        raise ValueError(\"No data returned by yfinance for the requested tickers/period\")\n",
    "\n",
    "    # Normalize rows\n",
    "    df = df.dropna(how=\"all\")\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"All rows are NaN after cleaning\")\n",
    "\n",
    "    # ---- Reshape to long (Ticker, Date, OHLCV) ----\n",
    "    # yfinance with group_by=\"column\" gives a MultiIndex on columns:\n",
    "    # one level = field names (Open, High, Low, Close, Adj Close, Volume)\n",
    "    # the other level = tickers. We'll detect which is which and reshape.\n",
    "    fields = {\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"}\n",
    "    if not isinstance(df.columns, pd.MultiIndex):\n",
    "        # Single-ticker case: plain columns -> make it look like multi for consistency\n",
    "        df.columns = pd.MultiIndex.from_product([df.columns, [tickers[0]]])\n",
    "\n",
    "    lvl0 = set(map(str, df.columns.get_level_values(0)))\n",
    "    lvl1 = set(map(str, df.columns.get_level_values(1)))\n",
    "\n",
    "    # Determine which level is fields\n",
    "    if fields & lvl0:\n",
    "        field_level, ticker_level = 0, 1\n",
    "    elif fields & lvl1:\n",
    "        field_level, ticker_level = 1, 0\n",
    "        df.columns = df.columns.swaplevel(0, 1)  # put (field, ticker) order\n",
    "    else:\n",
    "        # Fallback: assume first level is field\n",
    "        field_level, ticker_level = 0, 1\n",
    "\n",
    "    # After ensuring (field, ticker), stack tickers to rows\n",
    "    df_long = (\n",
    "        df.stack(level=1)  # stack ticker level to rows\n",
    "          .reset_index()\n",
    "          .rename(columns={\"level_1\": \"Ticker\"})\n",
    "    )\n",
    "\n",
    "    # Clean column names\n",
    "    df_long.columns = [str(c).replace(\" \", \"_\") for c in df_long.columns]\n",
    "    # Ensure standard set exists (some tickers may miss columns on illiquid days)\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj_Close\", \"Volume\"]:\n",
    "        if col not in df_long.columns:\n",
    "            df_long[col] = pd.NA\n",
    "\n",
    "    # ---- Per-ticker status summary ----\n",
    "    successful_tickers = []\n",
    "    empty_tickers = []\n",
    "    for t in tickers:\n",
    "        n = len(df_long[df_long[\"Ticker\"] == t])\n",
    "        if n > 0:\n",
    "            successful_tickers.append(t)\n",
    "            print(f\"✅ {t}: {n} rows\")\n",
    "        else:\n",
    "            empty_tickers.append(t)\n",
    "            print(f\"⚠️ {t}: no rows\")\n",
    "\n",
    "    if not successful_tickers:\n",
    "        raise ValueError(\"No data fetched for any ticker\")\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n✅ Fetched data for {len(successful_tickers)} tickers \"\n",
    "          f\"(empty: {len(empty_tickers)})\")\n",
    "    print(f\"   Total observations: {len(df_long)}\")\n",
    "    try:\n",
    "        min_d = df_long['Date'].min()\n",
    "        max_d = df_long['Date'].max()\n",
    "        print(f\"   Date range in data: {min_d} to {max_d}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df_long, successful_tickers\n",
    "\n",
    "# ---- Example usage ----\n",
    "# raw_data, successful_tickers = fetch_fresh_ohlcv_fast(tickers, days_back=30)\n",
    "# print(f\"\\n📊 Raw data shape: {raw_data.shape}\")\n",
    "# latest_date = raw_data['Date'].max()\n",
    "# latest_sample = raw_data[raw_data['Date'] == latest_date].head(3)\n",
    "# print(f\"\\n📅 Latest data sample ({latest_date}):\")\n",
    "# print(latest_sample[['Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']].to_string(index=False))\n",
    "\n",
    "raw_data, successful_tickers = fetch_fresh_ohlcv_fast(tickers, days_back=1)\n",
    "\n",
    "latest_data = raw_data[['Date', 'Ticker', 'Close', 'High', 'Low', 'Open', 'Volume',\n",
    "       'Adj_Close']]\n",
    "\n",
    "latest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "301fba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: TRANSFORM DATA USING STOCKDATAPIPELINE\n",
      "============================================================\n",
      "🔄 Starting StockDataPipeline transformation...\n",
      "🚀 Running StockDataPipeline for complete feature engineering...\n",
      "   Configuration:\n",
      "   - Lookbacks: [1, 3, 7, 30, 90, 252, 365]\n",
      "   - Horizons: [30] days\n",
      "   - Binary threshold: 100%\n",
      "   📊 Running complete pipeline...\n",
      "      - Step 1: Fetching stock data\n",
      "      - Step 2: Adding technical indicators (TA-Lib)\n",
      "      - Step 3: Adding macro indicators\n",
      "      - Step 4: Final validation and cleanup\n",
      "STOCK MARKET DATA PIPELINE\n",
      "======================================================================\n",
      "Processing 250 tickers: NVDA, TSLA, AAPL, PLTR, MSFT, AMD, AMZN, META, GOOGL, UNH, AVGO, GOOG, MSTR, LLY, COIN, NFLX, INTC, APP, ORCL, BRK-B, JPM, COST, V, MU, PANW, CRM, SMCI, GEV, WMT, NOW, BAC, XOM, UBER, BA, MA, INTU, MRVL, JNJ, IBM, CSCO, CRWD, TXN, HD, C, CVX, AMAT, CAT, GS, QCOM, ADBE, PEP, TMO, LRCX, ANET, UNP, GE, ASML, SHOP, WFC, ACN, BKNG, PG, ISRG, MCD, PFE, TMUS, PDD, KO, MRK, PM, SBUX, TTD, ABBV, ADI, DIS, CMG, APH, T, KLAC, VST, DASH, VRTX, ETN, CSX, MELI, COF, PYPL, WDAY, FI, UPS, HON, NKE, DHR, NSC, LMT, SNPS, SCHW, DDOG, CMCSA, CEG, AXP, TGT, PGR, FTNT, LIN, ELV, VZ, DELL, ABT, LOW, RCL, TJX, WBD, GILD, CHTR, SHW, BSX, NEE, LULU, DE, NEM, MS, F, ABNB, ARM, SPGI, MDT, MCHP, RTX, XYZ, COP, EBAY, BX, CDNS, DHI, MCK, FCX, FSLR, AMGN, JCI, NXPI, REGN, TT, CME, WDC, CI, BMY, KEY, AXON, ICE, BLK, PH, CVS, CB, STX, MO, CCL, ON, IBKR, AMT, URI, EA, FICO, MDLZ, AON, MMM, TDG, HCA, TPR, KDP, MPWR, SYK, HBAN, MMC, UAL, AJG, EMR, HWM, ADSK, CNC, TER, MSI, ZTS, KR, ZS, TEAM, BDX, ORLY, ADP, SLB, APO, KKR, HUM, SO, PWR, NRG, ROST, AZO, HLT, MSCI, RF, IDXX, GM, NOC, FDX, OXY, KMI, EQT, CL, DAL, DOW, ULTA, WELL, GLW, DLTR, WM, TTWO, TRI, IT, USB, TEL, DUK, CARR, WMB, AEP, AZN, DECK, MNST, PCG, EXPE, EL, ROP, COR, CAH, LEN, OTIS, HPE, EW, VLO, PLD, ALB, MAR, KHC, KVUE, CTAS, CCI, NCLH, MCO, LHX, EQIX\n",
      "Lookback periods: [1, 3, 7, 30, 90, 252, 365]\n",
      "Prediction horizons: [30]\n",
      "\n",
      "Step 1: Fetching stock data and basic features...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  250 of 250 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created binary target: is_positive_growth_30d_future (threshold=1.0)\n",
      "Raw stock data: (2109525, 30)\n",
      "Date range: 1962-01-02 00:00:00 to 2025-09-12 00:00:00\n",
      "Tickers: AAPL, ABBV, ABNB, ABT, ACN, ADBE, ADI, ADP, ADSK, AEP, AJG, ALB, AMAT, AMD, AMGN, AMT, AMZN, ANET, AON, APH, APO, APP, ARM, ASML, AVGO, AXON, AXP, AZN, AZO, BA, BAC, BDX, BKNG, BLK, BMY, BRK-B, BSX, BX, C, CAH, CARR, CAT, CB, CCI, CCL, CDNS, CEG, CHTR, CI, CL, CMCSA, CME, CMG, CNC, COF, COIN, COP, COR, COST, CRM, CRWD, CSCO, CSX, CTAS, CVS, CVX, DAL, DASH, DDOG, DE, DECK, DELL, DHI, DHR, DIS, DLTR, DOW, DUK, EA, EBAY, EL, ELV, EMR, EQIX, EQT, ETN, EW, EXPE, F, FCX, FDX, FI, FICO, FSLR, FTNT, GE, GEV, GILD, GLW, GM, GOOG, GOOGL, GS, HBAN, HCA, HD, HLT, HON, HPE, HUM, HWM, IBKR, IBM, ICE, IDXX, INTC, INTU, ISRG, IT, JCI, JNJ, JPM, KDP, KEY, KHC, KKR, KLAC, KMI, KO, KR, KVUE, LEN, LHX, LIN, LLY, LMT, LOW, LRCX, LULU, MA, MAR, MCD, MCHP, MCK, MCO, MDLZ, MDT, MELI, META, MMC, MMM, MNST, MO, MPWR, MRK, MRVL, MS, MSCI, MSFT, MSI, MSTR, MU, NCLH, NEE, NEM, NFLX, NKE, NOC, NOW, NRG, NSC, NVDA, NXPI, ON, ORCL, ORLY, OTIS, OXY, PANW, PCG, PDD, PEP, PFE, PG, PGR, PH, PLD, PLTR, PM, PWR, PYPL, QCOM, RCL, REGN, RF, ROP, ROST, RTX, SBUX, SCHW, SHOP, SHW, SLB, SMCI, SNPS, SO, SPGI, STX, SYK, T, TDG, TEAM, TEL, TER, TGT, TJX, TMO, TMUS, TPR, TRI, TSLA, TT, TTD, TTWO, TXN, UAL, UBER, ULTA, UNH, UNP, UPS, URI, USB, V, VLO, VRTX, VST, VZ, WBD, WDAY, WDC, WELL, WFC, WM, WMB, WMT, XOM, XYZ, ZS, ZTS\n",
      "\n",
      "Step 2: Adding technical analysis indicators...\n",
      "--------------------------------------------------\n",
      "Creating augmented dataset with technical indicators...\n",
      "Processing TA indicators for AAPL (1/250)...\n",
      "Processing TA indicators for ABBV (2/250)...\n",
      "Processing TA indicators for ABNB (3/250)...\n",
      "Processing TA indicators for ABT (4/250)...\n",
      "Processing TA indicators for ACN (5/250)...\n",
      "Processing TA indicators for ADBE (6/250)...\n",
      "Processing TA indicators for ADI (7/250)...\n",
      "Processing TA indicators for ADP (8/250)...\n",
      "Processing TA indicators for ADSK (9/250)...\n",
      "Processing TA indicators for AEP (10/250)...\n",
      "Processing TA indicators for AJG (11/250)...\n",
      "Processing TA indicators for ALB (12/250)...\n",
      "Processing TA indicators for AMAT (13/250)...\n",
      "Processing TA indicators for AMD (14/250)...\n",
      "Processing TA indicators for AMGN (15/250)...\n",
      "Processing TA indicators for AMT (16/250)...\n",
      "Processing TA indicators for AMZN (17/250)...\n",
      "Processing TA indicators for ANET (18/250)...\n",
      "Processing TA indicators for AON (19/250)...\n",
      "Processing TA indicators for APH (20/250)...\n",
      "Processing TA indicators for APO (21/250)...\n",
      "Processing TA indicators for APP (22/250)...\n",
      "Processing TA indicators for ARM (23/250)...\n",
      "Processing TA indicators for ASML (24/250)...\n",
      "Processing TA indicators for AVGO (25/250)...\n",
      "Processing TA indicators for AXON (26/250)...\n",
      "Processing TA indicators for AXP (27/250)...\n",
      "Processing TA indicators for AZN (28/250)...\n",
      "Processing TA indicators for AZO (29/250)...\n",
      "Processing TA indicators for BA (30/250)...\n",
      "Processing TA indicators for BAC (31/250)...\n",
      "Processing TA indicators for BDX (32/250)...\n",
      "Processing TA indicators for BKNG (33/250)...\n",
      "Processing TA indicators for BLK (34/250)...\n",
      "Processing TA indicators for BMY (35/250)...\n",
      "Processing TA indicators for BRK-B (36/250)...\n",
      "Processing TA indicators for BSX (37/250)...\n",
      "Processing TA indicators for BX (38/250)...\n",
      "Processing TA indicators for C (39/250)...\n",
      "Processing TA indicators for CAH (40/250)...\n",
      "Processing TA indicators for CARR (41/250)...\n",
      "Processing TA indicators for CAT (42/250)...\n",
      "Processing TA indicators for CB (43/250)...\n",
      "Processing TA indicators for CCI (44/250)...\n",
      "Processing TA indicators for CCL (45/250)...\n",
      "Processing TA indicators for CDNS (46/250)...\n",
      "Processing TA indicators for CEG (47/250)...\n",
      "Processing TA indicators for CHTR (48/250)...\n",
      "Processing TA indicators for CI (49/250)...\n",
      "Processing TA indicators for CL (50/250)...\n",
      "Processing TA indicators for CMCSA (51/250)...\n",
      "Processing TA indicators for CME (52/250)...\n",
      "Processing TA indicators for CMG (53/250)...\n",
      "Processing TA indicators for CNC (54/250)...\n",
      "Processing TA indicators for COF (55/250)...\n",
      "Processing TA indicators for COIN (56/250)...\n",
      "Processing TA indicators for COP (57/250)...\n",
      "Processing TA indicators for COR (58/250)...\n",
      "Processing TA indicators for COST (59/250)...\n",
      "Processing TA indicators for CRM (60/250)...\n",
      "Processing TA indicators for CRWD (61/250)...\n",
      "Processing TA indicators for CSCO (62/250)...\n",
      "Processing TA indicators for CSX (63/250)...\n",
      "Processing TA indicators for CTAS (64/250)...\n",
      "Processing TA indicators for CVS (65/250)...\n",
      "Processing TA indicators for CVX (66/250)...\n",
      "Processing TA indicators for DAL (67/250)...\n",
      "Processing TA indicators for DASH (68/250)...\n",
      "Processing TA indicators for DDOG (69/250)...\n",
      "Processing TA indicators for DE (70/250)...\n",
      "Processing TA indicators for DECK (71/250)...\n",
      "Processing TA indicators for DELL (72/250)...\n",
      "Processing TA indicators for DHI (73/250)...\n",
      "Processing TA indicators for DHR (74/250)...\n",
      "Processing TA indicators for DIS (75/250)...\n",
      "Processing TA indicators for DLTR (76/250)...\n",
      "Processing TA indicators for DOW (77/250)...\n",
      "Processing TA indicators for DUK (78/250)...\n",
      "Processing TA indicators for EA (79/250)...\n",
      "Processing TA indicators for EBAY (80/250)...\n",
      "Processing TA indicators for EL (81/250)...\n",
      "Processing TA indicators for ELV (82/250)...\n",
      "Processing TA indicators for EMR (83/250)...\n",
      "Processing TA indicators for EQIX (84/250)...\n",
      "Processing TA indicators for EQT (85/250)...\n",
      "Processing TA indicators for ETN (86/250)...\n",
      "Processing TA indicators for EW (87/250)...\n",
      "Processing TA indicators for EXPE (88/250)...\n",
      "Processing TA indicators for F (89/250)...\n",
      "Processing TA indicators for FCX (90/250)...\n",
      "Processing TA indicators for FDX (91/250)...\n",
      "Processing TA indicators for FI (92/250)...\n",
      "Processing TA indicators for FICO (93/250)...\n",
      "Processing TA indicators for FSLR (94/250)...\n",
      "Processing TA indicators for FTNT (95/250)...\n",
      "Processing TA indicators for GE (96/250)...\n",
      "Processing TA indicators for GEV (97/250)...\n",
      "Processing TA indicators for GILD (98/250)...\n",
      "Processing TA indicators for GLW (99/250)...\n",
      "Processing TA indicators for GM (100/250)...\n",
      "Processing TA indicators for GOOG (101/250)...\n",
      "Processing TA indicators for GOOGL (102/250)...\n",
      "Processing TA indicators for GS (103/250)...\n",
      "Processing TA indicators for HBAN (104/250)...\n",
      "Processing TA indicators for HCA (105/250)...\n",
      "Processing TA indicators for HD (106/250)...\n",
      "Processing TA indicators for HLT (107/250)...\n",
      "Processing TA indicators for HON (108/250)...\n",
      "Processing TA indicators for HPE (109/250)...\n",
      "Processing TA indicators for HUM (110/250)...\n",
      "Processing TA indicators for HWM (111/250)...\n",
      "Processing TA indicators for IBKR (112/250)...\n",
      "Processing TA indicators for IBM (113/250)...\n",
      "Processing TA indicators for ICE (114/250)...\n",
      "Processing TA indicators for IDXX (115/250)...\n",
      "Processing TA indicators for INTC (116/250)...\n",
      "Processing TA indicators for INTU (117/250)...\n",
      "Processing TA indicators for ISRG (118/250)...\n",
      "Processing TA indicators for IT (119/250)...\n",
      "Processing TA indicators for JCI (120/250)...\n",
      "Processing TA indicators for JNJ (121/250)...\n",
      "Processing TA indicators for JPM (122/250)...\n",
      "Processing TA indicators for KDP (123/250)...\n",
      "Processing TA indicators for KEY (124/250)...\n",
      "Processing TA indicators for KHC (125/250)...\n",
      "Processing TA indicators for KKR (126/250)...\n",
      "Processing TA indicators for KLAC (127/250)...\n",
      "Processing TA indicators for KMI (128/250)...\n",
      "Processing TA indicators for KO (129/250)...\n",
      "Processing TA indicators for KR (130/250)...\n",
      "Processing TA indicators for KVUE (131/250)...\n",
      "Processing TA indicators for LEN (132/250)...\n",
      "Processing TA indicators for LHX (133/250)...\n",
      "Processing TA indicators for LIN (134/250)...\n",
      "Processing TA indicators for LLY (135/250)...\n",
      "Processing TA indicators for LMT (136/250)...\n",
      "Processing TA indicators for LOW (137/250)...\n",
      "Processing TA indicators for LRCX (138/250)...\n",
      "Processing TA indicators for LULU (139/250)...\n",
      "Processing TA indicators for MA (140/250)...\n",
      "Processing TA indicators for MAR (141/250)...\n",
      "Processing TA indicators for MCD (142/250)...\n",
      "Processing TA indicators for MCHP (143/250)...\n",
      "Processing TA indicators for MCK (144/250)...\n",
      "Processing TA indicators for MCO (145/250)...\n",
      "Processing TA indicators for MDLZ (146/250)...\n",
      "Processing TA indicators for MDT (147/250)...\n",
      "Processing TA indicators for MELI (148/250)...\n",
      "Processing TA indicators for META (149/250)...\n",
      "Processing TA indicators for MMC (150/250)...\n",
      "Processing TA indicators for MMM (151/250)...\n",
      "Processing TA indicators for MNST (152/250)...\n",
      "Processing TA indicators for MO (153/250)...\n",
      "Processing TA indicators for MPWR (154/250)...\n",
      "Processing TA indicators for MRK (155/250)...\n",
      "Processing TA indicators for MRVL (156/250)...\n",
      "Processing TA indicators for MS (157/250)...\n",
      "Processing TA indicators for MSCI (158/250)...\n",
      "Processing TA indicators for MSFT (159/250)...\n",
      "Processing TA indicators for MSI (160/250)...\n",
      "Processing TA indicators for MSTR (161/250)...\n",
      "Processing TA indicators for MU (162/250)...\n",
      "Processing TA indicators for NCLH (163/250)...\n",
      "Processing TA indicators for NEE (164/250)...\n",
      "Processing TA indicators for NEM (165/250)...\n",
      "Processing TA indicators for NFLX (166/250)...\n",
      "Processing TA indicators for NKE (167/250)...\n",
      "Processing TA indicators for NOC (168/250)...\n",
      "Processing TA indicators for NOW (169/250)...\n",
      "Processing TA indicators for NRG (170/250)...\n",
      "Processing TA indicators for NSC (171/250)...\n",
      "Processing TA indicators for NVDA (172/250)...\n",
      "Processing TA indicators for NXPI (173/250)...\n",
      "Processing TA indicators for ON (174/250)...\n",
      "Processing TA indicators for ORCL (175/250)...\n",
      "Processing TA indicators for ORLY (176/250)...\n",
      "Processing TA indicators for OTIS (177/250)...\n",
      "Processing TA indicators for OXY (178/250)...\n",
      "Processing TA indicators for PANW (179/250)...\n",
      "Processing TA indicators for PCG (180/250)...\n",
      "Processing TA indicators for PDD (181/250)...\n",
      "Processing TA indicators for PEP (182/250)...\n",
      "Processing TA indicators for PFE (183/250)...\n",
      "Processing TA indicators for PG (184/250)...\n",
      "Processing TA indicators for PGR (185/250)...\n",
      "Processing TA indicators for PH (186/250)...\n",
      "Processing TA indicators for PLD (187/250)...\n",
      "Processing TA indicators for PLTR (188/250)...\n",
      "Processing TA indicators for PM (189/250)...\n",
      "Processing TA indicators for PWR (190/250)...\n",
      "Processing TA indicators for PYPL (191/250)...\n",
      "Processing TA indicators for QCOM (192/250)...\n",
      "Processing TA indicators for RCL (193/250)...\n",
      "Processing TA indicators for REGN (194/250)...\n",
      "Processing TA indicators for RF (195/250)...\n",
      "Processing TA indicators for ROP (196/250)...\n",
      "Processing TA indicators for ROST (197/250)...\n",
      "Processing TA indicators for RTX (198/250)...\n",
      "Processing TA indicators for SBUX (199/250)...\n",
      "Processing TA indicators for SCHW (200/250)...\n",
      "Processing TA indicators for SHOP (201/250)...\n",
      "Processing TA indicators for SHW (202/250)...\n",
      "Processing TA indicators for SLB (203/250)...\n",
      "Processing TA indicators for SMCI (204/250)...\n",
      "Processing TA indicators for SNPS (205/250)...\n",
      "Processing TA indicators for SO (206/250)...\n",
      "Processing TA indicators for SPGI (207/250)...\n",
      "Processing TA indicators for STX (208/250)...\n",
      "Processing TA indicators for SYK (209/250)...\n",
      "Processing TA indicators for T (210/250)...\n",
      "Processing TA indicators for TDG (211/250)...\n",
      "Processing TA indicators for TEAM (212/250)...\n",
      "Processing TA indicators for TEL (213/250)...\n",
      "Processing TA indicators for TER (214/250)...\n",
      "Processing TA indicators for TGT (215/250)...\n",
      "Processing TA indicators for TJX (216/250)...\n",
      "Processing TA indicators for TMO (217/250)...\n",
      "Processing TA indicators for TMUS (218/250)...\n",
      "Processing TA indicators for TPR (219/250)...\n",
      "Processing TA indicators for TRI (220/250)...\n",
      "Processing TA indicators for TSLA (221/250)...\n",
      "Processing TA indicators for TT (222/250)...\n",
      "Processing TA indicators for TTD (223/250)...\n",
      "Processing TA indicators for TTWO (224/250)...\n",
      "Processing TA indicators for TXN (225/250)...\n",
      "Processing TA indicators for UAL (226/250)...\n",
      "Processing TA indicators for UBER (227/250)...\n",
      "Processing TA indicators for ULTA (228/250)...\n",
      "Processing TA indicators for UNH (229/250)...\n",
      "Processing TA indicators for UNP (230/250)...\n",
      "Processing TA indicators for UPS (231/250)...\n",
      "Processing TA indicators for URI (232/250)...\n",
      "Processing TA indicators for USB (233/250)...\n",
      "Processing TA indicators for V (234/250)...\n",
      "Processing TA indicators for VLO (235/250)...\n",
      "Processing TA indicators for VRTX (236/250)...\n",
      "Processing TA indicators for VST (237/250)...\n",
      "Processing TA indicators for VZ (238/250)...\n",
      "Processing TA indicators for WBD (239/250)...\n",
      "Processing TA indicators for WDAY (240/250)...\n",
      "Processing TA indicators for WDC (241/250)...\n",
      "Processing TA indicators for WELL (242/250)...\n",
      "Processing TA indicators for WFC (243/250)...\n",
      "Processing TA indicators for WM (244/250)...\n",
      "Processing TA indicators for WMB (245/250)...\n",
      "Processing TA indicators for WMT (246/250)...\n",
      "Processing TA indicators for XOM (247/250)...\n",
      "Processing TA indicators for XYZ (248/250)...\n",
      "Processing TA indicators for ZS (249/250)...\n",
      "Processing TA indicators for ZTS (250/250)...\n",
      "Technical analysis complete: (2109525, 147)\n",
      "Added 140 technical indicators\n",
      "  - Momentum indicators: 19\n",
      "  - Volume indicators: 11\n",
      "  - Pattern indicators: 61\n",
      "\n",
      "Step 3: Adding macro economic indicators...\n",
      "--------------------------------------------------\n",
      "Fetching macro data from 1962-01-02\n",
      "Successfully fetched btc: 4015 rows\n",
      "Successfully fetched vix: 8991 rows\n",
      "Successfully fetched dax: 9535 rows\n",
      "Successfully fetched snp500: 24541 rows\n",
      "Successfully fetched dji: 8485 rows\n",
      "Successfully fetched epi: 4416 rows\n",
      "Successfully fetched gold: 6282 rows\n",
      "Successfully fetched brent_oil: 4509 rows\n",
      "Successfully fetched crude_oil: 6291 rows\n",
      "Successfully processed gdppot: 23103 rows\n",
      "  Recent YoY range: [0.0230, 0.0230]\n",
      "Successfully processed cpilfesl: 23193 rows\n",
      "  Recent YoY range: [0.0305, 0.0311]\n",
      "Successfully processed fedfunds: 23193 rows\n",
      "  Recent YoY range: [-0.1876, -0.1876]\n",
      "Successfully processed dgs1: 16618 rows\n",
      "  Recent YoY range: [-0.0759, -0.0450]\n",
      "Successfully processed dgs5: 16618 rows\n",
      "  Recent YoY range: [0.0228, 0.0904]\n",
      "Successfully processed dgs10: 16618 rows\n",
      "  Recent YoY range: [0.0722, 0.1694]\n",
      "Starting merge with 2109525 stock rows...\n",
      "Merging btc with 7 columns: ['growth_btc_1d', 'growth_btc_3d', 'growth_btc_7d', 'growth_btc_30d', 'growth_btc_90d', 'growth_btc_252d', 'growth_btc_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging vix with 7 columns: ['growth_vix_1d', 'growth_vix_3d', 'growth_vix_7d', 'growth_vix_30d', 'growth_vix_90d', 'growth_vix_252d', 'growth_vix_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging dax with 7 columns: ['growth_dax_1d', 'growth_dax_3d', 'growth_dax_7d', 'growth_dax_30d', 'growth_dax_90d', 'growth_dax_252d', 'growth_dax_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging snp500 with 7 columns: ['growth_snp500_1d', 'growth_snp500_3d', 'growth_snp500_7d', 'growth_snp500_30d', 'growth_snp500_90d', 'growth_snp500_252d', 'growth_snp500_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging dji with 7 columns: ['growth_dji_1d', 'growth_dji_3d', 'growth_dji_7d', 'growth_dji_30d', 'growth_dji_90d', 'growth_dji_252d', 'growth_dji_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging epi with 7 columns: ['growth_epi_1d', 'growth_epi_3d', 'growth_epi_7d', 'growth_epi_30d', 'growth_epi_90d', 'growth_epi_252d', 'growth_epi_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging gold with 7 columns: ['growth_gold_1d', 'growth_gold_3d', 'growth_gold_7d', 'growth_gold_30d', 'growth_gold_90d', 'growth_gold_252d', 'growth_gold_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging brent_oil with 7 columns: ['growth_brent_oil_1d', 'growth_brent_oil_3d', 'growth_brent_oil_7d', 'growth_brent_oil_30d', 'growth_brent_oil_90d', 'growth_brent_oil_252d', 'growth_brent_oil_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging crude_oil with 7 columns: ['growth_crude_oil_1d', 'growth_crude_oil_3d', 'growth_crude_oil_7d', 'growth_crude_oil_30d', 'growth_crude_oil_90d', 'growth_crude_oil_252d', 'growth_crude_oil_365d']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging gdppot with 2 columns: ['gdppot_yoy', 'gdppot_qoq']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging cpilfesl with 2 columns: ['cpilfesl_yoy', 'cpilfesl_qoq']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging fedfunds with 2 columns: ['fedfunds_yoy', 'fedfunds_qoq']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging dgs1 with 2 columns: ['dgs1_yoy', 'dgs1_qoq']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging dgs5 with 2 columns: ['dgs5_yoy', 'dgs5_qoq']\n",
      "  Merge successful: 2109525 rows\n",
      "Merging dgs10 with 2 columns: ['dgs10_yoy', 'dgs10_qoq']\n",
      "  Merge successful: 2109525 rows\n",
      "Forward filling 83 macro columns...\n",
      "  growth_1d: 233 -> 1 nulls\n",
      "  growth_3d: 697 -> 3 nulls\n",
      "  growth_7d: 1625 -> 7 nulls\n",
      "  growth_30d: 6961 -> 30 nulls\n",
      "  growth_90d: 20854 -> 90 nulls\n",
      "  growth_252d: 58276 -> 252 nulls\n",
      "  growth_365d: 84373 -> 365 nulls\n",
      "  growth_future_30d: 5568 -> 0 nulls\n",
      "  growth_btc_1d: 1449655 -> 8516 nulls\n",
      "  growth_btc_3d: 1450101 -> 8518 nulls\n",
      "  growth_btc_7d: 1450547 -> 8520 nulls\n",
      "  growth_btc_30d: 1454338 -> 8537 nulls\n",
      "  growth_btc_90d: 1463481 -> 8578 nulls\n",
      "  growth_btc_252d: 1488015 -> 8688 nulls\n",
      "  growth_btc_365d: 1505815 -> 8767 nulls\n",
      "  growth_vix_1d: 376821 -> 2289 nulls\n",
      "  growth_vix_3d: 377051 -> 2291 nulls\n",
      "  growth_vix_7d: 377511 -> 2295 nulls\n",
      "  growth_vix_30d: 380156 -> 2318 nulls\n",
      "  growth_vix_90d: 387113 -> 2378 nulls\n",
      "  growth_vix_252d: 405905 -> 2540 nulls\n",
      "  growth_vix_365d: 419113 -> 2653 nulls\n",
      "  growth_dax_1d: 354118 -> 1783 nulls\n",
      "  growth_dax_3d: 354344 -> 1785 nulls\n",
      "  growth_dax_7d: 354796 -> 1789 nulls\n",
      "  growth_dax_30d: 357395 -> 1812 nulls\n",
      "  growth_dax_90d: 364062 -> 1872 nulls\n",
      "  growth_dax_252d: 382048 -> 2036 nulls\n",
      "  growth_dax_365d: 394588 -> 2151 nulls\n",
      "  growth_dji_1d: 436167 -> 2795 nulls\n",
      "  growth_dji_3d: 436411 -> 2797 nulls\n",
      "  growth_dji_7d: 436899 -> 2801 nulls\n",
      "  growth_dji_30d: 439721 -> 2824 nulls\n",
      "  growth_dji_90d: 447213 -> 2884 nulls\n",
      "  growth_dji_252d: 468028 -> 3046 nulls\n",
      "  growth_dji_365d: 482942 -> 3159 nulls\n",
      "  growth_epi_1d: 1098564 -> 6864 nulls\n",
      "  growth_epi_3d: 1098966 -> 6866 nulls\n",
      "  growth_epi_7d: 1099770 -> 6870 nulls\n",
      "  growth_epi_30d: 1104423 -> 6893 nulls\n",
      "  growth_epi_90d: 1116643 -> 6953 nulls\n",
      "  growth_epi_252d: 1149691 -> 7115 nulls\n",
      "  growth_epi_365d: 1172743 -> 7228 nulls\n",
      "  growth_gold_1d: 760364 -> 4984 nulls\n",
      "  growth_gold_3d: 760700 -> 4986 nulls\n",
      "  growth_gold_7d: 761372 -> 4990 nulls\n",
      "  growth_gold_30d: 765240 -> 5013 nulls\n",
      "  growth_gold_90d: 775380 -> 5074 nulls\n",
      "  growth_gold_252d: 802897 -> 5236 nulls\n",
      "  growth_gold_365d: 822281 -> 5350 nulls\n",
      "  growth_brent_oil_1d: 1081253 -> 6719 nulls\n",
      "  growth_brent_oil_3d: 1081649 -> 6721 nulls\n",
      "  growth_brent_oil_7d: 1082441 -> 6725 nulls\n",
      "  growth_brent_oil_30d: 1087016 -> 6748 nulls\n",
      "  growth_brent_oil_90d: 1098997 -> 6808 nulls\n",
      "  growth_brent_oil_252d: 1131800 -> 6970 nulls\n",
      "  growth_brent_oil_365d: 1154852 -> 7086 nulls\n",
      "  growth_crude_oil_1d: 758742 -> 4979 nulls\n",
      "  growth_crude_oil_3d: 759078 -> 4981 nulls\n",
      "  growth_crude_oil_7d: 759750 -> 4985 nulls\n",
      "  growth_crude_oil_30d: 763614 -> 5008 nulls\n",
      "  growth_crude_oil_90d: 773753 -> 5069 nulls\n",
      "  growth_crude_oil_252d: 801255 -> 5231 nulls\n",
      "  growth_crude_oil_365d: 820629 -> 5345 nulls\n",
      "  gdppot_yoy: 17261 -> 0 nulls\n",
      "  gdppot_qoq: 13689 -> 0 nulls\n",
      "  cpilfesl_yoy: 11233 -> 0 nulls\n",
      "  cpilfesl_qoq: 7604 -> 0 nulls\n",
      "  fedfunds_yoy: 11233 -> 0 nulls\n",
      "  fedfunds_qoq: 7604 -> 0 nulls\n",
      "  dgs1_yoy: 4842 -> 0 nulls\n",
      "  dgs1_qoq: 1366 -> 0 nulls\n",
      "  dgs5_yoy: 4842 -> 0 nulls\n",
      "  dgs5_qoq: 1366 -> 0 nulls\n",
      "  dgs10_yoy: 4842 -> 0 nulls\n",
      "  dgs10_qoq: 1366 -> 0 nulls\n",
      "\n",
      "Macro Data Validation:\n",
      "========================================\n",
      "Found 83 macro columns\n",
      "  growth_1d: 0.0% nulls\n",
      "  growth_3d: 0.0% nulls\n",
      "  growth_7d: 0.0% nulls\n",
      "  growth_30d: 0.0% nulls\n",
      "  growth_90d: 0.0% nulls\n",
      "  growth_252d: 0.0% nulls\n",
      "  growth_365d: 0.0% nulls\n",
      "  growth_future_30d: Complete\n",
      "  growth_btc_1d: 0.4% nulls\n",
      "  growth_btc_3d: 0.4% nulls\n",
      "  ... and 73 more columns\n",
      "\n",
      "Sample YoY values (should be reasonable percentages):\n",
      "  gdppot_yoy: [0.0230, 0.0230]\n",
      "  cpilfesl_yoy: [0.0311, 0.0311]\n",
      "  fedfunds_yoy: [-0.1876, -0.1876]\n",
      "Macro integration complete: (2109525, 222)\n",
      "\n",
      "Step 4: Final validation and cleanup...\n",
      "--------------------------------------------------\n",
      "Data Quality Summary:\n",
      "  Total rows: 2,109,525\n",
      "  Total columns: 222\n",
      "  Date range: 1962-01-02 00:00:00 to 2025-09-12 00:00:00\n",
      "  Tickers: AAPL, ABBV, ABNB, ABT, ACN, ADBE, ADI, ADP, ADSK, AEP, AJG, ALB, AMAT, AMD, AMGN, AMT, AMZN, ANET, AON, APH, APO, APP, ARM, ASML, AVGO, AXON, AXP, AZN, AZO, BA, BAC, BDX, BKNG, BLK, BMY, BRK-B, BSX, BX, C, CAH, CARR, CAT, CB, CCI, CCL, CDNS, CEG, CHTR, CI, CL, CMCSA, CME, CMG, CNC, COF, COIN, COP, COR, COST, CRM, CRWD, CSCO, CSX, CTAS, CVS, CVX, DAL, DASH, DDOG, DE, DECK, DELL, DHI, DHR, DIS, DLTR, DOW, DUK, EA, EBAY, EL, ELV, EMR, EQIX, EQT, ETN, EW, EXPE, F, FCX, FDX, FI, FICO, FSLR, FTNT, GE, GEV, GILD, GLW, GM, GOOG, GOOGL, GS, HBAN, HCA, HD, HLT, HON, HPE, HUM, HWM, IBKR, IBM, ICE, IDXX, INTC, INTU, ISRG, IT, JCI, JNJ, JPM, KDP, KEY, KHC, KKR, KLAC, KMI, KO, KR, KVUE, LEN, LHX, LIN, LLY, LMT, LOW, LRCX, LULU, MA, MAR, MCD, MCHP, MCK, MCO, MDLZ, MDT, MELI, META, MMC, MMM, MNST, MO, MPWR, MRK, MRVL, MS, MSCI, MSFT, MSI, MSTR, MU, NCLH, NEE, NEM, NFLX, NKE, NOC, NOW, NRG, NSC, NVDA, NXPI, ON, ORCL, ORLY, OTIS, OXY, PANW, PCG, PDD, PEP, PFE, PG, PGR, PH, PLD, PLTR, PM, PWR, PYPL, QCOM, RCL, REGN, RF, ROP, ROST, RTX, SBUX, SCHW, SHOP, SHW, SLB, SMCI, SNPS, SO, SPGI, STX, SYK, T, TDG, TEAM, TEL, TER, TGT, TJX, TMO, TMUS, TPR, TRI, TSLA, TT, TTD, TTWO, TXN, UAL, UBER, ULTA, UNH, UNP, UPS, URI, USB, V, VLO, VRTX, VST, VZ, WBD, WDAY, WDC, WELL, WFC, WM, WMB, WMT, XOM, XYZ, ZS, ZTS\n",
      "\n",
      "No columns with excessive missing data\n",
      "\n",
      "Feature Summary:\n",
      "  Basic OHLCV: 7 features\n",
      "  Time Features: 5 features\n",
      "  Growth Features: 36 features\n",
      "  Moving Averages: 3 features\n",
      "  Volatility: 5 features\n",
      "  Technical - Momentum: 26 features\n",
      "  Technical - Volume: 9 features\n",
      "  Technical - Patterns: 58 features\n",
      "  Technical - Cycle: 9 features\n",
      "  Macro - Growth Rates: 35 features\n",
      "  Macro - YoY/QoQ: 12 features\n",
      "  Other: 17 features\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "Total processing time: 340.0 seconds\n",
      "Final dataset: 2,109,525 rows × 222 columns\n",
      "Ready for modeling!\n",
      "   ✅ StockDataPipeline complete!\n",
      "      Final shape: (2109525, 222)\n",
      "   📅 Prediction data ready:\n",
      "      Latest date: 2025-09-12\n",
      "      Stocks: 250\n",
      "\n",
      "   📈 Features created:\n",
      "      Basic OHLCV: 7\n",
      "      Growth Features: 70\n",
      "      Technical Indicators: 18\n",
      "      Candlestick Patterns: 61\n",
      "      Macro Features: 26\n",
      "      Target Variables: 2\n",
      "      Total: 184 features\n",
      "✅ StockDataPipeline transformation complete!\n",
      "   Ready for model predictions: 250 stocks\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: TRANSFORM DATA USING YOUR STOCKDATAPIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: TRANSFORM DATA USING STOCKDATAPIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_stock_pipeline_for_predictions(tickers):\n",
    "    \"\"\"Run your StockDataPipeline to get fully transformed data\"\"\"\n",
    "    print(\"🚀 Running StockDataPipeline for complete feature engineering...\")\n",
    "    \n",
    "    # Configuration (same as your run_data_extraction.py)\n",
    "    config = {\n",
    "        \"LOOKBACKS\": [1, 3, 7, 30, 90, 252, 365],\n",
    "        \"HORIZONS\": [30],\n",
    "        \"BINARY_THRESHOLDS\": {30: 1.00},  # 0% gain threshold\n",
    "    }\n",
    "    \n",
    "    print(f\"   Configuration:\")\n",
    "    print(f\"   - Lookbacks: {config['LOOKBACKS']}\")\n",
    "    print(f\"   - Horizons: {config['HORIZONS']} days\")  \n",
    "    print(f\"   - Binary threshold: {config['BINARY_THRESHOLDS'][30]:.0%}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize StockDataPipeline (same as extraction script)\n",
    "        pipeline = StockDataPipeline(\n",
    "            tickers=tickers,\n",
    "            lookbacks=config[\"LOOKBACKS\"],\n",
    "            horizons=config[\"HORIZONS\"],\n",
    "            binarize_thresholds=config[\"BINARY_THRESHOLDS\"],\n",
    "        )\n",
    "        \n",
    "        # Run complete pipeline: stock data + technical indicators + macro data\n",
    "        print(\"   📊 Running complete pipeline...\")\n",
    "        print(\"      - Step 1: Fetching stock data\")\n",
    "        print(\"      - Step 2: Adding technical indicators (TA-Lib)\")  \n",
    "        print(\"      - Step 3: Adding macro indicators\")\n",
    "        print(\"      - Step 4: Final validation and cleanup\")\n",
    "        \n",
    "        transformed_data = pipeline.run_complete_pipeline()\n",
    "        \n",
    "        print(f\"   ✅ StockDataPipeline complete!\")\n",
    "        print(f\"      Final shape: {transformed_data.shape}\")\n",
    "        \n",
    "        # Get latest data for predictions\n",
    "        latest_date = transformed_data['Date'].max()\n",
    "        prediction_data = transformed_data[transformed_data['Date'] == latest_date].copy()\n",
    "        \n",
    "        print(f\"   📅 Prediction data ready:\")\n",
    "        print(f\"      Latest date: {latest_date.date()}\")\n",
    "        print(f\"      Stocks: {len(prediction_data)}\")\n",
    "        \n",
    "        # Show feature categories\n",
    "        feature_categories = {\n",
    "            \"Basic OHLCV\": [c for c in transformed_data.columns if c in [\"Date\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]],\n",
    "            \"Growth Features\": [c for c in transformed_data.columns if c.startswith(\"growth_\") and \"future\" not in c],\n",
    "            \"Technical Indicators\": [c for c in transformed_data.columns if any(x in c.lower() for x in [\"rsi\", \"macd\", \"sma\", \"adx\", \"cci\"])],\n",
    "            \"Candlestick Patterns\": [c for c in transformed_data.columns if c.startswith(\"cdl\")],\n",
    "            \"Macro Features\": [c for c in transformed_data.columns if c.endswith((\"_yoy\", \"_qoq\")) or \"btc\" in c.lower() or \"vix\" in c.lower()],\n",
    "            \"Target Variables\": [c for c in transformed_data.columns if \"future\" in c and (\"positive\" in c or \"growth\" in c)],\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   📈 Features created:\")\n",
    "        total_features = 0\n",
    "        for category, features in feature_categories.items():\n",
    "            print(f\"      {category}: {len(features)}\")\n",
    "            total_features += len(features)\n",
    "        print(f\"      Total: {total_features} features\")\n",
    "        \n",
    "        return transformed_data, prediction_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ StockDataPipeline failed: {e}\")\n",
    "        print(f\"   This could be due to:\")\n",
    "        print(f\"   - API rate limits from yfinance/FRED\")\n",
    "        print(f\"   - Missing TA-Lib dependencies\") \n",
    "        print(f\"   - Network connectivity issues\")\n",
    "        print(f\"   - Insufficient historical data\")\n",
    "        raise\n",
    "\n",
    "# Run StockDataPipeline\n",
    "try:\n",
    "    print(\"🔄 Starting StockDataPipeline transformation...\")\n",
    "    full_data, prediction_data = run_stock_pipeline_for_predictions(tickers)\n",
    "    \n",
    "    print(\"✅ StockDataPipeline transformation complete!\")\n",
    "    print(f\"   Ready for model predictions: {len(prediction_data)} stocks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ StockDataPipeline failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check internet connection\")\n",
    "    print(\"2. Verify TA-Lib is installed: pip install TA-Lib\")  \n",
    "    print(\"3. Try with fewer tickers (reduce TOP_N_PICKS)\")\n",
    "    print(\"4. Check yfinance/FRED API limits\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: PREPARE FOR MODEL INFERENCE\n",
      "============================================================\n",
      "🤖 Using TrainModel for inference preparation...\n",
      "Preparing dataframe for modeling...\n",
      "Defining feature sets...\n",
      "Feature Set Summary:\n",
      "  Growth features: 70\n",
      "  Technical indicators: 56\n",
      "  Technical patterns: 61\n",
      "  Custom numerical: 7\n",
      "  Macro features: 75\n",
      "  Categorical (for dummies): 7\n",
      "  Target columns: 2\n",
      "  Total numerical features: 206\n",
      "  Unused columns: 0\n",
      "Creating dummy variables...\n",
      "Created 397 dummy variables\n",
      "Sample dummies: ['month_1', 'month_10', 'month_11', 'month_12', 'month_2']\n",
      "Filtered data from 2000-01-01\n",
      "Date range: 2000-01-03 00:00:00 to 2025-09-12 00:00:00\n",
      "Temporal split created:\n",
      "  train: 909,539 samples\n",
      "  validation: 232,029 samples\n",
      "  test: 238,866 samples\n",
      "Creating ML datasets...\n",
      "Total features before filtering: 603\n",
      "  - Numerical: 206\n",
      "  - Dummies: 397\n",
      "Features after removing 'future': 603\n",
      "Selected target: is_positive_growth_30d_future\n",
      "ML Dataset Summary:\n",
      "  Features used: 603\n",
      "  Train: 909,539 samples\n",
      "  Validation: 232,029 samples\n",
      "  Test: 238,866 samples\n",
      "  Train+Valid: 1,141,568 samples\n",
      "  Target: is_positive_growth_30d_future\n",
      "  Target distribution (train): {1: 532731, 0: 376808}\n",
      "   ✅ TrainModel preparation complete\n",
      "      Shape: (1380434, 620)\n",
      "   📊 Inference ready: 250 stocks\n",
      "\n",
      "📂 Loading trained model...\n",
      "[load_model_and_features] Using model file: best_rf_model.joblib\n",
      "[load_model_and_features] feature_names_in_: 603 features\n",
      "✅ Model loaded:\n",
      "   Type: RandomForestClassifier\n",
      "   Features expected: 603\n",
      "   Target: None\n",
      "✅ Model setup complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: PREPARE FOR MODEL INFERENCE USING TRAINMODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: PREPARE FOR MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def prepare_for_model_inference(pipeline_data,window_days=7):\n",
    "    \"\"\"Use TrainModel to prepare StockDataPipeline output for inference\"\"\"\n",
    "    print(\"🤖 Using TrainModel for inference preparation...\")\n",
    "    \n",
    "    try:\n",
    "        # Create adapter and TrainModel (consistent with predictions.py)\n",
    "        adapter = _TransformAdapter(pipeline_data)\n",
    "        tm = TrainModel(adapter)\n",
    "        \n",
    "        # Prepare for inference (creates dummy variables, etc.)\n",
    "        tm.prepare_dataframe(start_date=\"2000-01-01\")\n",
    "        \n",
    "        print(f\"   ✅ TrainModel preparation complete\")\n",
    "        print(f\"      Shape: {tm.df_full.shape}\")\n",
    "        \n",
    "        # Get latest data for predictions\n",
    "        latest_date = tm.df_full['Date'].max()\n",
    "        inference_data = tm.df_full[tm.df_full['Date'] == latest_date].copy()\n",
    "        \n",
    "        print(f\"   📊 Inference ready: {len(inference_data)} stocks\")\n",
    "        \n",
    "        return tm, inference_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ TrainModel preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Prepare data for inference\n",
    "    tm, inference_data = prepare_for_model_inference(full_data)\n",
    "    \n",
    "    # Load trained model\n",
    "    print(\"\\n📂 Loading trained model...\")\n",
    "    model, feature_cols, target_col = load_model_and_features(str(ARTIFACTS_DIR))\n",
    "    \n",
    "    print(f\"✅ Model loaded:\")\n",
    "    print(f\"   Type: {type(model).__name__}\")\n",
    "    print(f\"   Features expected: {len(feature_cols)}\")\n",
    "    print(f\"   Target: {target_col}\")\n",
    "    \n",
    "    # Set up TrainModel for inference (same as predictions.py)\n",
    "    tm.model = model\n",
    "    tm._inference_feature_columns = feature_cols\n",
    "    if target_col:\n",
    "        tm.target_col = target_col\n",
    "    \n",
    "    print(\"✅ Model setup complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Model setup failed: {e}\")\n",
    "    print(\"Make sure you have a trained model in the artifacts directory\")\n",
    "    print(\"Run: python run_model_training.py --mode basic\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddb575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11e80a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: PREPARE FOR MODEL INFERENCE\n",
      "============================================================\n",
      "🤖 Using TrainModel for inference preparation...\n",
      "Preparing dataframe for modeling...\n",
      "Defining feature sets...\n",
      "Feature Set Summary:\n",
      "  Growth features: 70\n",
      "  Technical indicators: 56\n",
      "  Technical patterns: 61\n",
      "  Custom numerical: 7\n",
      "  Macro features: 75\n",
      "  Categorical (for dummies): 7\n",
      "  Target columns: 2\n",
      "  Total numerical features: 206\n",
      "  Unused columns: 0\n",
      "Creating dummy variables...\n",
      "Created 397 dummy variables\n",
      "Sample dummies: ['month_1', 'month_10', 'month_11', 'month_12', 'month_2']\n",
      "Filtered data from 2000-01-01\n",
      "Date range: 2000-01-03 00:00:00 to 2025-09-12 00:00:00\n",
      "Temporal split created:\n",
      "  train: 909,539 samples\n",
      "  validation: 232,029 samples\n",
      "  test: 238,866 samples\n",
      "Creating ML datasets...\n",
      "Total features before filtering: 603\n",
      "  - Numerical: 206\n",
      "  - Dummies: 397\n",
      "Features after removing 'future': 603\n",
      "Selected target: is_positive_growth_30d_future\n",
      "ML Dataset Summary:\n",
      "  Features used: 603\n",
      "  Train: 909,539 samples\n",
      "  Validation: 232,029 samples\n",
      "  Test: 238,866 samples\n",
      "  Train+Valid: 1,141,568 samples\n",
      "  Target: is_positive_growth_30d_future\n",
      "  Target distribution (train): {1: 532731, 0: 376808}\n",
      "   ✅ TrainModel preparation complete\n",
      "      Shape: (1380434, 620)\n",
      "   📊 Inference window: 2025-09-06 → 2025-09-12  |  Rows: 1250\n",
      "\n",
      "📂 Loading trained model...\n",
      "[load_model_and_features] Using model file: best_rf_model.joblib\n",
      "[load_model_and_features] feature_names_in_: 603 features\n",
      "✅ Model loaded:\n",
      "   Type: RandomForestClassifier\n",
      "   Features expected: 603\n",
      "   Target: None\n",
      "✅ Model setup complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: PREPARE FOR MODEL INFERENCE USING TRAINMODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: PREPARE FOR MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd  # ensure available for Timedelta/to_datetime\n",
    "\n",
    "def prepare_for_model_inference(pipeline_data, window_days=7):\n",
    "    \"\"\"Use TrainModel to prepare StockDataPipeline output for inference.\"\"\"\n",
    "    print(\"🤖 Using TrainModel for inference preparation...\")\n",
    "    try:\n",
    "        # Create adapter and TrainModel (consistent with predictions.py)\n",
    "        adapter = _TransformAdapter(pipeline_data)\n",
    "        tm = TrainModel(adapter)\n",
    "\n",
    "        # Prepare for inference (creates dummy variables, etc.)\n",
    "        tm.prepare_dataframe(start_date=\"2000-01-01\")\n",
    "\n",
    "        print(f\"   ✅ TrainModel preparation complete\")\n",
    "        print(f\"      Shape: {tm.df_full.shape}\")\n",
    "\n",
    "        # ---- Slice to last `window_days` calendar days (with hardening) ----\n",
    "        # Safety: ensure Date is datetime\n",
    "        tm.df_full['Date'] = pd.to_datetime(tm.df_full['Date'])\n",
    "\n",
    "        latest_date = tm.df_full['Date'].max()\n",
    "        week_start = latest_date - pd.Timedelta(days=window_days - 1)\n",
    "\n",
    "        inference_data = tm.df_full[\n",
    "            (tm.df_full['Date'] >= week_start) &\n",
    "            (tm.df_full['Date'] <= latest_date)\n",
    "        ].copy()\n",
    "\n",
    "        if inference_data.empty:\n",
    "            raise ValueError(\n",
    "                f\"No inference rows between {week_start.date()} and {latest_date.date()}. \"\n",
    "                \"Check upstream dates/timezones or reduce window_days.\"\n",
    "            )\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        print(f\"   📊 Inference window: {week_start.date()} → {latest_date.date()}  |  Rows: {len(inference_data)}\")\n",
    "\n",
    "        return tm, inference_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ TrainModel preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Prepare data for inference (last 7 calendar days)\n",
    "    tm, inference_data = prepare_for_model_inference(full_data, window_days=7)\n",
    "\n",
    "    # Load trained model\n",
    "    print(\"\\n📂 Loading trained model...\")\n",
    "    model, feature_cols, target_col = load_model_and_features(str(ARTIFACTS_DIR))\n",
    "\n",
    "    print(f\"✅ Model loaded:\")\n",
    "    print(f\"   Type: {type(model).__name__}\")\n",
    "    print(f\"   Features expected: {len(feature_cols)}\")\n",
    "    print(f\"   Target: {target_col}\")\n",
    "\n",
    "    # Set up TrainModel for inference (same as predictions.py)\n",
    "    tm.model = model\n",
    "    tm._inference_feature_columns = feature_cols\n",
    "    if target_col:\n",
    "        tm.target_col = target_col\n",
    "\n",
    "    print(\"✅ Model setup complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model setup or prediction failed: {e}\")\n",
    "    print(\"Make sure you have a trained model in the artifacts directory\")\n",
    "    print(\"Run: python run_model_training.py --mode basic\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "301771cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug info:\n",
      "   Data shape: (1380434, 620)\n",
      "   Available features (sample): ['growth_1d', 'growth_3d', 'growth_7d', 'growth_30d', 'growth_90d', 'growth_252d', 'growth_365d', 'growth_btc_1d', 'growth_btc_3d', 'growth_btc_7d', 'growth_btc_30d', 'growth_btc_90d', 'growth_btc_252d', 'growth_btc_365d', 'growth_vix_1d', 'growth_vix_3d', 'growth_vix_7d', 'growth_vix_30d', 'growth_vix_90d', 'growth_vix_252d', 'growth_vix_365d', 'growth_dax_1d', 'growth_dax_3d', 'growth_dax_7d', 'growth_dax_30d', 'growth_dax_90d', 'growth_dax_252d', 'growth_dax_365d', 'growth_snp500_1d', 'growth_snp500_3d', 'growth_snp500_7d', 'growth_snp500_30d', 'growth_snp500_90d', 'growth_snp500_252d', 'growth_snp500_365d', 'growth_dji_1d', 'growth_dji_3d', 'growth_dji_7d', 'growth_dji_30d', 'growth_dji_90d', 'growth_dji_252d', 'growth_dji_365d', 'growth_epi_1d', 'growth_epi_3d', 'growth_epi_7d', 'growth_epi_30d', 'growth_epi_90d', 'growth_epi_252d', 'growth_epi_365d', 'growth_gold_1d', 'growth_gold_3d', 'growth_gold_7d', 'growth_gold_30d', 'growth_gold_90d', 'growth_gold_252d', 'growth_gold_365d', 'growth_brent_oil_1d', 'growth_brent_oil_3d', 'growth_brent_oil_7d', 'growth_brent_oil_30d', 'growth_brent_oil_90d', 'growth_brent_oil_252d', 'growth_brent_oil_365d', 'growth_crude_oil_1d', 'growth_crude_oil_3d', 'growth_crude_oil_7d', 'growth_crude_oil_30d', 'growth_crude_oil_90d', 'growth_crude_oil_252d', 'growth_crude_oil_365d', 'adx', 'adxr', 'apo', 'bop', 'cci', 'cmo', 'dx', 'mfi', 'mom', 'ppo', 'roc', 'rocp', 'rocr', 'rocr100', 'rsi', 'trix', 'ultosc', 'willr', 'macd', 'macd_signal', 'macd_hist', 'macd_ext', 'macd_signal_ext', 'macd_hist_ext', 'macd_fix', 'macd_signal_fix', 'macd_hist_fix', 'aroon_up', 'aroon_down', 'aroonosc', 'stoch_slowk', 'stoch_slowd', 'stoch_fastk', 'stoch_fastd', 'stochrsi_fastk', 'stochrsi_fastd', 'ad', 'adosc', 'obv', 'atr', 'natr', 'trange', 'plus_di', 'minus_di', 'plus_dm', 'avgprice', 'medprice', 'typprice', 'wclprice', 'ht_dcperiod', 'ht_dcphase', 'ht_phasor_inphase', 'ht_phasor_quadrature', 'ht_sine_sine', 'ht_sine_leadsine', 'ht_trendmode', 'cdl2crows', 'cdl3blackcrows', 'cdl3inside', 'cdl3linestrike', 'cdl3outside', 'cdl3starsinsouth', 'cdl3whitesoldiers', 'cdlabandonedbaby', 'cdladvanceblock', 'cdlbelthold', 'cdlbreakaway', 'cdlclosingmarubozu', 'cdlconcealbabyswall', 'cdlcounterattack', 'cdldarkcloudcover', 'cdldoji', 'cdldojistar', 'cdldragonflydoji', 'cdlengulfing', 'cdleveningdojistar', 'cdleveningstar', 'cdlgapsidesidewhite', 'cdlgravestonedoji', 'cdlhammer', 'cdlhangingman', 'cdlharami', 'cdlharamicross', 'cdlhighwave', 'cdlhikkake', 'cdlhikkakemod', 'cdlhomingpigeon', 'cdlidentical3crows', 'cdlinneck', 'cdlinvertedhammer', 'cdlkicking', 'cdlkickingbylength', 'cdlladderbottom', 'cdllongleggeddoji', 'cdllongline', 'cdlmarubozu', 'cdlmatchinglow', 'cdlmathold', 'cdlmorningdojistar', 'cdlmorningstar', 'cdlonneck', 'cdlpiercing', 'cdlrickshawman', 'cdlrisefall3methods', 'cdlseparatinglines', 'cdlshootingstar', 'cdlshortline', 'cdlspinningtop', 'cdlstalledpattern', 'cdlsticksandwich', 'cdltakuri', 'cdltasukigap', 'cdlthrusting', 'cdltristar', 'cdlunique3river', 'cdlupsidegap2crows', 'cdlxsidegap3methods', 'sma10', 'sma20', 'growing_moving_average', 'volatility', 'sharpe', 'high_minus_low_relative', 'ln_volume', 'gdppot_yoy', 'gdppot_qoq', 'cpilfesl_yoy', 'cpilfesl_qoq', 'fedfunds_yoy', 'fedfunds_qoq', 'dgs1_yoy', 'dgs1_qoq', 'dgs5_yoy', 'dgs5_qoq', 'dgs10_yoy', 'dgs10_qoq', 'month_1', 'month_10', 'month_11', 'month_12', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'Ticker_AAPL', 'Ticker_ABBV', 'Ticker_ABNB', 'Ticker_ABT', 'Ticker_ACN', 'Ticker_ADBE', 'Ticker_ADI', 'Ticker_ADP', 'Ticker_ADSK', 'Ticker_AEP', 'Ticker_AJG', 'Ticker_ALB', 'Ticker_AMAT', 'Ticker_AMD', 'Ticker_AMGN', 'Ticker_AMT', 'Ticker_AMZN', 'Ticker_ANET', 'Ticker_AON', 'Ticker_APH', 'Ticker_APO', 'Ticker_APP', 'Ticker_ARM', 'Ticker_ASML', 'Ticker_AVGO', 'Ticker_AXON', 'Ticker_AXP', 'Ticker_AZN', 'Ticker_AZO', 'Ticker_BA', 'Ticker_BAC', 'Ticker_BDX', 'Ticker_BKNG', 'Ticker_BLK', 'Ticker_BMY', 'Ticker_BRK-B', 'Ticker_BSX', 'Ticker_BX', 'Ticker_C', 'Ticker_CAH', 'Ticker_CARR', 'Ticker_CAT', 'Ticker_CB', 'Ticker_CCI', 'Ticker_CCL', 'Ticker_CDNS', 'Ticker_CEG', 'Ticker_CHTR', 'Ticker_CI', 'Ticker_CL', 'Ticker_CMCSA', 'Ticker_CME', 'Ticker_CMG', 'Ticker_CNC', 'Ticker_COF', 'Ticker_COIN', 'Ticker_COP', 'Ticker_COR', 'Ticker_COST', 'Ticker_CRM', 'Ticker_CRWD', 'Ticker_CSCO', 'Ticker_CSX', 'Ticker_CTAS', 'Ticker_CVS', 'Ticker_CVX', 'Ticker_DAL', 'Ticker_DASH', 'Ticker_DDOG', 'Ticker_DE', 'Ticker_DECK', 'Ticker_DELL', 'Ticker_DHI', 'Ticker_DHR', 'Ticker_DIS', 'Ticker_DLTR', 'Ticker_DOW', 'Ticker_DUK', 'Ticker_EA', 'Ticker_EBAY', 'Ticker_EL', 'Ticker_ELV', 'Ticker_EMR', 'Ticker_EQIX', 'Ticker_EQT', 'Ticker_ETN', 'Ticker_EW', 'Ticker_EXPE', 'Ticker_F', 'Ticker_FCX', 'Ticker_FDX', 'Ticker_FI', 'Ticker_FICO', 'Ticker_FSLR', 'Ticker_FTNT', 'Ticker_GE', 'Ticker_GEV', 'Ticker_GILD', 'Ticker_GLW', 'Ticker_GM', 'Ticker_GOOG', 'Ticker_GOOGL', 'Ticker_GS', 'Ticker_HBAN', 'Ticker_HCA', 'Ticker_HD', 'Ticker_HLT', 'Ticker_HON', 'Ticker_HPE', 'Ticker_HUM', 'Ticker_HWM', 'Ticker_IBKR', 'Ticker_IBM', 'Ticker_ICE', 'Ticker_IDXX', 'Ticker_INTC', 'Ticker_INTU', 'Ticker_ISRG', 'Ticker_IT', 'Ticker_JCI', 'Ticker_JNJ', 'Ticker_JPM', 'Ticker_KDP', 'Ticker_KEY', 'Ticker_KHC', 'Ticker_KKR', 'Ticker_KLAC', 'Ticker_KMI', 'Ticker_KO', 'Ticker_KR', 'Ticker_KVUE', 'Ticker_LEN', 'Ticker_LHX', 'Ticker_LIN', 'Ticker_LLY', 'Ticker_LMT', 'Ticker_LOW', 'Ticker_LRCX', 'Ticker_LULU', 'Ticker_MA', 'Ticker_MAR', 'Ticker_MCD', 'Ticker_MCHP', 'Ticker_MCK', 'Ticker_MCO', 'Ticker_MDLZ', 'Ticker_MDT', 'Ticker_MELI', 'Ticker_META', 'Ticker_MMC', 'Ticker_MMM', 'Ticker_MNST', 'Ticker_MO', 'Ticker_MPWR', 'Ticker_MRK', 'Ticker_MRVL', 'Ticker_MS', 'Ticker_MSCI', 'Ticker_MSFT', 'Ticker_MSI', 'Ticker_MSTR', 'Ticker_MU', 'Ticker_NCLH', 'Ticker_NEE', 'Ticker_NEM', 'Ticker_NFLX', 'Ticker_NKE', 'Ticker_NOC', 'Ticker_NOW', 'Ticker_NRG', 'Ticker_NSC', 'Ticker_NVDA', 'Ticker_NXPI', 'Ticker_ON', 'Ticker_ORCL', 'Ticker_ORLY', 'Ticker_OTIS', 'Ticker_OXY', 'Ticker_PANW', 'Ticker_PCG', 'Ticker_PDD', 'Ticker_PEP', 'Ticker_PFE', 'Ticker_PG', 'Ticker_PGR', 'Ticker_PH', 'Ticker_PLD', 'Ticker_PLTR', 'Ticker_PM', 'Ticker_PWR', 'Ticker_PYPL', 'Ticker_QCOM', 'Ticker_RCL', 'Ticker_REGN', 'Ticker_RF', 'Ticker_ROP', 'Ticker_ROST', 'Ticker_RTX', 'Ticker_SBUX', 'Ticker_SCHW', 'Ticker_SHOP', 'Ticker_SHW', 'Ticker_SLB', 'Ticker_SMCI', 'Ticker_SNPS', 'Ticker_SO', 'Ticker_SPGI', 'Ticker_STX', 'Ticker_SYK', 'Ticker_T', 'Ticker_TDG', 'Ticker_TEAM', 'Ticker_TEL', 'Ticker_TER', 'Ticker_TGT', 'Ticker_TJX', 'Ticker_TMO', 'Ticker_TMUS', 'Ticker_TPR', 'Ticker_TRI', 'Ticker_TSLA', 'Ticker_TT', 'Ticker_TTD', 'Ticker_TTWO', 'Ticker_TXN', 'Ticker_UAL', 'Ticker_UBER', 'Ticker_ULTA', 'Ticker_UNH', 'Ticker_UNP', 'Ticker_UPS', 'Ticker_URI', 'Ticker_USB', 'Ticker_V', 'Ticker_VLO', 'Ticker_VRTX', 'Ticker_VST', 'Ticker_VZ', 'Ticker_WBD', 'Ticker_WDAY', 'Ticker_WDC', 'Ticker_WELL', 'Ticker_WFC', 'Ticker_WM', 'Ticker_WMB', 'Ticker_WMT', 'Ticker_XOM', 'Ticker_XYZ', 'Ticker_ZS', 'Ticker_ZTS', 'ticker_type_US', 'year_1962', 'year_1963', 'year_1964', 'year_1965', 'year_1966', 'year_1967', 'year_1968', 'year_1969', 'year_1970', 'year_1971', 'year_1972', 'year_1973', 'year_1974', 'year_1975', 'year_1976', 'year_1977', 'year_1978', 'year_1979', 'year_1980', 'year_1981', 'year_1982', 'year_1983', 'year_1984', 'year_1985', 'year_1986', 'year_1987', 'year_1988', 'year_1989', 'year_1990', 'year_1991', 'year_1992', 'year_1993', 'year_1994', 'year_1995', 'year_1996', 'year_1997', 'year_1998', 'year_1999', 'year_2000', 'year_2001', 'year_2002', 'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007', 'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016', 'year_2017', 'year_2018', 'year_2019', 'year_2020', 'year_2021', 'year_2022', 'year_2023', 'year_2024', 'year_2025', 'wom_1', 'wom_2', 'wom_3', 'wom_4', 'wom_5', 'month_wom_April_w1', 'month_wom_April_w2', 'month_wom_April_w3', 'month_wom_April_w4', 'month_wom_April_w5', 'month_wom_August_w1', 'month_wom_August_w2', 'month_wom_August_w3', 'month_wom_August_w4', 'month_wom_August_w5', 'month_wom_December_w1', 'month_wom_December_w2', 'month_wom_December_w3', 'month_wom_December_w4', 'month_wom_December_w5', 'month_wom_February_w1', 'month_wom_February_w2', 'month_wom_February_w3', 'month_wom_February_w4', 'month_wom_February_w5', 'month_wom_January_w1', 'month_wom_January_w2', 'month_wom_January_w3', 'month_wom_January_w4', 'month_wom_January_w5', 'month_wom_July_w1', 'month_wom_July_w2', 'month_wom_July_w3', 'month_wom_July_w4', 'month_wom_July_w5', 'month_wom_June_w1', 'month_wom_June_w2', 'month_wom_June_w3', 'month_wom_June_w4', 'month_wom_June_w5', 'month_wom_March_w1', 'month_wom_March_w2', 'month_wom_March_w3', 'month_wom_March_w4', 'month_wom_March_w5', 'month_wom_May_w1', 'month_wom_May_w2', 'month_wom_May_w3', 'month_wom_May_w4', 'month_wom_May_w5', 'month_wom_November_w1', 'month_wom_November_w2', 'month_wom_November_w3', 'month_wom_November_w4', 'month_wom_November_w5', 'month_wom_October_w1', 'month_wom_October_w2', 'month_wom_October_w3', 'month_wom_October_w4', 'month_wom_October_w5', 'month_wom_September_w1', 'month_wom_September_w2', 'month_wom_September_w3', 'month_wom_September_w4', 'month_wom_September_w5']\n",
      "   Missing features (sample): []\n"
     ]
    }
   ],
   "source": [
    "print(f\"Debug info:\")\n",
    "print(f\"   Data shape: {tm.df_full.shape if hasattr(tm, 'df_full') else 'N/A'}\")\n",
    "if 'feature_cols' in locals() and hasattr(tm, 'df_full'):\n",
    "    available = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "    missing = [f for f in feature_cols if f not in tm.df_full.columns]\n",
    "    print(f\"   Available features (sample): {available}\")\n",
    "    print(f\"   Missing features (sample): {missing}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9463fa95",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# STEP 5: GENERATE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: GENERATE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Generate predictions using TrainModel.make_inference (same as predictions.py)\n",
    "    print(\"🎯 Generating predictions using TrainModel.make_inference()...\")\n",
    "    \n",
    "    prediction_results = tm.make_inference(\"realtime_probs\")\n",
    "    \n",
    "    print(f\"✅ Predictions generated!\")\n",
    "    \n",
    "    # Get prediction data with probabilities\n",
    "    latest_date = tm.df_full['Date'].max()\n",
    "    prediction_data = tm.df_full[tm.df_full['Date'] == latest_date].copy()\n",
    "    \n",
    "    if 'realtime_probs' in prediction_data.columns:\n",
    "        probabilities = prediction_data['realtime_probs'].values\n",
    "        \n",
    "        print(f\"   📊 Prediction summary:\")\n",
    "        print(f\"      Probability range: {probabilities.min():.3f} to {probabilities.max():.3f}\")\n",
    "        print(f\"      Mean probability: {probabilities.mean():.3f}\")\n",
    "        print(f\"      Std deviation: {probabilities.std():.3f}\")\n",
    "        \n",
    "        # Add additional columns\n",
    "        prediction_data['probability'] = probabilities\n",
    "        prediction_data['prediction'] = (probabilities >= 0.5).astype(int)\n",
    "        prediction_data['rank'] = prediction_data['probability'].rank(ascending=False)\n",
    "        prediction_data['percentile'] = prediction_data['probability'].rank(pct=True)\n",
    "        \n",
    "        positive_preds = prediction_data['prediction'].sum()\n",
    "        print(f\"      Positive predictions (>50%): {positive_preds}/{len(prediction_data)}\")\n",
    "        \n",
    "        # Feature alignment info\n",
    "        available_features = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "        print(f\"      Feature alignment: {len(available_features)}/{len(feature_cols)} ({len(available_features)/len(feature_cols):.1%})\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Prediction column 'realtime_probs' not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Prediction generation failed: {e}\")\n",
    "    \n",
    "    # Debug info\n",
    "    if 'tm' in locals():\n",
    "        print(f\"Debug info:\")\n",
    "        print(f\"   Data shape: {tm.df_full.shape if hasattr(tm, 'df_full') else 'N/A'}\")\n",
    "        if 'feature_cols' in locals() and hasattr(tm, 'df_full'):\n",
    "            available = [f for f in feature_cols[:10] if f in tm.df_full.columns]\n",
    "            missing = [f for f in feature_cols[:10] if f not in tm.df_full.columns]\n",
    "            print(f\"   Available features (sample): {available}\")\n",
    "            print(f\"   Missing features (sample): {missing}\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70f68d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: GENERATE PREDICTIONS\n",
      "============================================================\n",
      "🎯 Generating predictions using TrainModel.make_inference()...\n",
      "Generated predictions 'realtime_probs' and 'realtime_probs_rank'\n",
      "✅ Predictions generated!\n",
      "   📊 Window summary:\n",
      "      Dates: 2025-09-08 → 2025-09-12 (5 trading day(s))\n",
      "      Rows with probs: 1250\n",
      "      Probability range: 0.490 to 0.544\n",
      "      Mean probability: 0.519\n",
      "      Std deviation: 0.014\n",
      "   📅 Latest day: 2025-09-12 | rows: 250\n",
      "      Positive predictions (>50%): 250/250\n",
      "      Feature alignment: 603/603 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: GENERATE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: GENERATE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Generate predictions using TrainModel.make_inference (same as predictions.py)\n",
    "    print(\"🎯 Generating predictions using TrainModel.make_inference()...\")\n",
    "    prediction_results = tm.make_inference(\"realtime_probs\")\n",
    "    print(f\"✅ Predictions generated!\")\n",
    "\n",
    "    # Use the same 7-day inference window prepared in STEP 4\n",
    "    # (tm.df_full already contains the column added by make_inference)\n",
    "    if 'realtime_probs' not in tm.df_full.columns:\n",
    "        raise ValueError(\"Prediction column 'realtime_probs' not found after make_inference().\")\n",
    "\n",
    "    # Limit to the window you prepared earlier\n",
    "    latest_date = inference_data['Date'].max()\n",
    "    week_start = inference_data['Date'].min()\n",
    "    prediction_data = tm.df_full[\n",
    "        (tm.df_full['Date'] >= week_start) &\n",
    "        (tm.df_full['Date'] <= latest_date)\n",
    "    ].copy()\n",
    "\n",
    "    # Keep only rows that have probabilities\n",
    "    prediction_data = prediction_data.loc[prediction_data['realtime_probs'].notna()].copy()\n",
    "\n",
    "    if prediction_data.empty:\n",
    "        raise ValueError(\n",
    "            f\"No rows with 'realtime_probs' between {week_start.date()} and {latest_date.date()}.\"\n",
    "        )\n",
    "\n",
    "    # Add derived columns\n",
    "    probs = prediction_data['realtime_probs'].astype(float)\n",
    "    prediction_data['probability'] = probs\n",
    "    prediction_data['prediction'] = (probs >= 0.5).astype(int)\n",
    "\n",
    "    # Rank within each Date (highest prob = rank 1)\n",
    "    prediction_data['rank'] = prediction_data.groupby('Date')['probability'].rank(ascending=False, method='first')\n",
    "    prediction_data['percentile'] = prediction_data.groupby('Date')['probability'].rank(pct=True)\n",
    "\n",
    "    # --- Summary prints ---\n",
    "    print(\"   📊 Window summary:\")\n",
    "    print(f\"      Dates: {week_start.date()} → {latest_date.date()} \"\n",
    "          f\"({prediction_data['Date'].nunique()} trading day(s))\")\n",
    "    print(f\"      Rows with probs: {len(prediction_data)}\")\n",
    "    print(f\"      Probability range: {probs.min():.3f} to {probs.max():.3f}\")\n",
    "    print(f\"      Mean probability: {probs.mean():.3f}\")\n",
    "    print(f\"      Std deviation: {probs.std():.3f}\")\n",
    "\n",
    "    # Latest-day quick view (preserves your old behavior)\n",
    "    latest_slice = prediction_data[prediction_data['Date'] == latest_date].copy()\n",
    "    positive_preds = latest_slice['prediction'].sum()\n",
    "    print(f\"   📅 Latest day: {latest_date.date()} | rows: {len(latest_slice)}\")\n",
    "    print(f\"      Positive predictions (>50%): {positive_preds}/{len(latest_slice)}\")\n",
    "\n",
    "    # Feature alignment info (same as before)\n",
    "    available_features = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "    print(f\"      Feature alignment: {len(available_features)}/{len(feature_cols)} \"\n",
    "          f\"({len(available_features)/len(feature_cols):.1%})\")\n",
    "\n",
    "    # If you need these dataframes elsewhere, they are:\n",
    "    # - prediction_data: full 7-day window with probs/preds/ranks\n",
    "    # - latest_slice   : latest day only\n",
    "    # (Return or save as needed in your workflow.)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Prediction generation failed: {e}\")\n",
    "    # Debug info\n",
    "    if 'tm' in locals():\n",
    "        print(\"Debug info:\")\n",
    "        print(f\"   Data shape: {tm.df_full.shape if hasattr(tm, 'df_full') else 'N/A'}\")\n",
    "        if 'feature_cols' in locals() and hasattr(tm, 'df_full'):\n",
    "            available = [f for f in feature_cols[:10] if f in tm.df_full.columns]\n",
    "            missing = [f for f in feature_cols[:10] if f not in tm.df_full.columns]\n",
    "            print(f\"   Available features (sample): {available}\")\n",
    "            print(f\"   Missing features (sample): {missing}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3421204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>month_wom_September_w3</th>\n",
       "      <th>month_wom_September_w4</th>\n",
       "      <th>month_wom_September_w5</th>\n",
       "      <th>split</th>\n",
       "      <th>realtime_probs</th>\n",
       "      <th>realtime_probs_rank</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>rank</th>\n",
       "      <th>percentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11268</th>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>239.300003</td>\n",
       "      <td>240.149994</td>\n",
       "      <td>236.339996</td>\n",
       "      <td>237.880005</td>\n",
       "      <td>237.880005</td>\n",
       "      <td>48999500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.529511</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.529511</td>\n",
       "      <td>1</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11269</th>\n",
       "      <td>2025-09-09</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>238.779999</td>\n",
       "      <td>233.360001</td>\n",
       "      <td>234.350006</td>\n",
       "      <td>234.350006</td>\n",
       "      <td>66313900.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.531044</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.531044</td>\n",
       "      <td>1</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11270</th>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>232.190002</td>\n",
       "      <td>232.419998</td>\n",
       "      <td>225.949997</td>\n",
       "      <td>226.789993</td>\n",
       "      <td>226.789993</td>\n",
       "      <td>83440800.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>1</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11271</th>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>226.880005</td>\n",
       "      <td>230.449997</td>\n",
       "      <td>226.649994</td>\n",
       "      <td>230.029999</td>\n",
       "      <td>230.029999</td>\n",
       "      <td>50208600.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.494642</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.494642</td>\n",
       "      <td>0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11272</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>229.220001</td>\n",
       "      <td>234.509995</td>\n",
       "      <td>229.020004</td>\n",
       "      <td>234.070007</td>\n",
       "      <td>234.070007</td>\n",
       "      <td>55776500.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>242.0</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>1</td>\n",
       "      <td>242.0</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   adj_close  \\\n",
       "11268 2025-09-08  239.300003  240.149994  236.339996  237.880005  237.880005   \n",
       "11269 2025-09-09  237.000000  238.779999  233.360001  234.350006  234.350006   \n",
       "11270 2025-09-10  232.190002  232.419998  225.949997  226.789993  226.789993   \n",
       "11271 2025-09-11  226.880005  230.449997  226.649994  230.029999  230.029999   \n",
       "11272 2025-09-12  229.220001  234.509995  229.020004  234.070007  234.070007   \n",
       "\n",
       "           Volume Ticker  year month  ... month_wom_September_w3  \\\n",
       "11268  48999500.0   AAPL  2025     9  ...                      0   \n",
       "11269  66313900.0   AAPL  2025     9  ...                      0   \n",
       "11270  83440800.0   AAPL  2025     9  ...                      0   \n",
       "11271  50208600.0   AAPL  2025     9  ...                      0   \n",
       "11272  55776500.0   AAPL  2025     9  ...                      0   \n",
       "\n",
       "      month_wom_September_w4 month_wom_September_w5  split  realtime_probs  \\\n",
       "11268                      0                      0   test        0.529511   \n",
       "11269                      0                      0   test        0.531044   \n",
       "11270                      0                      0   test        0.511600   \n",
       "11271                      0                      0   test        0.494642   \n",
       "11272                      0                      0   test        0.509812   \n",
       "\n",
       "       realtime_probs_rank  probability  prediction   rank  percentile  \n",
       "11268                240.0     0.529511           1  240.0       0.044  \n",
       "11269                248.0     0.531044           1  248.0       0.012  \n",
       "11270                207.0     0.511600           1  207.0       0.176  \n",
       "11271                227.0     0.494642           0  227.0       0.096  \n",
       "11272                242.0     0.509812           1  242.0       0.036  \n",
       "\n",
       "[5 rows x 626 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c37ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: TRADING SIGNALS & DECISIONS\n",
      "============================================================\n",
      "📊 Dynamic thresholds:\n",
      "   High confidence (top 10%): 0.537\n",
      "   Medium confidence (top 25%): 0.534\n",
      "   Low confidence (top 40%): 0.524\n",
      "\n",
      "🎯 TOP 10 PICKS FOR 2025-09-08:\n",
      "================================================================================\n",
      " Rank Ticker Probability       Signal Investment    Price\n",
      "    1    DOW       54.4% 🟢 STRONG BUY     $1,500   $23.98\n",
      "    2   KVUE       54.3% 🟢 STRONG BUY     $1,500   $18.87\n",
      "    3   KVUE       54.2% 🟢 STRONG BUY     $1,500   $18.43\n",
      "    4    DOW       54.1% 🟢 STRONG BUY     $1,500   $24.14\n",
      "    5    PFE       54.1% 🟢 STRONG BUY     $1,500   $24.71\n",
      "    6    BLK       54.1% 🟢 STRONG BUY     $1,500 $1105.67\n",
      "    7   DELL       54.1% 🟢 STRONG BUY     $1,500  $121.29\n",
      "    8    COR       54.1% 🟢 STRONG BUY     $1,500  $297.86\n",
      "    9     PM       54.0% 🟢 STRONG BUY     $1,500  $164.74\n",
      "   10   AXON       54.0% 🟢 STRONG BUY     $1,500  $731.98\n",
      "\n",
      "📋 ACTION PLAN:\n",
      "============================================================\n",
      "🟢 IMMEDIATE ACTION (10 stocks):\n",
      "   • DOW    - $1,500 (prob: 54.4%)\n",
      "   • KVUE   - $1,500 (prob: 54.3%)\n",
      "   • KVUE   - $1,500 (prob: 54.2%)\n",
      "   • DOW    - $1,500 (prob: 54.1%)\n",
      "   • PFE    - $1,500 (prob: 54.1%)\n",
      "   • BLK    - $1,500 (prob: 54.1%)\n",
      "   • DELL   - $1,500 (prob: 54.1%)\n",
      "   • COR    - $1,500 (prob: 54.1%)\n",
      "   • PM     - $1,500 (prob: 54.0%)\n",
      "   • AXON   - $1,500 (prob: 54.0%)\n",
      "\n",
      "💰 TOTAL INVESTMENT RECOMMENDED: $15,000\n",
      "\n",
      "📊 SUMMARY:\n",
      "========================================\n",
      "   Analysis date: 2025-09-08\n",
      "   Stocks analyzed: 1250\n",
      "   Strong buy signals: 10\n",
      "   Buy signals: 0\n",
      "   Watch list: 0\n",
      "   Total capital needed: $15,000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: CREATE TRADING SIGNALS AND DECISIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: TRADING SIGNALS & DECISIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_trading_signals(data):\n",
    "    \"\"\"Create trading signals based on prediction probabilities\"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate dynamic thresholds\n",
    "    high_conf = df['probability'].quantile(0.90)  # Top 10%\n",
    "    med_conf = df['probability'].quantile(0.75)   # Top 25%\n",
    "    low_conf = df['probability'].quantile(0.60)   # Top 40%\n",
    "    \n",
    "    print(f\"📊 Dynamic thresholds:\")\n",
    "    print(f\"   High confidence (top 10%): {high_conf:.3f}\")\n",
    "    print(f\"   Medium confidence (top 25%): {med_conf:.3f}\")\n",
    "    print(f\"   Low confidence (top 40%): {low_conf:.3f}\")\n",
    "    \n",
    "    # Create signals\n",
    "    conditions = [\n",
    "        (df['probability'] >= high_conf),\n",
    "        (df['probability'] >= med_conf),\n",
    "        (df['probability'] >= low_conf),\n",
    "        (df['probability'] >= 0.5)\n",
    "    ]\n",
    "    \n",
    "    choices = ['🟢 STRONG BUY', '🟡 BUY', '🟠 CONSIDER', '🔵 WEAK BUY']\n",
    "    df['signal'] = np.select(conditions, choices, default='🔴 PASS')\n",
    "    \n",
    "    # Investment recommendations\n",
    "    df['investment'] = 0\n",
    "    df.loc[df['signal'].str.contains('STRONG'), 'investment'] = INVESTMENT_AMOUNT * 1.5\n",
    "    df.loc[df['signal'] == '🟡 BUY', 'investment'] = INVESTMENT_AMOUNT\n",
    "    df.loc[df['signal'].str.contains('CONSIDER'), 'investment'] = INVESTMENT_AMOUNT * 0.5\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_top_picks_analysis(data, top_n=TOP_N_PICKS):\n",
    "    \"\"\"Analyze top picks and create actionable recommendations\"\"\"\n",
    "    \n",
    "    # Sort by probability\n",
    "    top_picks = data.nlargest(top_n, 'probability')\n",
    "    \n",
    "    print(f\"\\n🎯 TOP {top_n} PICKS FOR {data['Date'].iloc[0].strftime('%Y-%m-%d')}:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create display\n",
    "    display_data = []\n",
    "    for i, (_, stock) in enumerate(top_picks.iterrows(), 1):\n",
    "        \n",
    "        display_data.append({\n",
    "            'Rank': i,\n",
    "            'Ticker': stock['Ticker'],\n",
    "            'Probability': f\"{stock['probability']:.1%}\",\n",
    "            'Signal': stock['signal'],\n",
    "            'Investment': f\"${int(stock['investment']):,}\" if stock['investment'] > 0 else \"-\",\n",
    "            'Price': f\"${stock.get('Close', 0):.2f}\" if 'Close' in stock and stock.get('Close', 0) > 0 else \"N/A\"\n",
    "        })\n",
    "    \n",
    "    display_df = pd.DataFrame(display_data)\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    return top_picks\n",
    "\n",
    "def create_action_plan(top_picks):\n",
    "    \"\"\"Create executable trading action plan\"\"\"\n",
    "    \n",
    "    print(f\"\\n📋 ACTION PLAN:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Group by signal type\n",
    "    strong_buys = top_picks[top_picks['signal'].str.contains('STRONG')]\n",
    "    buys = top_picks[top_picks['signal'] == '🟡 BUY']\n",
    "    considers = top_picks[top_picks['signal'].str.contains('CONSIDER')]\n",
    "    \n",
    "    total_investment = 0\n",
    "    \n",
    "    if len(strong_buys) > 0:\n",
    "        print(f\"🟢 IMMEDIATE ACTION ({len(strong_buys)} stocks):\")\n",
    "        for _, stock in strong_buys.iterrows():\n",
    "            investment = int(stock['investment'])\n",
    "            total_investment += investment\n",
    "            print(f\"   • {stock['Ticker']:6s} - ${investment:,} (prob: {stock['probability']:.1%})\")\n",
    "    \n",
    "    if len(buys) > 0:\n",
    "        print(f\"\\n🟡 SECONDARY TARGETS ({len(buys)} stocks):\")\n",
    "        for _, stock in buys.iterrows():\n",
    "            investment = int(stock['investment'])\n",
    "            total_investment += investment\n",
    "            print(f\"   • {stock['Ticker']:6s} - ${investment:,} (prob: {stock['probability']:.1%})\")\n",
    "    \n",
    "    if len(considers) > 0:\n",
    "        print(f\"\\n🟠 WATCH LIST ({len(considers)} stocks):\")\n",
    "        for _, stock in considers.iterrows():\n",
    "            print(f\"   • {stock['Ticker']:6s} - Monitor (prob: {stock['probability']:.1%})\")\n",
    "    \n",
    "    print(f\"\\n💰 TOTAL INVESTMENT RECOMMENDED: ${total_investment:,}\")\n",
    "    \n",
    "    return {\n",
    "        'strong_buys': strong_buys,\n",
    "        'buys': buys, \n",
    "        'considers': considers,\n",
    "        'total_investment': total_investment\n",
    "    }\n",
    "\n",
    "# Generate trading signals and analysis\n",
    "try:\n",
    "    # Create signals\n",
    "    prediction_data = create_trading_signals(prediction_data)\n",
    "    \n",
    "    # Analyze top picks\n",
    "    top_picks = create_top_picks_analysis(prediction_data, TOP_N_PICKS)\n",
    "    \n",
    "    # Create action plan\n",
    "    action_plan = create_action_plan(top_picks)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📊 SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"   Analysis date: {prediction_data['Date'].iloc[0].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Stocks analyzed: {len(prediction_data)}\")\n",
    "    print(f\"   Strong buy signals: {len(action_plan['strong_buys'])}\")\n",
    "    print(f\"   Buy signals: {len(action_plan['buys'])}\")\n",
    "    print(f\"   Watch list: {len(action_plan['considers'])}\")\n",
    "    print(f\"   Total capital needed: ${action_plan['total_investment']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Signal generation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fca01a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVE RESULTS\n",
      "============================================================\n",
      "💾 Results saved:\n",
      "   Predictions: /Users/sagardhal/Desktop/Practice/personal-stock/results/realtime_predictions_20250913_1338.csv\n",
      "   Action plan: /Users/sagardhal/Desktop/Practice/personal-stock/results/action_plan_20250913_1338.txt\n",
      "\n",
      "============================================================\n",
      "🚀 REAL-TIME ANALYSIS COMPLETE\n",
      "============================================================\n",
      "✅ Analysis successful!\n",
      "📅 Data date: 2025-09-12\n",
      "🎯 Stocks analyzed: 250\n",
      "💰 Total recommendations: $15,000\n",
      "\n",
      "🏆 TOP RECOMMENDATION:\n",
      "   KVUE - 52.3% confidence\n",
      "   Investment: $1,500\n",
      "\n",
      "⚠️ IMPORTANT REMINDERS:\n",
      "• Set stop losses at -15% to -20%\n",
      "• Don't invest more than 5-10% per position\n",
      "• Monitor positions daily\n",
      "• This is based on historical patterns only\n",
      "\n",
      "🔄 NEXT STEPS:\n",
      "1. Execute strong buy signals\n",
      "2. Set stop loss orders\n",
      "3. Monitor throughout trading day\n",
      "4. Re-run notebook daily for fresh signals\n",
      "\n",
      "📊 Performance tracking:\n",
      "   Results saved in: /Users/sagardhal/Desktop/Practice/personal-stock/results\n",
      "   Track actual vs predicted outcomes\n",
      "   Adjust model/thresholds based on results\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SAVE RESULTS AND FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create results directory\n",
    "    RESULTS_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Timestamp for files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions_file = RESULTS_DIR / f\"realtime_predictions_{timestamp}.csv\"\n",
    "    save_cols = ['Date', 'Ticker', 'probability', 'signal', 'investment']\n",
    "    if 'Close' in prediction_data.columns:\n",
    "        save_cols.append('Close')\n",
    "    \n",
    "    prediction_data[save_cols].to_csv(predictions_file, index=False)\n",
    "    \n",
    "    # Save action plan\n",
    "    action_file = RESULTS_DIR / f\"action_plan_{timestamp}.txt\"\n",
    "    with open(action_file, 'w') as f:\n",
    "        f.write(f\"Trading Action Plan - {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"STRONG BUY:\\n\")\n",
    "        for _, stock in action_plan['strong_buys'].iterrows():\n",
    "            f.write(f\"  {stock['Ticker']} - ${int(stock['investment']):,} ({stock['probability']:.1%})\\n\")\n",
    "        \n",
    "        f.write(\"\\nBUY:\\n\")\n",
    "        for _, stock in action_plan['buys'].iterrows():\n",
    "            f.write(f\"  {stock['Ticker']} - ${int(stock['investment']):,} ({stock['probability']:.1%})\\n\")\n",
    "        \n",
    "        f.write(f\"\\nTotal Investment: ${action_plan['total_investment']:,}\\n\")\n",
    "    \n",
    "    print(f\"💾 Results saved:\")\n",
    "    print(f\"   Predictions: {predictions_file}\")\n",
    "    print(f\"   Action plan: {action_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save results: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🚀 REAL-TIME ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'action_plan' in locals():\n",
    "    print(f\"✅ Analysis successful!\")\n",
    "    print(f\"📅 Data date: {prediction_data['Date'].iloc[0].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"🎯 Stocks analyzed: {len(prediction_data)}\")\n",
    "    print(f\"💰 Total recommendations: ${action_plan['total_investment']:,}\")\n",
    "    \n",
    "    if len(action_plan['strong_buys']) > 0:\n",
    "        best_pick = action_plan['strong_buys'].iloc[0]\n",
    "        print(f\"\\n🏆 TOP RECOMMENDATION:\")\n",
    "        print(f\"   {best_pick['Ticker']} - {best_pick['probability']:.1%} confidence\")\n",
    "        print(f\"   Investment: ${int(best_pick['investment']):,}\")\n",
    "    \n",
    "    print(f\"\\n⚠️ IMPORTANT REMINDERS:\")\n",
    "    print(\"• Set stop losses at -15% to -20%\")\n",
    "    print(\"• Don't invest more than 5-10% per position\")\n",
    "    print(\"• Monitor positions daily\")\n",
    "    print(\"• This is based on historical patterns only\")\n",
    "    \n",
    "    print(f\"\\n🔄 NEXT STEPS:\")\n",
    "    print(\"1. Execute strong buy signals\")\n",
    "    print(\"2. Set stop loss orders\")\n",
    "    print(\"3. Monitor throughout trading day\")\n",
    "    print(\"4. Re-run notebook daily for fresh signals\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Analysis incomplete - check errors above\")\n",
    "\n",
    "print(f\"\\n📊 Performance tracking:\")\n",
    "print(f\"   Results saved in: {RESULTS_DIR}\")\n",
    "print(f\"   Track actual vs predicted outcomes\")\n",
    "print(f\"   Adjust model/thresholds based on results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273132d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ea131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48bf727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_model_and_features] Using model file: best_rf_model.joblib\n",
      "[load_model_and_features] feature_names_in_: 603 features\n",
      "✅ Model loaded successfully\n",
      "   Features: 603\n",
      "   Target: is_positive_growth_30d_future\n",
      "   Feature coverage: 100.0% (603/603)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "try:\n",
    "    model, feature_cols, target_col = load_model_and_features(str(ARTIFACTS_DIR))\n",
    "    tm.model = model\n",
    "    tm._inference_feature_columns = feature_cols\n",
    "    if target_col:\n",
    "        tm.target_col = target_col\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully\")\n",
    "    print(f\"   Features: {len(feature_cols)}\")\n",
    "    print(f\"   Target: {tm.target_col}\")\n",
    "    \n",
    "    # Check feature availability\n",
    "    available_features = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "    missing_features = [f for f in feature_cols if f not in tm.df_full.columns]\n",
    "    \n",
    "    feature_coverage = len(available_features) / len(feature_cols)\n",
    "    print(f\"   Feature coverage: {feature_coverage:.1%} ({len(available_features)}/{len(feature_cols)})\")\n",
    "    \n",
    "    if feature_coverage < 0.8:\n",
    "        print(f\"⚠️ Warning: Low feature coverage ({feature_coverage:.1%})\")\n",
    "        print(\"Model and data may be incompatible\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Model loading failed: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70dabd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 4: GENERATE PREDICTIONS\n",
      "==================================================\n",
      "Creating manual rule-based predictions...\n",
      "Manual prediction summary:\n",
      "  pred0_manual_cci: 2.5% positive predictions\n",
      "  pred1_manual_prev_g1: 58.2% positive predictions\n",
      "  pred2_manual_prev_g1_and_snp: 0.0% positive predictions\n",
      "  pred3_manual_declining_rates: 47.6% positive predictions\n",
      "  pred4_manual_fed_easing: 40.5% positive predictions\n",
      "  pred5_manual_vix_contrarian: 18.9% positive predictions\n",
      "  pred6_manual_stock_btc_momentum: 0.2% positive predictions\n",
      "✅ Manual predictions created\n",
      "[add_ml_predictions] Non-finite detected in: ['growth_1d', 'growth_3d', 'growth_7d', 'growth_30d', 'growth_90d', 'growth_252d', 'growth_365d', 'growth_btc_1d', 'growth_btc_3d', 'growth_btc_7d', 'growth_btc_30d', 'growth_btc_90d']...\n",
      "ML prediction summary:\n",
      "  pred10_rf_thresh_21: 100.0% positive predictions\n",
      "  pred11_rf_thresh_50: 92.6% positive predictions\n",
      "  pred12_rf_thresh_65: 20.4% positive predictions\n",
      "  pred13_rf_thresh_80: 2.1% positive predictions\n",
      "  pred14_rf_thresh_90: 0.1% positive predictions\n",
      "✅ ML predictions created\n",
      "[auto-threshold] pred15_rf_auto_rate_1p: thr=0.884 | val rate=1.00% | test rate=0.00%\n",
      "[auto-threshold] pred15_rf_auto_rate_3p: thr=0.839 | val rate=3.00% | test rate=0.25%\n",
      "[auto-threshold] pred15_rf_auto_rate_5p: thr=0.798 | val rate=5.00% | test rate=1.73%\n",
      "✅ Adaptive thresholds created\n",
      "[Top-3 daily] pred30_top3_daily: test prediction rate ~1.22%\n",
      "[Top-5 daily] pred30_top5_daily: test prediction rate ~2.03%\n",
      "✅ Top-K strategies created\n",
      "📊 Total strategies created: 17\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generate Predictions (only if everything loaded)\n",
    "#if not missing_requirements and 'tm' in locals() and 'model' in locals():\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 4: GENERATE PREDICTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create prediction comparator\n",
    "comparator = PredictionComparator(tm.df_full, tm.target_col)\n",
    "\n",
    "# Add manual rule-based predictions\n",
    "try:\n",
    "    comparator.add_manual_predictions()\n",
    "    print(\"✅ Manual predictions created\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Manual predictions failed: {e}\")\n",
    "\n",
    "# Add ML predictions\n",
    "try:\n",
    "    comparator.add_ml_predictions(model, feature_cols)\n",
    "    print(\"✅ ML predictions created\")\n",
    "    ml_success = True\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ ML predictions failed: {e}\")\n",
    "    # Create fallback probability column\n",
    "    comparator.df['rf_prob_30d'] = 0.5\n",
    "    ml_success = False\n",
    "\n",
    "# Add additional strategies if ML worked\n",
    "if ml_success and 'rf_prob_30d' in comparator.df.columns:\n",
    "    try:\n",
    "        comparator.add_ml_thresholds_from_validation(\"rf_prob_30d\")\n",
    "        print(\"✅ Adaptive thresholds created\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Adaptive thresholds failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        comparator.add_daily_topn(proba_col=\"rf_prob_30d\", n=3)\n",
    "        comparator.add_daily_topn(proba_col=\"rf_prob_30d\", n=5)\n",
    "        print(\"✅ Top-K strategies created\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Top-K strategies failed: {e}\")\n",
    "\n",
    "print(f\"📊 Total strategies created: {len(comparator.prediction_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d54bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 5: ANALYZE YOUR TICKERS\n",
      "==================================================\n",
      "📅 Latest data date: 2025-09-03 00:00:00\n",
      "📊 Your tickers in recent data: 0/250\n",
      "❌ No data found for your tickers in recent period\n",
      "Your tickers might not be in the processed dataset\n",
      "Available tickers (sample): AAPL, ADBE, AMAT, AMD, AMZN, APP, AVGO, BA, BAC, BRK-B, C, CAT, COIN, COST, CRM, CRWD, CSCO, CVX, GEV, GOOG\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 5: ANALYZE YOUR TICKERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get latest date and filter to your tickers\n",
    "latest_date = comparator.df['Date'].max()\n",
    "print(f\"📅 Latest data date: {latest_date}\")\n",
    "\n",
    "# Filter to your tickers and recent data\n",
    "your_data = comparator.df[\n",
    "    (comparator.df['Ticker'].isin(tickers)) & \n",
    "    (comparator.df['Date'] >= latest_date - timedelta(days=7))\n",
    "].copy()\n",
    "\n",
    "print(f\"📊 Your tickers in recent data: {your_data['Ticker'].nunique()}/{len(tickers)}\")\n",
    "\n",
    "if len(your_data) == 0:\n",
    "    print(\"❌ No data found for your tickers in recent period\")\n",
    "    print(\"Your tickers might not be in the processed dataset\")\n",
    "    available_tickers = comparator.df['Ticker'].unique()[:20]\n",
    "    print(f\"Available tickers (sample): {', '.join(available_tickers)}\")\n",
    "else:\n",
    "    # Get most recent data for each ticker\n",
    "    latest_by_ticker = your_data.loc[your_data.groupby('Ticker')['Date'].idxmax()]\n",
    "    \n",
    "    # Sort by prediction probability\n",
    "    prob_col = 'rf_prob_30d' if 'rf_prob_30d' in latest_by_ticker.columns else None\n",
    "    if prob_col and latest_by_ticker[prob_col].std() > 0:\n",
    "        latest_by_ticker = latest_by_ticker.sort_values(prob_col, ascending=False)\n",
    "        prob_source = \"ML Model\"\n",
    "    else:\n",
    "        # Fallback to manual prediction\n",
    "        manual_cols = [c for c in latest_by_ticker.columns if c.startswith('pred') and 'manual' in c]\n",
    "        if manual_cols:\n",
    "            prob_col = manual_cols[0]\n",
    "            latest_by_ticker = latest_by_ticker.sort_values(prob_col, ascending=False)\n",
    "            prob_source = \"Manual Rules\"\n",
    "        else:\n",
    "            prob_col = None\n",
    "            prob_source = \"None\"\n",
    "    \n",
    "    # Top picks\n",
    "    top_picks = latest_by_ticker.head(TOP_N_PICKS)\n",
    "    \n",
    "    print(f\"\\n🎯 TOP {len(top_picks)} PICKS (sorted by {prob_source}):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (_, stock) in enumerate(top_picks.iterrows(), 1):\n",
    "        ticker = stock['Ticker']\n",
    "        date = stock['Date'].strftime('%Y-%m-%d')\n",
    "        \n",
    "        if prob_col and prob_col == 'rf_prob_30d':\n",
    "            prob_value = f\"{stock[prob_col]*100:.1f}%\"\n",
    "        elif prob_col:\n",
    "            prob_value = f\"{stock[prob_col]:.0f}\"\n",
    "        else:\n",
    "            prob_value = \"N/A\"\n",
    "        \n",
    "        print(f\"{i:2d}. {ticker:6s} | Prob: {prob_value:6s} | Date: {date} | Investment: ${INVESTMENT_AMOUNT:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e3f6a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 5: ANALYZE YOUR TICKERS\n",
      "==================================================\n",
      "📅 Latest data date: 2025-09-03 00:00:00\n",
      "📊 Your tickers in recent data: 0/250\n",
      "❌ No data found for your tickers in recent period\n",
      "Your tickers might not be in the processed dataset\n",
      "Available tickers (sample): AAPL, ADBE, AMAT, AMD, AMZN, APP, AVGO, BA, BAC, BRK-B, C, CAT, COIN, COST, CRM, CRWD, CSCO, CVX, GEV, GOOG\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Analyze Your Tickers\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 5: ANALYZE YOUR TICKERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get latest date and filter to your tickers\n",
    "latest_date = comparator.df['Date'].max()\n",
    "print(f\"📅 Latest data date: {latest_date}\")\n",
    "\n",
    "# Filter to your tickers and recent data\n",
    "your_data = comparator.df[\n",
    "    (comparator.df['Ticker'].isin(tickers)) & \n",
    "    (comparator.df['Date'] >= latest_date - timedelta(days=7))\n",
    "].copy()\n",
    "\n",
    "print(f\"📊 Your tickers in recent data: {your_data['Ticker'].nunique()}/{len(tickers)}\")\n",
    "\n",
    "if len(your_data) == 0:\n",
    "    print(\"❌ No data found for your tickers in recent period\")\n",
    "    print(\"Your tickers might not be in the processed dataset\")\n",
    "    available_tickers = comparator.df['Ticker'].unique()[:20]\n",
    "    print(f\"Available tickers (sample): {', '.join(available_tickers)}\")\n",
    "else:\n",
    "    # Get most recent data for each ticker\n",
    "    latest_by_ticker = your_data.loc[your_data.groupby('Ticker')['Date'].idxmax()]\n",
    "    \n",
    "    # Sort by prediction probability\n",
    "    prob_col = 'rf_prob_30d' if 'rf_prob_30d' in latest_by_ticker.columns else None\n",
    "    if prob_col and latest_by_ticker[prob_col].std() > 0:\n",
    "        latest_by_ticker = latest_by_ticker.sort_values(prob_col, ascending=False)\n",
    "        prob_source = \"ML Model\"\n",
    "    else:\n",
    "        # Fallback to manual prediction\n",
    "        manual_cols = [c for c in latest_by_ticker.columns if c.startswith('pred') and 'manual' in c]\n",
    "        if manual_cols:\n",
    "            prob_col = manual_cols[0]\n",
    "            latest_by_ticker = latest_by_ticker.sort_values(prob_col, ascending=False)\n",
    "            prob_source = \"Manual Rules\"\n",
    "        else:\n",
    "            prob_col = None\n",
    "            prob_source = \"None\"\n",
    "    \n",
    "    # Top picks\n",
    "    top_picks = latest_by_ticker.head(TOP_N_PICKS)\n",
    "    \n",
    "    print(f\"\\n🎯 TOP {len(top_picks)} PICKS (sorted by {prob_source}):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (_, stock) in enumerate(top_picks.iterrows(), 1):\n",
    "        ticker = stock['Ticker']\n",
    "        date = stock['Date'].strftime('%Y-%m-%d')\n",
    "        \n",
    "        if prob_col and prob_col == 'rf_prob_30d':\n",
    "            prob_value = f\"{stock[prob_col]*100:.1f}%\"\n",
    "        elif prob_col:\n",
    "            prob_value = f\"{stock[prob_col]:.0f}\"\n",
    "        else:\n",
    "            prob_value = \"N/A\"\n",
    "        \n",
    "        print(f\"{i:2d}. {ticker:6s} | Prob: {prob_value:6s} | Date: {date} | Investment: ${INVESTMENT_AMOUNT:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d316414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 6: DECISION MATRIX\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top_picks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m total_strong_buy = \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m total_buy = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, stock \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtop_picks\u001b[49m.iterrows():\n\u001b[32m     11\u001b[39m     ticker = stock[\u001b[33m'\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Determine probability and action\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'top_picks' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 6: Decision Matrix\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 6: DECISION MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "decisions = []\n",
    "total_strong_buy = 0\n",
    "total_buy = 0\n",
    "\n",
    "for _, stock in top_picks.iterrows():\n",
    "    ticker = stock['Ticker']\n",
    "    \n",
    "    # Determine probability and action\n",
    "    if prob_col == 'rf_prob_30d':\n",
    "        prob = stock[prob_col]\n",
    "        prob_display = f\"{prob*100:.1f}%\"\n",
    "        \n",
    "        if prob >= 0.8:\n",
    "            action = \"🟢 STRONG BUY\"\n",
    "            total_strong_buy += 1\n",
    "        elif prob >= 0.7:\n",
    "            action = \"🟡 BUY\"\n",
    "            total_buy += 1\n",
    "        elif prob >= 0.6:\n",
    "            action = \"🟠 CONSIDER\"\n",
    "        else:\n",
    "            action = \"🔴 WAIT\"\n",
    "    else:\n",
    "        prob_display = \"Manual\"\n",
    "        action = \"🟡 BUY\" if stock.get(prob_col, 0) > 0 else \"🔴 WAIT\"\n",
    "        if action == \"🟡 BUY\":\n",
    "            total_buy += 1\n",
    "    \n",
    "    decisions.append({\n",
    "        'Rank': len(decisions) + 1,\n",
    "        'Ticker': ticker,\n",
    "        'Signal': prob_display,\n",
    "        'Action': action,\n",
    "        'Investment': f\"${INVESTMENT_AMOUNT:,}\"\n",
    "    })\n",
    "\n",
    "# Display decision table\n",
    "decision_df = pd.DataFrame(decisions)\n",
    "print(decision_df.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n💰 INVESTMENT SUMMARY:\")\n",
    "print(f\"   Strong Buy signals: {total_strong_buy}\")\n",
    "print(f\"   Buy signals: {total_buy}\")\n",
    "total_positions = total_strong_buy + total_buy\n",
    "total_investment = total_positions * INVESTMENT_AMOUNT\n",
    "print(f\"   Total positions: {total_positions}\")\n",
    "print(f\"   Total investment: ${total_investment:,}\")\n",
    "\n",
    "# Action plan\n",
    "print(f\"\\n📋 ACTION PLAN:\")\n",
    "strong_buys = [d['Ticker'] for d in decisions if 'STRONG' in d['Action']]\n",
    "buys = [d['Ticker'] for d in decisions if d['Action'] == '🟡 BUY']\n",
    "\n",
    "if strong_buys:\n",
    "    print(f\"🟢 IMMEDIATE: Buy {', '.join(strong_buys)}\")\n",
    "if buys:\n",
    "    print(f\"🟡 SECONDARY: Consider {', '.join(buys)}\")\n",
    "if not strong_buys and not buys:\n",
    "    print(\"🔴 WAIT: No strong signals today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Save Results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 7: SAVE RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "results_file = RESULTS_DIR / f\"notebook_decisions_{timestamp}.csv\"\n",
    "\n",
    "# Create tracking dataframe\n",
    "tracking_df = pd.DataFrame(decisions)\n",
    "tracking_df['Analysis_Date'] = datetime.now()\n",
    "tracking_df['Data_Date'] = latest_date\n",
    "tracking_df['Model_Source'] = prob_source\n",
    "tracking_df['Ticker_Count'] = len(tickers)\n",
    "\n",
    "# Save\n",
    "tracking_df.to_csv(results_file, index=False)\n",
    "print(f\"💾 Results saved: {results_file}\")\n",
    "\n",
    "# Also save just the buy signals for easy reference\n",
    "buy_signals = tracking_df[tracking_df['Action'].str.contains('BUY')]\n",
    "if len(buy_signals) > 0:\n",
    "    buy_file = RESULTS_DIR / f\"buy_signals_{timestamp}.csv\"\n",
    "    buy_signals[['Ticker', 'Action', 'Investment']].to_csv(buy_file, index=False)\n",
    "    print(f\"💾 Buy signals saved: {buy_file}\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if missing_requirements:\n",
    "print(\"❌ Analysis incomplete due to missing requirements\")\n",
    "print(\"Complete the setup steps above and restart\")\n",
    "elif 'decisions' in locals():\n",
    "print(\"✅ Analysis complete!\")\n",
    "print(f\"📊 Analyzed {len(top_picks)} stocks from {len(tickers)} tickers\")\n",
    "print(f\"💰 Investment recommendations: ${total_investment:,}\")\n",
    "print(f\"📅 Based on data through: {latest_date}\")\n",
    "\n",
    "print(f\"\\n🔄 To refresh analysis:\")\n",
    "print(\"1. Update data: python run_data_extraction.py\")\n",
    "print(\"2. Retrain model (optional): python run_model_training.py\")\n",
    "print(\"3. Re-run this notebook\")\n",
    "else:\n",
    "print(\"⚠️ Analysis incomplete - check errors above\")\n",
    "\n",
    "print(f\"\\n📝 Remember:\")\n",
    "print(\"• Set stop losses at -15% to -20%\")\n",
    "print(\"• Monitor positions daily\")\n",
    "print(\"• Diversify - don't put more than 5-10% in any single position\")\n",
    "print(\"• Past performance doesn't guarantee future results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
