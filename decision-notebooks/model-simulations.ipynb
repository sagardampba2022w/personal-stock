{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f333a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_path = \"/Users/sagardhal/Desktop/Practice/personal-stock/results/predictions_20250908_092155.parquet\"   # example\n",
    "\n",
    "\n",
    "if predictions_path.endswith(\".csv\"):\n",
    "    df = pd.read_csv(predictions_path)\n",
    "elif predictions_path.endswith(\".parquet\"):\n",
    "    df = pd.read_parquet(predictions_path)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported file format for predictions\")\n",
    "\n",
    "\n",
    "print(\"Data loaded:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "new_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ALL current predictions\n",
    "PREDICTIONS = [k for k in new_df.keys() if k.startswith('pred')]\n",
    "PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9995e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[(new_df.split=='test')].Date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred13_rf_thresh_80 = pd.DataFrame(new_df[(new_df.split=='test')&(new_df.pred13_rf_thresh_80==1)].groupby('Date')['pred13_rf_thresh_80'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f36937",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred13_rf_thresh_80.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f45ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find all predictions (starting from 'pred'), generate is_correct (correctness of each prediction)\n",
    "# and precision on TEST dataset (assuming there is df[\"split\"] column with values 'train','validation','test'\n",
    "\n",
    "# returns 2 lists of features: PREDICTIONS and IS_CORRECT\n",
    "\n",
    "def get_predictions_correctness(df:pd.DataFrame, to_predict:str):\n",
    "  PREDICTIONS = [k for k in df.keys() if k.startswith('pred')]\n",
    "  print(f'Prediction columns founded: {PREDICTIONS}')\n",
    "\n",
    "  # add columns is_correct_\n",
    "  for pred in PREDICTIONS:\n",
    "    part1 = pred.split('_')[0] # first prefix before '_'\n",
    "    df[f'is_correct_{part1}'] =  (new_df[pred] == new_df[to_predict]).astype(int)\n",
    "\n",
    "  # IS_CORRECT features set\n",
    "  IS_CORRECT =  [k for k in df.keys() if k.startswith('is_correct_')]\n",
    "  print(f'Created columns is_correct: {IS_CORRECT}')\n",
    "\n",
    "  print('Precision on TEST set for each prediction:')\n",
    "  # define \"Precision\" for ALL predictions on a Test dataset (~4 last years of trading)\n",
    "  for i,column in enumerate(IS_CORRECT):\n",
    "    prediction_column = PREDICTIONS[i]\n",
    "    is_correct_column = column\n",
    "    filter = (new_df.split=='test') & (new_df[prediction_column]==1)\n",
    "    print(f'Prediction column:{prediction_column} , is_correct_column: {is_correct_column}')\n",
    "    print(new_df[filter][is_correct_column].value_counts())\n",
    "    print(new_df[filter][is_correct_column].value_counts()/len(new_df[filter]))\n",
    "    print('---------')\n",
    "\n",
    "  return PREDICTIONS, IS_CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = 'is_positive_growth_30d_future'\n",
    "\n",
    "\n",
    "PREDICTIONS, IS_CORRECT = get_predictions_correctness(df = new_df, to_predict= to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.groupby('split').Date.agg(['min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ebd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fin. result for ALL predictions (manual and produced by models)\n",
    "\n",
    "sim1_results = [] # results in Array\n",
    "\n",
    "# Iterate over all predictions\n",
    "for pred in PREDICTIONS:\n",
    "  print(f'Calculating sumulation for prediction {pred}:')\n",
    "  print(f\"    Count times of investment {len(new_df[(new_df.split=='test')&(new_df[pred]==1)])} out of {len(new_df[(new_df.split=='test')])} TEST records\")\n",
    "\n",
    "  # Prefix: e.g. pred1 or pred10\n",
    "  pred_prefix= pred.split('_')[0]\n",
    "\n",
    "  # Fin. result columns: define new records for EACH positive prediction\n",
    "  new_df['sim1_gross_rev_'+pred_prefix] = new_df[pred] * 100 * (new_df['growth_future_30d']-1)\n",
    "  new_df['sim1_fees_'+pred_prefix] = -new_df[pred] * 100 * 0.002\n",
    "  new_df['sim1_net_rev_'+pred_prefix] = new_df['sim1_gross_rev_'+pred_prefix] + new_df['sim1_fees_'+pred_prefix]\n",
    "\n",
    "  # calculate agg. results for each PREDICTION columns (pred) on TEST\n",
    "  filter_test_and_positive_pred = (new_df.split=='test')&(new_df[pred]==1) # filter records on TEST set, when current prediction is 1 (we invest $100 for 1 week ahead - 5 periods)\n",
    "  sim1_count_investments = len(new_df[filter_test_and_positive_pred])\n",
    "  sim1_gross_rev = new_df[filter_test_and_positive_pred]['sim1_gross_rev_'+pred_prefix].sum()\n",
    "  sim1_fees = new_df[filter_test_and_positive_pred]['sim1_fees_'+pred_prefix].sum()\n",
    "  sim1_net_rev = new_df[filter_test_and_positive_pred]['sim1_net_rev_'+pred_prefix].sum()\n",
    "\n",
    "  if sim1_gross_rev>0:\n",
    "    sim1_fees_percentage = -sim1_fees/sim1_gross_rev\n",
    "  else:\n",
    "    sim1_fees_percentage = None\n",
    "\n",
    "  if sim1_count_investments>0:\n",
    "    sim1_average_net_revenue = sim1_net_rev/sim1_count_investments\n",
    "  else:\n",
    "    sim1_average_net_revenue = None\n",
    "\n",
    "  # APPROXIMATE CAPITAL REQUIRED and CAGR Calculation\n",
    "  df_investments_count_daily = pd.DataFrame(new_df[filter_test_and_positive_pred].groupby('Date')[pred].count())\n",
    "  sim1_avg_investments_per_day = df_investments_count_daily[pred].mean()\n",
    "  sim1_q75_investments_per_day = df_investments_count_daily[pred].quantile(0.75)  # 75% case - how many $100 investments per day do we have?\n",
    "  # df_investments_count_daily[pred].mean()\n",
    "  sim1_capital = 100 * 30 * sim1_q75_investments_per_day # 30 days in a row with positive predictions\n",
    "\n",
    "  # CAGR: average growth per year. E.g. if you have 1.5 return (50% growth in 4 years) --> (1.5)**(1/4) = 1.106 or 10.6% average\n",
    "  sim1_CAGR = ((sim1_capital+sim1_net_rev)/sim1_capital)**(1/4)\n",
    "\n",
    "  # append to DF\n",
    "  sim1_results.append((pred,sim1_count_investments,sim1_gross_rev,sim1_fees,sim1_net_rev,sim1_fees_percentage,sim1_average_net_revenue,sim1_avg_investments_per_day,sim1_capital,sim1_CAGR))\n",
    "\n",
    "\n",
    "  # output for all predictions with some positive predictions\n",
    "  if  sim1_count_investments>1:\n",
    "    print(f\"    Financial Result: \\n {new_df[filter_test_and_positive_pred][['sim1_gross_rev_'+pred_prefix,'sim1_fees_'+pred_prefix,'sim1_net_rev_'+pred_prefix]].sum()}\")\n",
    "    print(f\"        Count Investments in 4 years (on TEST): {sim1_count_investments}\")\n",
    "    print(f\"        Gross Revenue: ${int(sim1_gross_rev)}\")\n",
    "    print(f\"        Fees (0.2% for buy+sell): ${int(-sim1_fees)}\")\n",
    "    print(f\"        Net Revenue: ${int(sim1_net_rev)}\")\n",
    "    print(f\"        Fees are {int(-100.0*sim1_fees/sim1_gross_rev)} % from Gross Revenue\")\n",
    "    print(f\"        Capital Required : ${int(sim1_capital)} (Vbegin)\")\n",
    "    print(f\"        Final value (Vbegin + Net_revenue) : ${int(sim1_capital + sim1_net_rev)} (Vfinal)\")\n",
    "\n",
    "    print(f\"        Average CAGR on TEST (4 years) : {np.round(sim1_CAGR,3)}, or {np.round(100.0*(sim1_CAGR-1),1)}% \")\n",
    "\n",
    "    print(f\"        Average daily stats: \")\n",
    "    print(f\"            Average net revenue per investment: ${np.round(sim1_net_rev/sim1_count_investments,2)} \")\n",
    "    print(f\"            Average investments per day: {int(np.round(sim1_avg_investments_per_day))} \")\n",
    "    print(f\"            Q75 investments per day: {int(np.round(sim1_q75_investments_per_day))} \")\n",
    "    print('=============================================+')\n",
    "\n",
    "\n",
    "# results in a DataFrame from an Array\n",
    "columns_simulation = ['prediction', 'sim1_count_investments', 'sim1_gross_rev', 'sim1_fees', 'sim1_net_rev', 'sim1_fees_percentage','sim1_average_net_revenue','sim1_avg_investments_per_day','sim1_capital','sim1_CAGR']\n",
    "\n",
    "df_sim1_results = pd.DataFrame(sim1_results,columns=columns_simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeacfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim1_results['sim1_growth_capital_4y'] = (df_sim1_results.sim1_net_rev+df_sim1_results.sim1_capital) / df_sim1_results.sim1_capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9438fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    df_sim1_results.dropna(),\n",
    "    x='sim1_avg_investments_per_day',\n",
    "    y='sim1_CAGR',\n",
    "    size='sim1_growth_capital_4y',  # Use the 'size' parameter for sim1_CAGR\n",
    "    text='prediction',\n",
    "    title='Compound Annual Growth vs. Time spent (Average investments per day)',\n",
    "    labels={'sim1_capital': 'Initial Capital Requirement', 'growth_capital_4y': '4-Year Capital Growth'}\n",
    ")\n",
    "\n",
    "# Update the layout to improve readability of the annotations\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed info from the bubble chart above on the winners\n",
    "# top 1 result\n",
    "df_sim1_results[df_sim1_results.prediction.isin(['pred21_ens_auto1p_or_top3'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207f011",
   "metadata": {},
   "source": [
    "## . Advanced Simulation\n",
    "* to_predict = 'is_positive_growth_30d_future'\n",
    "* invest ~3% of capital each day (sell positions from 30 days ago), REINVEST PREVIOUS GAINS\n",
    "* use predict_proba predictions\n",
    "* select top1..3..x predictions > threshold (0.53)\n",
    "* invest proportionally the prediction\n",
    "* stop loss y%\n",
    "* take profit z%\n",
    "* Not included: portfolio optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/17775935/sql-like-window-functions-in-pandas-row-numbering-in-python-pandas-dataframe\n",
    "# rank of prediction\n",
    "\n",
    "new_df[\"rf_pred_rank\"] = new_df.groupby(\"Date\")[\"rf_prob_30d\"].rank(method=\"first\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.sort_values(by=['Ticker', 'Date'])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55903c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['growth_future_30d'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['growth_future_30d'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad55c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop loss when -11%\n",
    "new_df['growth_future_30d'].quantile(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4992fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a sample on the test dataset\n",
    "new_df[new_df.split=='test'][['Date','High','Low','Close','Ticker','ticker_type','growth_future_30d']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the rolling max High and min Low for the next 30 trading days\n",
    "\n",
    "# Sort the DataFrame: Sorting by Ticker and Date ensures that we are looking at each ticker's data in chronological order.\n",
    "# Rolling window calculation: We use the rolling method with a window of 5 to calculate the maximum high and minimum low over the next 5 days.\n",
    "# The shift method is used to align these values correctly with the current row.\n",
    "\n",
    "def rolling_max_min(df, window=30):\n",
    "    # high/low in 30 days\n",
    "    df['Max_High_Next_30'] = df['High'].rolling(window=window, min_periods=1).max().shift(-window+1)\n",
    "    df['Min_Low_Next_30'] = df['Low'].rolling(window=window, min_periods=1).min().shift(-window+1)\n",
    "\n",
    "    # low in 1 day (for lower entry)\n",
    "    df['Min_Low_Next_1'] = df['Low'].rolling(window=1, min_periods=1).min().shift(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each group of Ticker\n",
    "# Important!: need to drop index from groupby operation (reset_index) - so that you can merge that afterwards\n",
    "result = new_df[new_df.split=='test'][['Date','High','Low','Close','Ticker']].groupby('Ticker').apply(rolling_max_min).reset_index(drop=True)\n",
    "result.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.Ticker=='AAPL'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratios + safe divide\n",
    "result['Ratio_MaxHighNext30_to_Close'] = np.where(result['Close'] == 0, np.nan, result['Max_High_Next_30']/result['Close'])\n",
    "result['Ratio_MinLowNext30_to_Close'] =  np.where(result['Close'] == 0, np.nan,  result['Min_Low_Next_30']/result['Close'])\n",
    "\n",
    "result['Ratio_MinLowNext1_to_Close'] =  np.where(result['Close'] == 0, np.nan,  result['Min_Low_Next_1']/result['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33618e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.Ticker=='AAPL'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Ratio_MinLowNext1_to_Close.hist(bins=20)\n",
    "plt.title(f'Distribution of MinLowNext1_to_Close (ratio)')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Ratio_MinLowNext1_to_Close.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40af0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Ratio_MaxHighNext30_to_Close.hist()\n",
    "plt.title(f'Distribution of Max_High_Next_30_to_Close (ratio)')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5555e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.Ratio_MaxHighNext30_to_Close>=1.3].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a66f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~1.8% cases TAKE PROFIT (30%)\n",
    "len(result[result.Ratio_MaxHighNext30_to_Close>=1.3])/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe599bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High intra-day volatility, especially on earlier days after the IPO\n",
    "result.Ratio_MinLowNext30_to_Close.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eefe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Ratio_MinLowNext30_to_Close.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2177c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.Ratio_MinLowNext30_to_Close<=0.8].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the results back with the original dataframe\n",
    "new_df = new_df.merge(result[['Date', 'Ticker', 'Max_High_Next_30', 'Min_Low_Next_30','Ratio_MaxHighNext30_to_Close','Ratio_MinLowNext30_to_Close','Ratio_MinLowNext1_to_Close']], on=['Date', 'Ticker'])\n",
    "\n",
    "\n",
    "new_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f10a9a",
   "metadata": {},
   "source": [
    "### Generate fin.result for one date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf738a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SimulationParams:\n",
    "    initial_capital: float\n",
    "    threshold: float\n",
    "    fees: float\n",
    "    top_k: int\n",
    "    portfolio_optimization: bool\n",
    "    stop_loss: float\n",
    "    take_profit: float\n",
    "    lower_entry: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of initializing with some values\n",
    "sim_params = SimulationParams(\n",
    "    initial_capital=10000,        # initial capital = $10k\n",
    "    threshold=0.53,               # select all binary predictions with probability>=0.55\n",
    "    fees=0.002,                   # trading fees = 0.2% (buy+sell)\n",
    "    top_k=10,                     # select top_k predictions\n",
    "    portfolio_optimization=False, # no portfolio optimizaiton\n",
    "    stop_loss=0.8,                # automatic sell (with loss) if price (any of next 30 days) is lower than -20% from Close\n",
    "    take_profit=1.3,              # automatic sell (with profit) if price (any of next 30 days) is higher than +30% from Close\n",
    "    lower_entry= 0.995                # buy next day with the price = [Close] * 0.995 (try to buy cheaper)\n",
    ")\n",
    "\n",
    "print(sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted array of dates for Simulation\n",
    "DATES = new_df[new_df.split=='test'].sort_values(by='Date').Date.unique()\n",
    "print(f' Min date {DATES.min()}, max date {DATES.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = DATES[0]\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab55df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get TOP_K predictions from pred10_rf_best_rule_50, that are higher than THE THRESHOLD\n",
    "if sim_params.top_k is None:\n",
    "  one_day_predictions_df = new_df[(new_df.Date==date)&(new_df.rf_prob_30d > sim_params.threshold)]\n",
    "else:\n",
    "  one_day_predictions_df = new_df[(new_df.Date==date)&(new_df.rf_prob_30d > sim_params.threshold)&(new_df.rf_pred_rank<=sim_params.top_k)]\n",
    "\n",
    "\n",
    "one_day_predictions_df[['Date','Ticker',to_predict,'growth_future_30d','rf_prob_30d','rf_pred_rank',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get non-normalized weights: probability-threshold + 0.01\n",
    "one_day_predictions_df['weight'] = one_day_predictions_df.rf_prob_30d - sim_params.threshold +0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df[['Date','Ticker','Close', to_predict,'growth_future_30d','rf_prob_30d','rf_pred_rank','weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b102f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get normalized weights\n",
    "one_day_predictions_df['weight_norm'] = one_day_predictions_df['weight']/one_day_predictions_df['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df[['Date','Ticker','Close', to_predict,'growth_future_30d','rf_prob_30d','rf_pred_rank','weight','weight_norm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Capital: either 1/30 of initial (assuming you trade every day), or everything that you can sell from 30 days ago\n",
    "one_day_predictions_df['investment'] = one_day_predictions_df['weight_norm'] * sim_params.initial_capital /30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c855382",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df[['Date','Ticker','Close', to_predict,'growth_future_30d','rf_prob_30d','rf_pred_rank','weight','weight_norm','investment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd762320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Lower Entry: the trade is executed only is Low price for next day is lower than bet\n",
    "one_day_predictions_df['lower_entry'] = (one_day_predictions_df['Ratio_MinLowNext1_to_Close']<=sim_params.lower_entry).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b931fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df['stop_loss'] = (one_day_predictions_df['Ratio_MinLowNext30_to_Close'] <= sim_params.stop_loss).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8dbe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df['take_profit'] = (one_day_predictions_df['Ratio_MaxHighNext30_to_Close'] >= sim_params.take_profit).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315aee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# future_gross_return, depending on lower_entry, take_profit, stop_loss\n",
    "\n",
    "import random\n",
    "\n",
    "def get_future_gross_return(row, sim_params:SimulationParams):\n",
    "  if row['lower_entry']==0: # no trade, investment is untouched, no fees\n",
    "    return row['investment']\n",
    "\n",
    "  # buy trade is filled for ALL next cases:\n",
    "  if row['take_profit']==1 and row['stop_loss']==1:\n",
    "    if random.random()>0.5: #assume take_profit event was first\n",
    "      return  row['investment']*(sim_params.take_profit+(1-sim_params.lower_entry))\n",
    "    else: #assume stop_loss event was first\n",
    "      return row['investment']*(sim_params.stop_loss+(1-sim_params.lower_entry))\n",
    "\n",
    "  if row['take_profit']==1: # take some good profit, pay fees\n",
    "    return  row['investment']*(sim_params.take_profit+(1-sim_params.lower_entry))\n",
    "\n",
    "  if row['stop_loss']==1: # fix the loss, pay fees\n",
    "      return row['investment']*(sim_params.stop_loss+(1-sim_params.lower_entry))\n",
    "\n",
    "  # no stop_loss and no take_profit\n",
    "  if pd.isna(row['growth_future_30d']):\n",
    "    return row['investment'] # no information on growth in 30 days --> return the same investment in 5 days\n",
    "  else:\n",
    "    return row['investment']*(row['growth_future_30d']+(1-sim_params.lower_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fees, depending on lower_entry, take_profit, stop_loss\n",
    "\n",
    "def get_fees(row, sim_params:SimulationParams):\n",
    "  if row['lower_entry']==0: # no trade ==> no fees\n",
    "    return 0\n",
    "\n",
    "  # pay fees in all other cases\n",
    "  return -row['investment']*sim_params.fees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c043d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. calculate future returns (when the order is executed + stop_loss True/False + take_profit True/False)\n",
    "one_day_predictions_df['future_gross_return'] = one_day_predictions_df.apply(lambda row: get_future_gross_return(row,sim_params=sim_params), axis=1)\n",
    "one_day_predictions_df['fees'] =  one_day_predictions_df.apply(lambda row: get_fees(row,sim_params=sim_params), axis=1)\n",
    "one_day_predictions_df['future_net_return'] = one_day_predictions_df['future_gross_return'] + one_day_predictions_df['fees']\n",
    "\n",
    "# OLD code when no stop_loss, take_profit, and if order executed\n",
    "# one_day_predictions_df['investment'] * one_day_predictions_df['growth_future_5d']\n",
    "# one_day_predictions_df['fees'] =   - one_day_predictions_df['investment'] * sim_params.fees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352954b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14bc500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944229b0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Wrap up one day simulation into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a54607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Parameters\n",
    "sim_params = SimulationParams(\n",
    "    initial_capital=10000,                 # initial capital = $10k\n",
    "    threshold=0.53,                        # select all binary predictions with probability>=0.55\n",
    "    fees=0.002,                            # trading fees = 0.2% (buy+sell)\n",
    "    top_k=10,                              # select top_k predictions\n",
    "    portfolio_optimization=False,          # no portfolio optimizaiton\n",
    "    stop_loss=0.8,                         # automatic sell (with loss) if price (any of next 30 trading days) is lower than -20% from Close\n",
    "    take_profit=1.3,                       # automatic sell (with profit) if price (any of next 30 trading days) is higher than +30% from Close\n",
    "    lower_entry= 0.995                     # buy next day with the price = [Close] * 0.995 (try to buy cheaper)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_date_simulation(date:str, invest_sum:float, df:pd.DataFrame, sim_params:SimulationParams, predictor:str='proba_pred10'):\n",
    "\n",
    "  #rank_column = predictor.split('_')[1]+'_rank' # e.g. 'proba_pred10' --> 'pred10_rank'\n",
    "  rank_column = predictor.split('_')[0] +'_pred_rank'\n",
    "\n",
    "  # 1. get TOP_K (or ALL) predictions from the predictor (pred14_rf_best_rule_53 by default), that are higher than THE THRESHOLD\n",
    "  if sim_params.top_k is None:\n",
    "    one_day_predictions_df = df[(df.Date==date)&(df[predictor] > sim_params.threshold)]\n",
    "  else:\n",
    "    one_day_predictions_df = df[(df.Date==date)&(df[predictor] > sim_params.threshold)&(df[rank_column]<=sim_params.top_k)]\n",
    "\n",
    "  FIELDS = ['Close', 'Ticker', 'Date', predictor, rank_column, 'growth_future_30d', 'Ratio_MaxHighNext30_to_Close','Ratio_MinLowNext30_to_Close','Ratio_MinLowNext1_to_Close']\n",
    "  result_df = one_day_predictions_df[FIELDS].copy()\n",
    "\n",
    "  # 2. Get non-normalized weights: probability-threshold + 0.01\n",
    "  result_df['weight'] = result_df[predictor] - sim_params.threshold +0.01\n",
    "\n",
    "  # 3. Get normalized weights\n",
    "  result_df['weight_norm'] = result_df['weight']/result_df['weight'].sum()\n",
    "\n",
    "  # 4. Make bets to allocate 'invest_sum' across all suitable predictions\n",
    "  result_df['investment'] = result_df['weight_norm'] * invest_sum\n",
    "\n",
    "  # 5. Lower Entry: the trade is executed only is Low price for next day is lower than the bet (Adj_Close_today * sim_params.lower_entry)\n",
    "    # [ONLY TRADES with lower_entry==1 are filled by the exchange]\n",
    "  result_df['lower_entry'] = (result_df['Ratio_MinLowNext1_to_Close'] <= sim_params.lower_entry).astype(int)\n",
    "\n",
    "  # 6. Stop Loss: happens if the current price (or Low price) goes below stop loss threshold during one of the next 5 periods (1 week)\n",
    "  result_df['stop_loss'] = (result_df['Ratio_MinLowNext30_to_Close'] <= sim_params.stop_loss).astype(int)\n",
    "\n",
    "  # 7. Take Profit: take the money if the current Price (or Max_price) goes higher than sim_params.take_profit\n",
    "  result_df['take_profit'] = (result_df['Ratio_MaxHighNext30_to_Close'] >= sim_params.take_profit).astype(int)\n",
    "\n",
    "  # 8. Calculate future returns (when the order is executed + stop_loss True/False + take_profit True/False)\n",
    "  result_df['future_gross_return'] = result_df.apply(lambda row: get_future_gross_return(row,sim_params=sim_params), axis=1)\n",
    "  result_df['fees'] =  result_df.apply(lambda row: get_fees(row,sim_params=sim_params), axis=1)\n",
    "  result_df['future_net_return'] = result_df['future_gross_return'] + result_df['fees']\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = 'rf_prob_30d'\n",
    "rank_column = predictor.split('_')[0] +'_pred_rank'\n",
    "print(rank_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed73c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = one_date_simulation(date='2021-10-28', invest_sum=sim_params.initial_capital/30, df=new_df, sim_params=sim_params, predictor=predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial investment\n",
    "r.investment.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdea080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result in 30 days (returns+fees)\n",
    "r.future_net_return.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb9881",
   "metadata": {},
   "source": [
    "# Generate fin result for ALL days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = new_df[new_df.split=='test'].sort_values(by='Date').Date.unique()\n",
    "all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate for all dates but last 5\n",
    "all_dates[0:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58073bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these last days we only \"sell\" the positions\n",
    "all_dates[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital= 5 * [sim_params.initial_capital/5]\n",
    "capital[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(df:pd.DataFrame, sim_params:SimulationParams):\n",
    "\n",
    "  simulation_df = None\n",
    "\n",
    "  # all dates for simulation\n",
    "  all_dates = df[df.split=='test'].sort_values(by='Date').Date.unique()\n",
    "\n",
    "  # arrays of dates and capital available (capital for the first 5 days)\n",
    "  dates = []\n",
    "  capital= 30 * [sim_params.initial_capital/30]  # first 30 periods trade with 1/30 of the initial_capital. e.g. [333,...,333] = 10k in total\n",
    "\n",
    "  for current_date in all_dates[0:-30]:  #growth_future_30d is not defined for the last 30 days : ALL, but last 30 dates\n",
    "\n",
    "    current_invest_sum = capital[-30]    # take the value or everything that you can sell from 30 days ago\n",
    "\n",
    "    one_day_simulation_results = one_date_simulation(date = current_date,  # one day simulation result\n",
    "                                    invest_sum = current_invest_sum,\n",
    "                                    df = df,\n",
    "                                    sim_params=sim_params,\n",
    "                                    predictor=predictor)\n",
    "\n",
    "    # add capital available in 30 days\n",
    "    if len(one_day_simulation_results)==0:  #no predictions -> no trades\n",
    "      capital.append(current_invest_sum)\n",
    "    else:\n",
    "      capital.append(one_day_simulation_results.future_net_return.sum())\n",
    "    dates.append(current_date)\n",
    "\n",
    "    if simulation_df is None:\n",
    "      simulation_df = one_day_simulation_results\n",
    "    else:\n",
    "      simulation_df = pd.concat([simulation_df, one_day_simulation_results], ignore_index=True)\n",
    "\n",
    "  # add last 5 days to make the count of data points equal for dates/capital arrays\n",
    "  dates.extend(all_dates[-30:])\n",
    "  capital_df = pd.DataFrame({'capital':capital}, index=pd.to_datetime(dates))\n",
    "\n",
    "  # results:\n",
    "  print(f'============================================================================================')\n",
    "  print(f'SIMULATION STARTED')\n",
    "  print(f'Simulations params: {sim_params}')\n",
    "  print(f' Count bids {len(simulation_df)} in total, avg.bids per day {len(simulation_df)/simulation_df.Date.nunique()},  filled bids {len(simulation_df[simulation_df.lower_entry==1])}, fill bids percent = {len(simulation_df[simulation_df.lower_entry==1])/len(simulation_df)}')\n",
    "  stop_loss_filter = (simulation_df.lower_entry==1)&(simulation_df.stop_loss==1)\n",
    "  print(f'  Stop loss events: count = {len(simulation_df[stop_loss_filter])}, net loss = {simulation_df[stop_loss_filter].future_net_return.sum()-simulation_df[stop_loss_filter].investment.sum()} ')\n",
    "  take_profit_filter = (simulation_df.lower_entry==1)&(simulation_df.take_profit==1)\n",
    "  print(f'  Take profit events: count = {len(simulation_df[take_profit_filter])}, net profit = {simulation_df[take_profit_filter].future_net_return.sum()-simulation_df[take_profit_filter].investment.sum()} ')\n",
    "  print(f'  Start capital = {sim_params.initial_capital}, Resulting capital: {capital_df[-30:].capital.sum()} ')\n",
    "  print(f'  CAGR in 4 years: {np.round((capital_df[-30:].capital.sum()/sim_params.initial_capital)**(1/4),3)} or {np.round(((capital_df[-30:].capital.sum()/sim_params.initial_capital)**(1/4)-1)*100.0,2)} % of avg. growth per year')\n",
    "  print(f'============================================================================================')\n",
    "  return simulation_df,capital_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One initial simulation\n",
    "sim_params = SimulationParams(\n",
    "    initial_capital = 10000,        # initial capital = $10k\n",
    "    threshold = 0.55,               # select all binary predictions with probability>=0.55\n",
    "    fees = 0.002,                   # trading fees = 0.2% (buy+sell)\n",
    "    top_k = 5,                      # select top_k=5 predictions\n",
    "    portfolio_optimization = False, # no portfolio optimization\n",
    "    stop_loss = 0.8,                # automatic sell (with loss) if price (any of next 5 days) is lower than -20% from Adj.Close\n",
    "    take_profit = 1.3,              # automatic sell (with profit) if price (any of next 5 days) is higher than +30% from Adj.Close\n",
    "    lower_entry = 0.99               # buy next day with the price = [Close] * 0.99 (try to buy cheaper)\n",
    ")\n",
    "\n",
    "res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d51c9",
   "metadata": {},
   "source": [
    "#  Find optimal parameters of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PREVIOUS CAGR is 7.8%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3, lower_entry=0.99)\n",
    "# iterate over lower_entry\n",
    "for lower_entry in [0.97,0.98,0.99,1,1.01,1.02,1.03]:\n",
    "  # One simulation\n",
    "  sim_params = SimulationParams(\n",
    "      initial_capital = 10000,        # initial capital = $10k\n",
    "      threshold = 0.55,               # select all binary predictions with probability>=0.55\n",
    "      fees = 0.002,                   # trading fees = 0.2% (buy+sell)\n",
    "      top_k = 5,                     # select top_k predictions\n",
    "      portfolio_optimization = False, # no portfolio optimization\n",
    "      stop_loss = 0.8,                # automatic sell (with loss) if price (any of next 5 days) is lower than -5% from Adj.Close\n",
    "      take_profit = 1.3,              # automatic sell (with profit) if price (any of next 5 days) is higher than +20% from Adj.Close\n",
    "      lower_entry = lower_entry                # buy next day with the price = [Adj.Close] * 0.995 (try to buy cheaper)\n",
    "  )\n",
    "\n",
    "  res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PREVIOUS is CAGR 11.16%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3, lower_entry=0.99 ==> lower_entry=1.0)\n",
    "\n",
    "# iterate over take profit   ==> new best take_profit=1.5\n",
    "for take_profit in [1.2,1.25,1.3,1.35,1.4,1.45,1.5]:\n",
    "  # One simulation\n",
    "  sim_params = SimulationParams(\n",
    "      initial_capital = 10000,        # initial capital = $10k\n",
    "      threshold = 0.55,               # select all binary predictions with probability>=0.55\n",
    "      fees = 0.002,                   # trading fees = 0.2% (buy+sell)\n",
    "      top_k = 5,                     # select top_k predictions\n",
    "      portfolio_optimization = False, # no portfolio optimization\n",
    "      stop_loss = 0.8,                # automatic sell (with loss) if price (any of next 30 days) is lower than -5% from Adj.Close\n",
    "      take_profit = take_profit,        # automatic sell (with profit) if price (any of next 30 days) is higher than +20% from Adj.Close\n",
    "      lower_entry = 1                # buy next day with the price = [Close] * lower_entry (try to buy cheaper)\n",
    "  )\n",
    "\n",
    "  res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b15dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PREVIOUS is CAGR 11.16%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 11.91%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3 => take_profit=1.5, lower_entry=1.0)\n",
    "\n",
    "# iterate over stop_loss\n",
    "for stop_loss in [0.55,0.6,0.65,0.7,0.75,0.8]:\n",
    "  # One simulation\n",
    "  sim_params = SimulationParams(\n",
    "      initial_capital = 10000,        # initial capital = $10k\n",
    "      threshold = 0.55,               # select all binary predictions with probability>=0.55\n",
    "      fees = 0.002,                   # trading fees = 0.2% (buy+sell)\n",
    "      top_k = 5,                     # select top_k predictions\n",
    "      portfolio_optimization = False, # no portfolio optimization\n",
    "      stop_loss = stop_loss,                # automatic sell (with loss) if price (any of next 30 days) is lower than -\"stop_loss\"% from Close\n",
    "      take_profit = 1.5,              # automatic sell (with profit) if price (any of next 30 days) is higher than +50% from Close\n",
    "      lower_entry = 1                # buy next day with the price = [Close] * 1 (try to buy cheaper)\n",
    "  )\n",
    "\n",
    "  res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PREVIOUS is CAGR 11.16%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 11.91%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3 => take_profit=1.5, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 13.19%  (top_k=5, portfolio_optimization=False, stop_loss=0.8 => stop_loss=0.6, take_profit=1.5, lower_entry=1.0)\n",
    "\n",
    "\n",
    "# iterate over lower_entry ++ take profit of 2.5%\n",
    "  # best lower_entry ==0.98\n",
    "for threshold in [0.51,0.52,0.53,0.54,0.55,0.56,0.57]:\n",
    "  # One simulation\n",
    "  sim_params = SimulationParams(\n",
    "      initial_capital = 10000,        # initial capital = $10k\n",
    "      threshold = threshold,               # select all binary predictions with probability>=threshold (iter param)\n",
    "      fees = 0.002,                   # trading fees = 0.2% (buy+sell)\n",
    "      top_k = 5,                     # select top_k predictions\n",
    "      portfolio_optimization = False, # no portfolio optimization\n",
    "      stop_loss = 0.6,                # automatic sell (with loss) if price (any of next 30 days) is lower than -40% from Close\n",
    "      take_profit = 1.5,              # automatic sell (with profit) if price (any of next 30 days) is higher than +50% from Adj.Close\n",
    "      lower_entry = 1                # buy next day with the price = [Close] * lower_entry (try to buy cheaper)\n",
    "  )\n",
    "\n",
    "  res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PREVIOUS is CAGR 11.16%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 11.91%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3 => take_profit=1.5, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 13.19%  (top_k=5, portfolio_optimization=False, stop_loss=0.8 => stop_loss=0.6, take_profit=1.5, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 13.5%  (threshold=0.55 -> threshold=0.56 -- for a decision rule, top_k=5, portfolio_optimization=False, stop_loss=0.6, take_profit=1.5, lower_entry=1.0)\n",
    "\n",
    "\n",
    "# iterate over top_k\n",
    "for top_k in [1,2,3,4,5,6,8,10,20,33]:\n",
    "  # One simulation\n",
    "  sim_params = SimulationParams(\n",
    "      initial_capital = 10000,        # initial capital = $10k\n",
    "      threshold = 0.56,               # select all binary predictions with probability>=0.56\n",
    "      fees = 0.002,                     # trading fees = 0.2% (buy+sell)\n",
    "      top_k = top_k,                    # select top_k predictions\n",
    "      portfolio_optimization = False,   # no portfolio optimization\n",
    "      stop_loss = 0.6,                 # automatic sell (with loss) if price (any of next 30 days) is lower than\n",
    "      take_profit = 1.5,              # automatic sell (with profit) if price (any of next 30 days) is higher than +50% from Close\n",
    "      lower_entry = 1.0                # buy next day with the price = [Close] * (try to buy cheaper)\n",
    "  )\n",
    "\n",
    "  res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PREVIOUS is CAGR 11.16%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 11.91%  (top_k=5, portfolio_optimization=False, stop_loss=0.8, take_profit=1.3 => take_profit=1.5, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 13.19%  (top_k=5, portfolio_optimization=False, stop_loss=0.8 => stop_loss=0.6, take_profit=1.5, lower_entry=1.0)\n",
    "# BEST PREVIOUS is CAGR 13.5%  (threshold=0.55 -> threshold=0.56 -- for a decision rule, top_k=5, portfolio_optimization=False, stop_loss=0.6, take_profit=1.5, lower_entry=1.0)\n",
    "\n",
    "# BEST PREVIOUS is CAGR 13.9%  (threshold=0.56, top_k=5 ==> top_k=4, portfolio_optimization=False, stop_loss=0.6, take_profit=1.5, lower_entry=1.0)\n",
    "\n",
    "\n",
    "# iterate over lower_entry\n",
    "for stop_loss in [0.6, 0.65, 0.7,0.75, 0.8, 0.85]:\n",
    "  # One simulation\n",
    "  sim_params = SimulationParams(\n",
    "      initial_capital = 10000,        # initial capital = $10k\n",
    "      threshold = 0.56,               # select all binary predictions with probability>=0.56\n",
    "      fees = 0.002,                   # trading fees = 0.2% (buy+sell)\n",
    "      top_k = 4,                      # select top_k predictions\n",
    "      portfolio_optimization = False, # no portfolio optimization\n",
    "      stop_loss = stop_loss,                # automatic sell (with loss) if price (any of next 5 days) is lower than -5% from Adj.Close\n",
    "      take_profit = 1.5,              # automatic sell (with profit) if price (any of next 5 days) is higher than +20% from Adj.Close\n",
    "      lower_entry = 1                # buy next day with the price = [Adj.Close] * 0.995 (try to buy cheaper)\n",
    "  )\n",
    "\n",
    "  res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97938de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4d93d92",
   "metadata": {},
   "source": [
    "###  Explore the best simulation params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822cc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = SimulationParams(initial_capital=10000, threshold=0.51, fees=0.002, top_k=8, portfolio_optimization=False, stop_loss=0.6, take_profit=1.5, lower_entry=1)\n",
    "\n",
    "res, capital = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res.Date=='2024-04-30']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d98c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled deals\n",
    "res.lower_entry.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435510f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop loss (regardless of a filled bid)\n",
    "\n",
    "res.stop_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5412f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take profit (regardless of a filled bid)\n",
    "res.take_profit.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[(res.stop_loss==1)&(res.lower_entry==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd592ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could take profit, but the bid was not executed\n",
    "res[(res.take_profit==1)&(res.lower_entry==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ae722",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[(res.take_profit==1) &(res.lower_entry==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f90f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res.Date=='2024-04-30'].future_net_return.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 20 days\n",
    "capital[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital.rolling(5).sum().plot.line()\n",
    "print(capital[-5:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f33f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047b1197",
   "metadata": {},
   "source": [
    "### Debug optimal strategy with k=6 max trades per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08503b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = SimulationParams(initial_capital=10000, threshold=0.55, fees=0.002, top_k=4, portfolio_optimization=False, stop_loss=0.6, take_profit=1.5, lower_entry=1)\n",
    "\n",
    "res, capital_4trades = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d90329",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_stop_loss = (res.lower_entry==1) & (res.stop_loss==1)\n",
    "print(f'Average real close price if not stop_loss: {res[filter_stop_loss].growth_future_30d.mean()}')\n",
    "res[filter_stop_loss].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e222a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_take_profit = (res.lower_entry==1) & (res.take_profit==1) & (res.stop_loss==0)\n",
    "print(f'Average real close price if not take_profit: {res[filter_take_profit].growth_future_30d.mean()}')\n",
    "res[filter_take_profit].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d01c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_no_stop_loss_no_take_profit = (res.lower_entry==1) & (res.take_profit==0) & (res.stop_loss==0)\n",
    "print(f'Average real close price if no take_profit or stop loss: {res[filter_no_stop_loss_no_take_profit].growth_future_30d.mean()}')\n",
    "res['realised_profit'] = res.future_net_return/res.investment\n",
    "res[filter_no_stop_loss_no_take_profit][['growth_future_30d','realised_profit']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70383f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_4trades.rolling(30).sum().plot.line()\n",
    "print(capital_4trades[-30:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbed5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8dffd80",
   "metadata": {},
   "source": [
    "### Debug optimal strategy with k=1 max trades per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f404ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = SimulationParams(initial_capital=10000, threshold=0.55, fees=0.002, top_k=1, portfolio_optimization=False, stop_loss=0.6, take_profit=1.5, lower_entry=1.0)\n",
    "\n",
    "res, capital_1_trade = simulate(new_df, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO STOP LOSS events\n",
    "filter_stop_loss = (res.lower_entry==1) & (res.stop_loss==1)\n",
    "print(f'Average real close price if not stop_loss: {res[filter_stop_loss].growth_future_30d.mean()}')\n",
    "res[filter_stop_loss].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4754e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_take_profit = (res.lower_entry==1) & (res.take_profit==1) & (res.stop_loss==0)\n",
    "print(f'Average real close price if not take_profit: {res[filter_take_profit].growth_future_30d.mean()}')\n",
    "res[filter_take_profit].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daeb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_no_stop_loss_no_take_profit = (res.lower_entry==1) & (res.take_profit==0) & (res.stop_loss==0)\n",
    "print(f'Average real close price if no take_profit or stop loss: {res[filter_no_stop_loss_no_take_profit].growth_future_30d.mean()}')\n",
    "res['realised_profit'] = res.future_net_return/res.investment\n",
    "res[filter_no_stop_loss_no_take_profit][['growth_future_30d','realised_profit']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_1_trade.rolling(30).sum().plot.line()\n",
    "print(capital_1_trade[-30:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming capital_6trades and capital_1_trade are defined and have the same index\n",
    "capital_6trades_rolling = capital_4trades.rolling(30).sum()\n",
    "capital_1_trade_rolling = capital_1_trade.rolling(30).sum()\n",
    "\n",
    "# Plot the rolling sums on the same graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(capital_6trades_rolling, label='Capital 4 Trades Rolling Sum (30)')\n",
    "plt.plot(capital_1_trade_rolling, label='Capital 1 Trade Rolling Sum (30)')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Sum of Capital for 4 Trades and 1 Trade (Window=30 days)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rolling Sum')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the ending values\n",
    "print(f\"Capital 4 Trades Ending Sum: {capital_4trades[-30:].sum()}\")\n",
    "print(f\"Capital 1 Trade Ending Sum: {capital_1_trade[-30:].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecae782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac93851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72765fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rf_prob_30d.hist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rf_prob_30d.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"is_positive_growth_30d_future\"\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COL}' not found in df.columns\")\n",
    "y_true = df[TARGET_COL].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d79b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) Helpers ---\n",
    "def precision_at_k(y_true: pd.Series, y_scores: pd.Series, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Precision@k: Of the top k highest-scored samples, how many are positive?\n",
    "    - Handles k > n by clipping to n\n",
    "    - Handles NaNs by treating them as lowest score\n",
    "    - Keeps index alignment correct\n",
    "    \"\"\"\n",
    "    n = len(y_scores)\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    k = min(k, n)\n",
    "    # Replace NaNs with -inf so they go to the bottom\n",
    "    scores = y_scores.fillna(float(\"-inf\")).to_numpy()\n",
    "    # Argpartition is O(n) and faster than full sort for small k\n",
    "    if k == n:\n",
    "        topk_idx = np.argsort(scores)[::-1][:k]\n",
    "    else:\n",
    "        # get indices of k largest (unordered), then sort them to get true top order\n",
    "        part = np.argpartition(scores, -k)[-k:]\n",
    "        topk_idx = part[np.argsort(scores[part])[::-1]]\n",
    "    # y_true is a Series; iloc keeps positional alignment\n",
    "    topk_true = y_true.iloc[topk_idx]\n",
    "    return float(topk_true.mean())\n",
    "\n",
    "\n",
    "def lift_at_k(prec_k: float, base_rate: float) -> float:\n",
    "    \"\"\"Lift@k = precision@k / base_rate (>= 0).\"\"\"\n",
    "    if base_rate == 0:\n",
    "        return np.nan\n",
    "    return float(prec_k / base_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75509d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcaddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in df.columns if col.startswith('pred')]\n",
    "\n",
    "results = []\n",
    "k_values = [5, 10, 20, 50, 100, 200]\n",
    "base_rate = y_true.mean()\n",
    "\n",
    "for col in pred_cols:\n",
    "    y_scores = df[col]\n",
    "    # Robust binarization (0.5 threshold by default). If your model outputs logits,\n",
    "    # apply sigmoid first or adjust threshold as needed.\n",
    "    y_pred = (y_scores >= 0.5).astype(int)\n",
    "\n",
    "    # Some metrics need non-NaN scores\n",
    "    scores_no_nan = y_scores.fillna(y_scores.min() - 1e9)\n",
    "\n",
    "    row = {\n",
    "        \"model\": col,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, scores_no_nan),\n",
    "        \"pr_auc\": average_precision_score(y_true, scores_no_nan),  # area under PR curve\n",
    "        \"base_rate_%\": base_rate * 100,\n",
    "    }\n",
    "\n",
    "    # precision@k and lift@k\n",
    "    for k in k_values:\n",
    "        p_at_k = precision_at_k(y_true, y_scores, k)\n",
    "        row[f\"precision@{k}\"] = p_at_k\n",
    "        row[f\"lift@{k}\"] = lift_at_k(p_at_k, base_rate)\n",
    "\n",
    "    results.append(row)\n",
    "\n",
    "metrics_df = (\n",
    "    pd.DataFrame(results)\n",
    "      .sort_values([\"roc_auc\", \"pr_auc\"], ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Pretty display: show precision@k and lift@k together\n",
    "cols_order = (\n",
    "    [\"model\", \"roc_auc\", \"pr_auc\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"base_rate_%\"]\n",
    "    + sum(([f\"precision@{k}\", f\"lift@{k}\"] for k in k_values), [])\n",
    ")\n",
    "display(metrics_df[cols_order])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.describe(include=\"all\").transpose())\n",
    "\n",
    "# Missing values\n",
    "missing_summary = df.isnull().mean().sort_values(ascending=False)\n",
    "print(\"Missing % per column:\\n\", missing_summary.head(20))\n",
    "\n",
    "# Class balance\n",
    "print(\"Target balance:\", y_true.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a17e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c81f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [c for c in df.columns if c.startswith(\"pred\") and c != \"rf_prob_30d\"]\n",
    "prob_cols   = [\"rf_prob_30d\"]  # extend if you add more prob outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b71350",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in binary_cols:\n",
    "    y_pred = df[col].fillna(0).astype(int)\n",
    "    print(\n",
    "        col,\n",
    "        \"Acc:\", accuracy_score(y_true, y_pred),\n",
    "        \"Prec:\", precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Rec:\", recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1:\", f1_score(y_true, y_pred, zero_division=0),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [5, 10, 20, 50]:\n",
    "    p_at_k = precision_at_k(y_true, df[\"rf_prob_30d\"], k)\n",
    "    print(f\"rf_prob_30d | Precision@{k}: {p_at_k:.3f}, Lift@{k}: {lift_at_k(p_at_k, base_rate):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "prob_true, prob_pred = calibration_curve(y_true, df[\"rf_prob_30d\"], n_bins=10)\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"rf_prob_30d\")\n",
    "plt.plot([0,1],[0,1],\"--\", color=\"gray\")\n",
    "plt.legend(); plt.title(\"Calibration curve\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_threshold(y_true, y_scores, threshold=0.6):\n",
    "    picks = (y_scores >= threshold).astype(int)\n",
    "    return precision_score(y_true, picks), recall_score(y_true, picks)\n",
    "\n",
    "for t in [0.4, 0.5, 0.6, 0.7]:\n",
    "    prec, rec = simulate_threshold(y_true, df[\"rf_prob_30d\"], t)\n",
    "    print(f\"rf_prob_30d ={t}: Precision={prec:.3f}, Recall={rec:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e19cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_binary_strategy(df, pred_col, return_col=\"growth_future_30d\"):\n",
    "    picks = df[df[pred_col] == 1]\n",
    "    if picks.empty:\n",
    "        return np.nan\n",
    "    return {\n",
    "        \"avg_return\": picks[return_col].mean(),\n",
    "        \"win_rate\": (picks[return_col] > 0).mean(),\n",
    "        \"sharpe\": picks[return_col].mean() / (picks[return_col].std() + 1e-9),\n",
    "        \"n_trades\": len(picks),\n",
    "    }\n",
    "\n",
    "for col in binary_cols:\n",
    "    print(col, simulate_binary_strategy(df, col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eeccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_prob_threshold(df, prob_col=\"rf_prob_30d\", return_col=\"growth_future_30d\", tau=0.6):\n",
    "    picks = df[df[prob_col] >= tau]\n",
    "    if picks.empty:\n",
    "        return np.nan\n",
    "    return {\n",
    "        \"avg_return\": picks[return_col].mean(),\n",
    "        \"win_rate\": (picks[return_col] > 0).mean(),\n",
    "        \"sharpe\": picks[return_col].mean() / (picks[return_col].std() + 1e-9),\n",
    "        \"n_trades\": len(picks),\n",
    "    }\n",
    "\n",
    "for t in [0.4, 0.5, 0.6, 0.7,0.8,0.9,0.95]:\n",
    "    print(f\"={t}\", simulate_prob_threshold(df, tau=t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_topk(df, prob_col=\"rf_prob_30d\", return_col=\"growth_future_30d\", k=20):\n",
    "    ranked = df.sort_values(prob_col, ascending=False).head(k)\n",
    "    return {\n",
    "        \"avg_return\": ranked[return_col].mean(),\n",
    "        \"win_rate\": (ranked[return_col] > 0).mean(),\n",
    "        \"sharpe\": ranked[return_col].mean() / (ranked[return_col].std() + 1e-9),\n",
    "        \"n_trades\": len(ranked),\n",
    "    }\n",
    "\n",
    "for k in [5, 10, 20, 50,100,200,250]:\n",
    "    print(f\"Top-{k}\", simulate_topk(df, k=k))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9bf81fc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "def analyze_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"is_positive_growth_30d_future\",\n",
    "    return_col: str = \"growth_future_30d\",\n",
    "    prob_cols: Optional[List[str]] = None,   # e.g. [\"rf_prob_30d\"]\n",
    "    binary_cols: Optional[List[str]] = None, # e.g. [c for c in df if c.startswith(\"pred_\")]\n",
    "    k_values: List[int] = [5, 10, 20, 50, 100, 200],\n",
    "    thresholds: List[float] = [0.4, 0.5, 0.6, 0.7,0.8,0.9,0.95],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Unified EDA + Simulation for predictions (without calibration).\n",
    "\n",
    "    Returns:\n",
    "        metrics_df: rows per (strategy, variant) with classification metrics & precision@k/lift@k.\n",
    "        simulations_df: rows per (strategy, variant) with investment metrics: avg_return, win_rate, sharpe, n_trades.\n",
    "    \"\"\"\n",
    "    # ---------- helpers ----------\n",
    "    def precision_at_k(y_true: pd.Series, y_scores: pd.Series, k: int) -> float:\n",
    "        n = len(y_scores)\n",
    "        if n == 0:\n",
    "            return np.nan\n",
    "        k = min(k, n)\n",
    "        scores = y_scores.fillna(float(\"-inf\")).to_numpy()\n",
    "        if k == n:\n",
    "            topk_idx = np.argsort(scores)[::-1][:k]\n",
    "        else:\n",
    "            part = np.argpartition(scores, -k)[-k:]\n",
    "            topk_idx = part[np.argsort(scores[part])[::-1]]\n",
    "        return float(y_true.iloc[topk_idx].mean())\n",
    "\n",
    "    def lift_at_k(prec_k: float, base_rate: float) -> float:\n",
    "        if base_rate == 0 or pd.isna(prec_k):\n",
    "            return np.nan\n",
    "        return float(prec_k / base_rate)\n",
    "\n",
    "    def sharpe_ratio(returns: pd.Series, rf: float = 0.0) -> float:\n",
    "        returns = pd.to_numeric(returns, errors=\"coerce\").dropna()\n",
    "        if returns.empty:\n",
    "            return np.nan\n",
    "        excess = returns - rf\n",
    "        std = excess.std(ddof=1)\n",
    "        return float(excess.mean() / (std + 1e-12))\n",
    "\n",
    "    def _safe_auc(y_true, scores):\n",
    "        s = pd.to_numeric(scores, errors=\"coerce\")\n",
    "        if s.nunique(dropna=True) <= 1:\n",
    "            return np.nan, np.nan\n",
    "        s = s.fillna(s.min() - 1e9)\n",
    "        return float(roc_auc_score(y_true, s)), float(average_precision_score(y_true, s))\n",
    "\n",
    "    # ---------- inputs & guards ----------\n",
    "    assert target_col in df.columns, f\"Missing target_col: {target_col}\"\n",
    "    assert return_col in df.columns, f\"Missing return_col: {return_col}\"\n",
    "\n",
    "    if prob_cols is None:\n",
    "        prob_cols = [c for c in df.columns if c.endswith(\"_prob_30d\") or c.startswith(\"rf_prob\")]\n",
    "    if binary_cols is None:\n",
    "        binary_cols = [c for c in df.columns if c.startswith(\"pred\") and c not in prob_cols]\n",
    "\n",
    "    y_true = df[target_col].astype(int)\n",
    "    base_rate = float(y_true.mean())\n",
    "\n",
    "    # ---------- build outputs ----------\n",
    "    metrics_rows: List[Dict] = []\n",
    "    sim_rows: List[Dict] = []\n",
    "\n",
    "    # ===== 1) Binary strategies (0/1) =====\n",
    "    for col in binary_cols:\n",
    "        y_pred = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        row = {\n",
    "            \"strategy\": col,\n",
    "            \"type\": \"binary\",\n",
    "            \"base_rate\": base_rate,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"roc_auc\": np.nan,\n",
    "            \"pr_auc\": np.nan,\n",
    "        }\n",
    "        for k in k_values:\n",
    "            p_at_k = precision_at_k(y_true, y_pred, k)\n",
    "            row[f\"precision@{k}\"] = p_at_k\n",
    "            row[f\"lift@{k}\"] = lift_at_k(p_at_k, base_rate)\n",
    "        metrics_rows.append(row)\n",
    "        \n",
    "        std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "        efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "        ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan  # NaN-safe\n",
    "\n",
    "\n",
    "        # simulation: buy where label==1\n",
    "        picks = df.loc[y_pred == 1, return_col]\n",
    "        sim_rows.append({\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"binary@0.5\",\n",
    "            \"type\": \"binary\",\n",
    "            \"n_trades\": int(picks.notna().sum()),\n",
    "            \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "            \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "            \"sharpe\": sharpe_ratio(picks)\n",
    "        })\n",
    "\n",
    "    # ===== 2) Probabilistic strategies =====\n",
    "    for col in prob_cols:\n",
    "        scores = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        y_pred = (scores.fillna(0) >= 0.5).astype(int)\n",
    "        roc_auc, pr_auc = _safe_auc(y_true, scores)\n",
    "        row = {\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"prob_raw\",\n",
    "            \"type\": \"prob\",\n",
    "            \"base_rate\": base_rate,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"pr_auc\": pr_auc,\n",
    "        }\n",
    "        for k in k_values:\n",
    "            p_at_k = precision_at_k(y_true, scores, k)\n",
    "            row[f\"precision@{k}\"] = p_at_k\n",
    "            row[f\"lift@{k}\"] = lift_at_k(p_at_k, base_rate)\n",
    "        metrics_rows.append(row)\n",
    "\n",
    "        # simulations: thresholds\n",
    "        for t in thresholds:\n",
    "            picks = df.loc[scores >= t, return_col]\n",
    "            sim_rows.append({\n",
    "                \"strategy\": col,\n",
    "                \"variant\": f\"={t:.2f}\",\n",
    "                \"type\": \"prob\",\n",
    "                \"n_trades\": int(picks.notna().sum()),\n",
    "                \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "                \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "                \"sharpe\": sharpe_ratio(picks),\n",
    "            })\n",
    "\n",
    "        # simulations: top-k\n",
    "        for k in k_values:\n",
    "            ranked = df.loc[scores.sort_values(ascending=False).index][:k]\n",
    "            picks = ranked[return_col]\n",
    "            sim_rows.append({\n",
    "                \"strategy\": col,\n",
    "                \"variant\": f\"top-{k}\",\n",
    "                \"type\": \"prob\",\n",
    "                \"n_trades\": int(picks.notna().sum()),\n",
    "                \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "                \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "                \"sharpe\": sharpe_ratio(picks),\n",
    "                \"std_return\": std_ret,            # <-- NEW\n",
    "                \"efficiency\": efficiency,         # <-- NEW\n",
    "            \"ra_efficiency\": ra_efficiency,   # <-- NEW\n",
    "            })\n",
    "        \n",
    "        std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "        efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "        ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan  # NaN-safe\n",
    "\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows).reset_index(drop=True)\n",
    "    simulations_df = pd.DataFrame(sim_rows).reset_index(drop=True)\n",
    "\n",
    "    return metrics_df, simulations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "def analyze_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"is_positive_growth_30d_future\",\n",
    "    return_col: str = \"growth_future_30d\",\n",
    "    prob_cols: Optional[List[str]] = None,   # e.g. [\"rf_prob_30d\"]\n",
    "    binary_cols: Optional[List[str]] = None, # e.g. [c for c in df if c.startswith(\"pred_\")]\n",
    "    k_values: List[int] = [5, 10, 20, 50, 100, 200],\n",
    "    thresholds: List[float] = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95],\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Unified EDA + Simulation for predictions (without calibration).\n",
    "\n",
    "    Returns:\n",
    "        metrics_df: rows per (strategy, variant) with classification metrics & precision@k/lift@k.\n",
    "        simulations_df: rows per (strategy, variant) with investment metrics: avg_return, win_rate, sharpe, n_trades,\n",
    "                        plus std_return, efficiency, ra_efficiency.\n",
    "    \"\"\"\n",
    "    # ---------- helpers ----------\n",
    "    def precision_at_k(y_true: pd.Series, y_scores: pd.Series, k: int) -> float:\n",
    "        n = len(y_scores)\n",
    "        if n == 0: return np.nan\n",
    "        k = min(k, n)\n",
    "        scores = y_scores.fillna(float(\"-inf\")).to_numpy()\n",
    "        if k == n:\n",
    "            topk_idx = np.argsort(scores)[::-1][:k]\n",
    "        else:\n",
    "            part = np.argpartition(scores, -k)[-k:]\n",
    "            topk_idx = part[np.argsort(scores[part])[::-1]]\n",
    "        return float(y_true.iloc[topk_idx].mean())\n",
    "\n",
    "    def lift_at_k(prec_k: float, base_rate: float) -> float:\n",
    "        if base_rate == 0 or pd.isna(prec_k): return np.nan\n",
    "        return float(prec_k / base_rate)\n",
    "\n",
    "    def sharpe_ratio(returns: pd.Series, rf: float = 0.0) -> float:\n",
    "        r = pd.to_numeric(returns, errors=\"coerce\").dropna()\n",
    "        if r.empty: return np.nan\n",
    "        excess = r - rf\n",
    "        std = excess.std(ddof=1)\n",
    "        return float(excess.mean() / (std + 1e-12))\n",
    "\n",
    "    def _safe_auc(y_true, scores):\n",
    "        s = pd.to_numeric(scores, errors=\"coerce\")\n",
    "        if s.nunique(dropna=True) <= 1:\n",
    "            return np.nan, np.nan\n",
    "        s = s.fillna(s.min() - 1e9)\n",
    "        return float(roc_auc_score(y_true, s)), float(average_precision_score(y_true, s))\n",
    "\n",
    "    # ---------- inputs & guards ----------\n",
    "    assert target_col in df.columns, f\"Missing target_col: {target_col}\"\n",
    "    assert return_col in df.columns, f\"Missing return_col: {return_col}\"\n",
    "\n",
    "    if prob_cols is None:\n",
    "        prob_cols = [c for c in df.columns if c.endswith(\"_prob_30d\") or c.startswith(\"rf_prob\")]\n",
    "    if binary_cols is None:\n",
    "        binary_cols = [c for c in df.columns if c.startswith(\"pred\") and c not in prob_cols]\n",
    "\n",
    "    y_true = df[target_col].astype(int)\n",
    "    base_rate = float(y_true.mean())\n",
    "\n",
    "    metrics_rows: List[Dict] = []\n",
    "    sim_rows: List[Dict] = []\n",
    "\n",
    "    # ===== 1) Binary strategies (0/1) =====\n",
    "    for col in binary_cols:\n",
    "        y_pred = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        row = {\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"binary@0.5\",\n",
    "            \"type\": \"binary\",\n",
    "            \"base_rate\": base_rate,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"roc_auc\": np.nan,\n",
    "            \"pr_auc\": np.nan,\n",
    "        }\n",
    "        for k in k_values:\n",
    "            p_at_k = precision_at_k(y_true, y_pred, k)\n",
    "            row[f\"precision@{k}\"] = p_at_k\n",
    "            row[f\"lift@{k}\"] = lift_at_k(p_at_k, base_rate)\n",
    "        metrics_rows.append(row)\n",
    "\n",
    "        # Simulation: buy where label == 1\n",
    "        #picks = df.loc[y_pred == 1, return_col]\n",
    "\n",
    "        # after (factor -> simple)\n",
    "        picks_factor = df.loc[y_pred == 1, return_col]\n",
    "        picks = pd.to_numeric(picks_factor, errors=\"coerce\") - 1.0\n",
    "        std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "        efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "        ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "        sim_rows.append({\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"binary@0.5\",\n",
    "            \"type\": \"binary\",\n",
    "            \"n_trades\": int(picks.notna().sum()),\n",
    "            \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "            \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "            \"sharpe\": sharpe_ratio(picks),\n",
    "            \"std_return\": std_ret,\n",
    "            \"efficiency\": efficiency,\n",
    "            \"ra_efficiency\": ra_efficiency,\n",
    "        })\n",
    "\n",
    "    # ===== 2) Probabilistic strategies =====\n",
    "    for col in prob_cols:\n",
    "        scores = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Classifier view at 0.5\n",
    "        y_pred = (scores.fillna(0) >= 0.5).astype(int)\n",
    "        roc_auc, pr_auc = _safe_auc(y_true, scores)\n",
    "\n",
    "        row = {\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"prob_raw\",\n",
    "            \"type\": \"prob\",\n",
    "            \"base_rate\": base_rate,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"pr_auc\": pr_auc,\n",
    "        }\n",
    "        for k in k_values:\n",
    "            p_at_k = precision_at_k(y_true, scores, k)\n",
    "            row[f\"precision@{k}\"] = p_at_k\n",
    "            row[f\"lift@{k}\"] = lift_at_k(p_at_k, base_rate)\n",
    "        metrics_rows.append(row)\n",
    "\n",
    "        # Threshold simulations\n",
    "        for t in thresholds:\n",
    "            #picks = df.loc[scores >= t, return_col]\n",
    "            # after (factor -> simple)\n",
    "            mask = scores >= t                     # <-- use  here\n",
    "\n",
    "            picks_factor = df.loc[mask, return_col]\n",
    "\n",
    "            picks = pd.to_numeric(picks_factor, errors=\"coerce\") - 1.0\n",
    "            std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "            efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "            ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "            sim_rows.append({\n",
    "                \"strategy\": col,\n",
    "                \"variant\": f\"={t:.2f}\",\n",
    "                \"type\": \"prob\",\n",
    "                \"n_trades\": int(picks.notna().sum()),\n",
    "                \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "                \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "                \"sharpe\": sharpe_ratio(picks),\n",
    "                \"std_return\": std_ret,\n",
    "                \"efficiency\": efficiency,\n",
    "                \"ra_efficiency\": ra_efficiency,\n",
    "            })\n",
    "\n",
    "        # Top-K simulations\n",
    "        sorted_idx = scores.sort_values(ascending=False).index\n",
    "        for k in k_values:\n",
    "            ranked = df.loc[sorted_idx][:k]\n",
    "            #picks = ranked[return_col]\n",
    "\n",
    "            # after (factor -> simple)\n",
    "            picks_factor = ranked[return_col]      # <-- use ranked rows\n",
    "\n",
    "\n",
    "            picks = pd.to_numeric(picks_factor, errors=\"coerce\") - 1.0\n",
    "            std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "            efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "            ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "            sim_rows.append({\n",
    "                \"strategy\": col,\n",
    "                \"variant\": f\"top-{k}\",\n",
    "                \"type\": \"prob\",\n",
    "                \"n_trades\": int(picks.notna().sum()),\n",
    "                \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "                \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "                \"sharpe\": sharpe_ratio(picks),\n",
    "                \"std_return\": std_ret,\n",
    "                \"efficiency\": efficiency,\n",
    "                \"ra_efficiency\": ra_efficiency,\n",
    "            })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows).reset_index(drop=True)\n",
    "    simulations_df = pd.DataFrame(sim_rows).reset_index(drop=True)\n",
    "    return metrics_df, simulations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_cols = [\"rf_prob_30d\"]\n",
    "binary_cols = [c for c in df.columns if c.startswith(\"pred\")]\n",
    "\n",
    "metrics_df, simulations_df = analyze_predictions(\n",
    "    df.query('split == \"test\"'),\n",
    "    target_col=\"is_positive_growth_30d_future\",\n",
    "    return_col=\"growth_future_30d\",\n",
    "    prob_cols=prob_cols,\n",
    "    binary_cols=binary_cols,\n",
    "    k_values=[5, 10, 20, 50],\n",
    "    thresholds=[0.4, 0.5, 0.6, 0.7,0.8,0.9,0.95]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d6e4540",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "def analyze_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"is_positive_growth_30d_future\",\n",
    "    return_col: str = \"growth_future_30d\",\n",
    "    prob_cols: Optional[List[str]] = None,   # e.g. [\"rf_prob_30d\"]\n",
    "    binary_cols: Optional[List[str]] = None, # e.g. [c for c in df if c.startswith(\"pred_\")]\n",
    "    k_values: List[int] = [5, 10, 20, 50, 100, 200],\n",
    "    thresholds: List[float] = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "    split_col: Optional[str] = None,         # e.g. \"split\"\n",
    "    split_name: Optional[str] = None,        # e.g. \"test\" or None for all data\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Unified EDA + Simulation for predictions (without calibration).\n",
    "\n",
    "    Returns:\n",
    "        metrics_df: rows per (strategy, variant) with classification metrics & precision@k/lift@k.\n",
    "        simulations_df: rows per (strategy, variant) with investment metrics: avg_return, win_rate, sharpe, n_trades,\n",
    "                        plus std_return, efficiency, ra_efficiency.\n",
    "    \"\"\"\n",
    "    # ---------- helpers ----------\n",
    "    def precision_at_k(y_true: pd.Series, y_scores: pd.Series, k: int) -> float:\n",
    "        n = len(y_scores)\n",
    "        if n == 0: return np.nan\n",
    "        k = min(k, n)\n",
    "        scores = y_scores.fillna(float(\"-inf\")).to_numpy()\n",
    "        if k == n:\n",
    "            topk_idx = np.argsort(scores)[::-1][:k]\n",
    "        else:\n",
    "            part = np.argpartition(scores, -k)[-k:]\n",
    "            topk_idx = part[np.argsort(scores[part])[::-1]]\n",
    "        return float(y_true.iloc[topk_idx].mean())\n",
    "\n",
    "    def lift_at_k(prec_k: float, base_rate: float) -> float:\n",
    "        if base_rate == 0 or pd.isna(prec_k): return np.nan\n",
    "        return float(prec_k / base_rate)\n",
    "\n",
    "    def sharpe_ratio(returns: pd.Series, rf: float = 0.0) -> float:\n",
    "        r = pd.to_numeric(returns, errors=\"coerce\").dropna()\n",
    "        if r.empty: return np.nan\n",
    "        excess = r - rf\n",
    "        std = excess.std(ddof=1)\n",
    "        return float(excess.mean() / (std + 1e-12))\n",
    "\n",
    "    def _safe_auc(y_true, scores):\n",
    "        s = pd.to_numeric(scores, errors=\"coerce\")\n",
    "        if s.nunique(dropna=True) <= 1:\n",
    "            return np.nan, np.nan\n",
    "        s = s.fillna(s.min() - 1e9)\n",
    "        return float(roc_auc_score(y_true, s)), float(average_precision_score(y_true, s))\n",
    "\n",
    "    # ---------- inputs & guards ----------\n",
    "    assert target_col in df.columns, f\"Missing target_col: {target_col}\"\n",
    "    assert return_col in df.columns, f\"Missing return_col: {return_col}\"\n",
    "\n",
    "    # Filter to split if requested\n",
    "    work = df.copy()\n",
    "    if split_col and split_name:\n",
    "        work = work.loc[work[split_col] == split_name].copy()\n",
    "        if work.empty:\n",
    "            raise ValueError(f\"No rows found for {split_col} == {split_name}\")\n",
    "\n",
    "    if prob_cols is None:\n",
    "        prob_cols = [c for c in work.columns if c.endswith(\"_prob_30d\") or c.startswith(\"rf_prob\")]\n",
    "    if binary_cols is None:\n",
    "        binary_cols = [c for c in work.columns if c.startswith(\"pred\") and c not in prob_cols]\n",
    "\n",
    "    y_true = work[target_col].astype(int)\n",
    "    base_rate = float(y_true.mean())\n",
    "\n",
    "    metrics_rows: List[Dict] = []\n",
    "    sim_rows: List[Dict] = []\n",
    "\n",
    "    # ===== 1) Binary strategies (0/1) =====\n",
    "    for col in binary_cols:\n",
    "        y_pred = pd.to_numeric(work[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        row = {\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"binary@0.5\",\n",
    "            \"type\": \"binary\",\n",
    "            \"base_rate\": base_rate,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"roc_auc\": np.nan,\n",
    "            \"pr_auc\": np.nan,\n",
    "        }\n",
    "        for k in k_values:\n",
    "            p_at_k = precision_at_k(y_true, y_pred, k)\n",
    "            row[f\"precision@{k}\"] = p_at_k\n",
    "            row[f\"lift@{k}\"] = lift_at_k(p_at_k, base_rate)\n",
    "        metrics_rows.append(row)\n",
    "\n",
    "        # Simulation: buy where label == 1\n",
    "        picks_factor = work.loc[y_pred == 1, return_col]\n",
    "        picks = pd.to_numeric(picks_factor, errors=\"coerce\") - 1.0\n",
    "        std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "        efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "        ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "        sim_rows.append({\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"binary@0.5\",\n",
    "            \"type\": \"binary\",\n",
    "            \"n_trades\": int(picks.notna().sum()),\n",
    "            \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "            \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "            \"sharpe\": sharpe_ratio(picks),\n",
    "            \"std_return\": std_ret,\n",
    "            \"efficiency\": efficiency,\n",
    "            \"ra_efficiency\": ra_efficiency,\n",
    "        })\n",
    "\n",
    "    # ===== 2) Probabilistic strategies =====\n",
    "    for col in prob_cols:\n",
    "        scores = pd.to_numeric(work[col], errors=\"coerce\")\n",
    "\n",
    "        # Classifier view at 0.5\n",
    "        y_pred = (scores.fillna(0) >= 0.5).astype(int)\n",
    "        roc_auc, pr_auc = _safe_auc(y_true, scores)\n",
    "\n",
    "        row = {\n",
    "            \"strategy\": col,\n",
    "            \"variant\": \"prob_raw\",\n",
    "            \"type\": \"prob\",\n",
    "            \"base_rate\": base_rate,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"pr_auc\": pr_auc,\n",
    "        }\n",
    "        for k in k_values:\n",
    "            p_at_k = precision_at_k(y_true, scores, k)\n",
    "            row[f\"precision@{k}\"] = p_at_k\n",
    "            row[f\"lift@{k}\"] = lift_at_k(p_at_k, base_rate)\n",
    "        metrics_rows.append(row)\n",
    "\n",
    "        # Threshold simulations\n",
    "        for t in thresholds:\n",
    "            mask = scores >= t\n",
    "            picks_factor = work.loc[mask, return_col]\n",
    "            picks = pd.to_numeric(picks_factor, errors=\"coerce\") - 1.0\n",
    "            std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "            efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "            ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "            sim_rows.append({\n",
    "                \"strategy\": col,\n",
    "                \"variant\": f\"={t:.2f}\",\n",
    "                \"type\": \"prob\",\n",
    "                \"n_trades\": int(picks.notna().sum()),\n",
    "                \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "                \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "                \"sharpe\": sharpe_ratio(picks),\n",
    "                \"std_return\": std_ret,\n",
    "                \"efficiency\": efficiency,\n",
    "                \"ra_efficiency\": ra_efficiency,\n",
    "            })\n",
    "\n",
    "        # Top-K simulations\n",
    "        sorted_idx = scores.sort_values(ascending=False).index\n",
    "        for k in k_values:\n",
    "            ranked = work.loc[sorted_idx][:k]\n",
    "            picks_factor = ranked[return_col]\n",
    "            picks = pd.to_numeric(picks_factor, errors=\"coerce\") - 1.0\n",
    "            std_ret = float(picks.std(ddof=1)) if len(picks) > 1 else np.nan\n",
    "            efficiency = float(picks.mean() * len(picks)) if not picks.empty else np.nan\n",
    "            ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "            sim_rows.append({\n",
    "                \"strategy\": col,\n",
    "                \"variant\": f\"top-{k}\",\n",
    "                \"type\": \"prob\",\n",
    "                \"n_trades\": int(picks.notna().sum()),\n",
    "                \"avg_return\": float(picks.mean()) if not picks.empty else np.nan,\n",
    "                \"win_rate\": float((picks > 0).mean()) if not picks.empty else np.nan,\n",
    "                \"sharpe\": sharpe_ratio(picks),\n",
    "                \"std_return\": std_ret,\n",
    "                \"efficiency\": efficiency,\n",
    "                \"ra_efficiency\": ra_efficiency,\n",
    "            })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows).reset_index(drop=True)\n",
    "    simulations_df = pd.DataFrame(sim_rows).reset_index(drop=True)\n",
    "    return metrics_df, simulations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f7ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ae2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.split.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== METRICS ===\")\n",
    "metrics_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be0c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== SIMULATIONS ===\")\n",
    "(simulations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454134a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(simulations_df[[\"strategy\",\"variant\",\"n_trades\",\"avg_return\",\"win_rate\"]]\n",
    "        .sort_values([\"avg_return\",\"n_trades\"], ascending= False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7204368",
   "metadata": {},
   "outputs": [],
   "source": [
    "(simulations_df.query(\"type=='prob'\")[[\"strategy\",\"variant\",\"n_trades\",\"avg_return\",\"win_rate\"]]\n",
    "        .sort_values([\"avg_return\",\"n_trades\"], ascending= False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef634094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pareto_front(df: pd.DataFrame, cols=(\"avg_return\",\"sharpe\",\"n_trades\")) -> pd.DataFrame:\n",
    "    \"\"\"Return non-dominated strategies on the given columns (maximize all).\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    keep = []\n",
    "    vals = df[list(cols)].to_numpy()\n",
    "    for i, vi in enumerate(vals):\n",
    "        dominated = False\n",
    "        for j, vj in enumerate(vals):\n",
    "            if j == i: \n",
    "                continue\n",
    "            if np.all(np.nan_to_num(vj, nan=-1e9) >= np.nan_to_num(vi, nan=-1e9)) and \\\n",
    "               np.any(np.nan_to_num(vj, nan=-1e9) >  np.nan_to_num(vi, nan=-1e9)):\n",
    "                dominated = True\n",
    "                break\n",
    "        if not dominated:\n",
    "            keep.append(i)\n",
    "    return df.iloc[keep].sort_values(list(cols), ascending=False)\n",
    "\n",
    "def rank_strategies(\n",
    "    simulations_df: pd.DataFrame,\n",
    "    prefer_type: str = \"prob\",     # focus on prob strategies by default\n",
    "    min_trades: int = 5,           # basic capacity filter\n",
    "    weights=(1.0, 2.0, 0.2),       # (avg_return, sharpe, log(1+n_trades)) for utility\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns several ranked views:\n",
    "      - 'filtered': filtered sims\n",
    "      - 'pareto': Pareto frontier (avg_return, sharpe, n_trades)\n",
    "      - 'top_utility': by weighted utility\n",
    "      - 'top_efficiency': by avg_return * n_trades\n",
    "      - 'top_ra_efficiency': risk-adjusted efficiency\n",
    "    \"\"\"\n",
    "    if simulations_df.empty:\n",
    "        return {\"filtered\": simulations_df, \"pareto\": simulations_df,\n",
    "                \"top_utility\": simulations_df, \"top_efficiency\": simulations_df,\n",
    "                \"top_ra_efficiency\": simulations_df}\n",
    "\n",
    "    sdf = simulations_df.copy()\n",
    "    if prefer_type:\n",
    "        sdf = sdf[sdf[\"type\"] == prefer_type].copy()\n",
    "    if min_trades is not None:\n",
    "        sdf = sdf[sdf[\"n_trades\"] >= min_trades].copy()\n",
    "\n",
    "    # Utility score\n",
    "    w1, w2, w3 = weights\n",
    "    sdf[\"utility\"] = (\n",
    "        w1 * sdf[\"avg_return\"].fillna(-1e9) +\n",
    "        w2 * sdf[\"sharpe\"].fillna(-1e9) +\n",
    "        w3 * np.log1p(sdf[\"n_trades\"].clip(lower=0))\n",
    "    )\n",
    "\n",
    "    views = {\n",
    "        \"filtered\": sdf.sort_values([\"strategy\",\"variant\"]).reset_index(drop=True),\n",
    "        \"pareto\": pareto_front(sdf, cols=(\"avg_return\",\"sharpe\",\"n_trades\")).reset_index(drop=True),\n",
    "        \"top_utility\": sdf.sort_values(\"utility\", ascending=False).head(20).reset_index(drop=True),\n",
    "        \"top_efficiency\": sdf.sort_values(\"efficiency\", ascending=False).head(20).reset_index(drop=True),\n",
    "        \"top_ra_efficiency\": sdf.sort_values(\"ra_efficiency\", ascending=False).head(20).reset_index(drop=True),\n",
    "    }\n",
    "    return views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61926fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best strategies with multiple lenses\n",
    "views = rank_strategies(simulations_df, prefer_type=\"prob\", min_trades=5, weights=(1.0, 2.0, 0.2))\n",
    "\n",
    "pareto_df         = views[\"pareto\"]\n",
    "top_utility_df    = views[\"top_utility\"]\n",
    "top_eff_df        = views[\"top_efficiency\"]\n",
    "top_ra_eff_df     = views[\"top_ra_efficiency\"]\n",
    "\n",
    "print(\"Pareto frontier:\\n\", pareto_df.head(10))\n",
    "print(\"\\nTop by utility:\\n\", top_utility_df.head(10))\n",
    "print(\"\\nTop by efficiency:\\n\", top_eff_df.head(10))\n",
    "print(\"\\nTop by risk-adjusted efficiency:\\n\", top_ra_eff_df.head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63fe3c39",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def _ensure_dt(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if s.isna().any():\n",
    "        raise ValueError(\"Some dates could not be parsed. Check your date_col.\")\n",
    "    return s\n",
    "\n",
    "def _max_drawdown_from_equity(equity: pd.Series) -> Tuple[float, float]:\n",
    "    \"\"\"Return (max_dd_abs, max_dd_pct). equity must be indexed by date and positive.\"\"\"\n",
    "    if equity.empty:\n",
    "        return 0.0, 0.0\n",
    "    peaks = equity.cummax()\n",
    "    dd = equity - peaks\n",
    "    dd_pct = equity / peaks - 1.0\n",
    "    return float(dd.min()), float(dd_pct.min())  # negatives\n",
    "\n",
    "def _sharpe(series: pd.Series, rf: float = 0.0) -> float:\n",
    "    x = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if x.empty:\n",
    "        return np.nan\n",
    "    excess = x - rf\n",
    "    std = excess.std(ddof=1)\n",
    "    return float(excess.mean() / (std + 1e-12))\n",
    "\n",
    "# ----------------------------\n",
    "# Core simulator\n",
    "# ----------------------------\n",
    "def simulate_from_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    return_col: str = \"growth_future_30d\",     # realized forward *factor* (1+r), e.g., 1.03 for +3%\n",
    "    prob_cols: Optional[List[str]] = None,     # e.g., [\"rf_prob_30d\", \"xgb_prob_30d\"]\n",
    "    binary_cols: Optional[List[str]] = None,   # e.g., all columns starting with \"pred_\"\n",
    "    k_values: List[int] = (5, 10, 20, 50),     # per-date Top-K by prob\n",
    "    prob_thresholds: List[float] = (0.6, 0.7, 0.8, 0.9),  # global prob cut (applied per date naturally)\n",
    "    hold_days: int = 30,                       # holding window in days\n",
    "    invest_per_trade: float = 1000.0,          # notional per position\n",
    "    fee_rate: float = 0.0005,                  # round-trip fee as fraction of notional (e.g., 5 bps)\n",
    "    split_col: Optional[str] = None,           # if you want to restrict to a split\n",
    "    split_name: Optional[str] = None,          # e.g., \"test\"\n",
    "    risk_free_annual: float = 0.0,             # set to e.g. 0.05 for 5% annual US T-bill equiv\n",
    "    debug: bool = False,                       # set True to print debug info for problematic strategies\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a summary DataFrame: one row per (strategy, variant) with:\n",
    "      n_trades, avg_return, win_rate, per_trade_sharpe, daily_sharpe,\n",
    "      capital_required, net_pnl, max_dd_abs, max_dd_pct, cagr,\n",
    "      avg_positions/day, p75_positions/day, efficiency, ra_efficiency.\n",
    "\n",
    "    NOTE: `return_col` must be a *factor* (1+r). We convert to simple returns internally (r = factor - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    assert return_col in df.columns, f\"{return_col} not in df\"\n",
    "    assert date_col in df.columns, f\"{date_col} not in df\"\n",
    "\n",
    "    # Filter to split if requested\n",
    "    work = df.copy()\n",
    "    if split_col and split_name:\n",
    "        work = work.loc[work[split_col] == split_name].copy()\n",
    "        if work.empty:\n",
    "            raise ValueError(f\"No rows found for {split_col} == {split_name}\")\n",
    "\n",
    "    # Autodetect columns if not provided\n",
    "    if prob_cols is None:\n",
    "        prob_cols = [c for c in work.columns if c.endswith(\"_prob_30d\") or c.startswith(\"rf_prob\")]\n",
    "    if binary_cols is None:\n",
    "        binary_cols = [c for c in work.columns if c.lower().startswith(\"pred\") and c not in prob_cols]\n",
    "\n",
    "    # Guard\n",
    "    if len(prob_cols) == 0 and len(binary_cols) == 0:\n",
    "        raise ValueError(\"No strategies found. Provide prob_cols and/or binary_cols.\")\n",
    "\n",
    "    # Prepare\n",
    "    work = work.copy()\n",
    "    work[date_col] = _ensure_dt(work[date_col])\n",
    "    work = work.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    # Quick sanity: ensure return_col looks like a factor (~1.0 median)\n",
    "    s = pd.to_numeric(work[return_col], errors=\"coerce\").dropna()\n",
    "    if not (0.5 < s.median() < 1.5):\n",
    "        raise ValueError(f\"{return_col} doesn't look like a factor (1+r). If it's already simple returns, \"\n",
    "                         f\"rename or adjust the code to skip '- 1.0'. Median value: {s.median():.4f}\")\n",
    "\n",
    "    # Risk-free per trade (approx over hold_days)\n",
    "    if risk_free_annual and risk_free_annual > 0:\n",
    "        rf_per_trade = (1.0 + risk_free_annual)**(hold_days/365.25) - 1.0\n",
    "    else:\n",
    "        rf_per_trade = 0.0\n",
    "\n",
    "    summaries: List[Dict] = []\n",
    "\n",
    "    # Helper: compute summary from a table of selected rows (\"trades\")\n",
    "    def summarize_trades(trades: pd.DataFrame, label_strategy: str, label_variant: str) -> Dict:\n",
    "        # --- Realized returns per trade (FACTOR -> SIMPLE) ---\n",
    "        ret_factor = pd.to_numeric(trades[return_col], errors=\"coerce\").dropna()\n",
    "        ret = ret_factor - 1.0                     # simple returns (e.g., 1.03 -> 0.03)\n",
    "        n_trades = int(len(ret))\n",
    "\n",
    "        if n_trades == 0:\n",
    "            return {\n",
    "                \"strategy\": label_strategy, \"variant\": label_variant,\n",
    "                \"n_trades\": 0, \"avg_return\": np.nan, \"win_rate\": np.nan,\n",
    "                \"per_trade_sharpe\": np.nan, \"daily_sharpe\": np.nan,\n",
    "                \"capital_required\": 0.0, \"net_pnl\": 0.0,\n",
    "                \"max_dd_abs\": 0.0, \"max_dd_pct\": 0.0, \"cagr\": 0.0,\n",
    "                \"avg_pos_per_day\": 0.0, \"p75_pos_per_day\": 0.0,\n",
    "                \"efficiency\": np.nan, \"ra_efficiency\": np.nan\n",
    "            }\n",
    "\n",
    "        # --- Per-trade economics (use SIMPLE return) ---\n",
    "        gross = invest_per_trade * ret              # $ P&L per trade before fees\n",
    "        fees  = -invest_per_trade * fee_rate        # constant round-trip fee per trade\n",
    "        net_per_trade = gross + fees\n",
    "\n",
    "        net_pnl = float(net_per_trade.sum())\n",
    "        avg_return = float(ret.mean())\n",
    "        win_rate = float((ret > 0).mean())\n",
    "        per_trade_sharpe = _sharpe(ret, rf=rf_per_trade)\n",
    "\n",
    "        # Build entry/exit dates for concurrency & daily P&L\n",
    "        trades_with_dates = trades.copy()\n",
    "        trades_with_dates[\"_entry\"] = pd.to_datetime(trades_with_dates[date_col])\n",
    "        trades_with_dates[\"_exit\"]  = trades_with_dates[\"_entry\"] + pd.Timedelta(days=hold_days - 1)\n",
    "\n",
    "        # Daily P&L (book P&L on entry day)\n",
    "        daily = (\n",
    "            pd.DataFrame({\"date\": trades_with_dates[\"_entry\"], \"pnl\": net_per_trade.values})\n",
    "            .groupby(\"date\")[\"pnl\"].sum()\n",
    "        )\n",
    "\n",
    "        # Active positions per day via difference array (vectorized)\n",
    "        start_day = trades_with_dates[\"_entry\"].min()\n",
    "        end_day   = trades_with_dates[\"_exit\"].max()\n",
    "        days = pd.date_range(start_day, end_day, freq=\"D\")\n",
    "        idx = pd.Index(days)\n",
    "\n",
    "        delta = pd.Series(0.0, index=idx)\n",
    "        entry_counts = trades_with_dates[\"_entry\"].value_counts()\n",
    "        exit_counts  = (trades_with_dates[\"_exit\"] + pd.Timedelta(days=1)).value_counts()\n",
    "        delta = delta.add(entry_counts, fill_value=0.0)\n",
    "        delta = delta.add(-exit_counts, fill_value=0.0)\n",
    "        active_pos = delta.cumsum()\n",
    "\n",
    "        avg_pos = float(active_pos.mean())\n",
    "        p75_pos = float(active_pos.quantile(0.75))\n",
    "        \n",
    "        # --- REVISED CAPITAL CALCULATION ---\n",
    "        # Use a more robust approach that doesn't create extreme values\n",
    "        total_capital_deployed = float(n_trades * invest_per_trade)\n",
    "        avg_capital_needed = float(invest_per_trade * max(avg_pos, 1.0))  # At least 1 position worth\n",
    "        \n",
    "        # Capital required = max of several reasonable estimates\n",
    "        capital_estimates = [\n",
    "            avg_capital_needed,                           # Based on average positions\n",
    "            total_capital_deployed * 0.1,                # 10% of total capital deployed\n",
    "            invest_per_trade * max(p75_pos, 1.0),       # P75 approach (with minimum)\n",
    "        ]\n",
    "        capital_required = float(max(capital_estimates))\n",
    "\n",
    "        # --- REVISED EQUITY CURVE ---\n",
    "        # Align daily pnl to full index\n",
    "        daily_all = pd.Series(0.0, index=idx)\n",
    "        daily_all.loc[daily.index] = daily.values\n",
    "        \n",
    "        # Start equity curve at capital_required\n",
    "        equity = capital_required + daily_all.cumsum()\n",
    "        \n",
    "        # Ensure equity never goes negative (add emergency capital if needed)\n",
    "        min_equity = equity.min()\n",
    "        if min_equity < 0:\n",
    "            emergency_capital = abs(min_equity) * 1.1  # 10% buffer\n",
    "            capital_required += emergency_capital\n",
    "            equity = capital_required + daily_all.cumsum()\n",
    "\n",
    "        max_dd_abs, max_dd_pct = _max_drawdown_from_equity(equity)\n",
    "\n",
    "        # Daily Sharpe (uses daily net pnl / capital as return proxy)\n",
    "        daily_ret_proxy = daily_all / (capital_required + 1e-12)\n",
    "        daily_sharpe = _sharpe(daily_ret_proxy, rf=0.0)\n",
    "\n",
    "        # --- REVISED CAGR CALCULATION ---\n",
    "        years = max((idx[-1] - idx[0]).days / 365.25, 1/365.25)\n",
    "        starting = float(capital_required)\n",
    "        ending   = float(equity.iloc[-1])\n",
    "        \n",
    "        if starting > 0:\n",
    "            cagr_raw = (ending / starting) ** (1.0 / years) - 1.0\n",
    "            # Cap CAGR at reasonable bounds to prevent extreme values\n",
    "            cagr = float(max(-0.99, min(cagr_raw, 50.0)))  # Between -99% and 5000%\n",
    "        else:\n",
    "            cagr = 0.0\n",
    "\n",
    "        # Debug output for problematic cases\n",
    "        if debug and (abs(cagr) > 5.0 or max_dd_pct == 0.0):\n",
    "            print(f\"\\n=== DEBUG: {label_strategy} | {label_variant} ===\")\n",
    "            print(f\"n_trades: {n_trades}, avg_return: {avg_return:.4f}\")\n",
    "            print(f\"Capital required: {capital_required:.0f}\")\n",
    "            print(f\"Starting equity: {starting:.0f}, Ending equity: {ending:.0f}\")\n",
    "            print(f\"Years: {years:.2f}, Raw CAGR: {cagr_raw:.4f}, Capped CAGR: {cagr:.4f}\")\n",
    "            print(f\"Active positions: min={active_pos.min():.1f}, mean={avg_pos:.1f}, max={active_pos.max():.1f}\")\n",
    "            print(f\"Equity: min={equity.min():.0f}, max={equity.max():.0f}\")\n",
    "            print(f\"Max DD: {max_dd_abs:.0f} ({max_dd_pct:.4f})\")\n",
    "\n",
    "        # Custom efficiency metrics\n",
    "        efficiency = float(avg_return * n_trades)\n",
    "        std_ret = float(ret.std(ddof=1)) if n_trades > 1 else np.nan\n",
    "        ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "        return {\n",
    "            \"strategy\": label_strategy, \"variant\": label_variant,\n",
    "            \"n_trades\": n_trades, \"avg_return\": avg_return, \"win_rate\": win_rate,\n",
    "            \"per_trade_sharpe\": per_trade_sharpe, \"daily_sharpe\": daily_sharpe,\n",
    "            \"capital_required\": capital_required, \"net_pnl\": net_pnl,\n",
    "            \"max_dd_abs\": max_dd_abs, \"max_dd_pct\": max_dd_pct, \"cagr\": cagr,\n",
    "            \"avg_pos_per_day\": avg_pos, \"p75_pos_per_day\": p75_pos,\n",
    "            \"efficiency\": efficiency, \"ra_efficiency\": ra_efficiency\n",
    "        }\n",
    "\n",
    "    # ---------------------------------\n",
    "    # A) Binary strategies (per-date)\n",
    "    # ---------------------------------\n",
    "    for col in (binary_cols or []):\n",
    "        if col not in work.columns:\n",
    "            continue\n",
    "        trades = work.loc[work[col].astype(float) == 1.0, [date_col, return_col]].copy()\n",
    "        summaries.append(summarize_trades(trades, col, \"binary@1\"))\n",
    "\n",
    "    # ---------------------------------\n",
    "    # B) Prob strategies: per-date Top-K & thresholds\n",
    "    # ---------------------------------\n",
    "    for col in (prob_cols or []):\n",
    "        if col not in work.columns:\n",
    "            continue\n",
    "        scores = pd.to_numeric(work[col], errors=\"coerce\")\n",
    "\n",
    "        # Top-K per date\n",
    "        g = work.assign(_score=scores).dropna(subset=[\"_score\"]).groupby(work[date_col])\n",
    "        for k in k_values:\n",
    "            try:\n",
    "                picks = (\n",
    "                    g.apply(lambda d: d.nlargest(k, columns=\"_score\"))\n",
    "                     .reset_index(level=0, drop=True)\n",
    "                     [[date_col, return_col]]\n",
    "                )\n",
    "                summaries.append(summarize_trades(picks, col, f\"top-{k}\"))\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"Warning: Failed to process {col} top-{k}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Threshold (scores >= )\n",
    "        for t in prob_thresholds:\n",
    "            picks = work.loc[scores >= t, [date_col, return_col]]\n",
    "            summaries.append(summarize_trades(picks, col, f\"={t:.2f}\"))\n",
    "\n",
    "    result = pd.DataFrame(summaries)\n",
    "    if not result.empty:\n",
    "        result = result.sort_values([\"strategy\", \"variant\"]).reset_index(drop=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def _ensure_dt(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if s.isna().any():\n",
    "        raise ValueError(\"Some dates could not be parsed. Check your date_col.\")\n",
    "    return s\n",
    "\n",
    "def _max_drawdown_from_equity(equity: pd.Series) -> Tuple[float, float]:\n",
    "    \"\"\"Return (max_dd_abs, max_dd_pct). equity must be indexed by date and positive.\"\"\"\n",
    "    if equity.empty:\n",
    "        return 0.0, 0.0\n",
    "    peaks = equity.cummax()\n",
    "    dd = equity - peaks\n",
    "    dd_pct = equity / peaks - 1.0\n",
    "    return float(dd.min()), float(dd_pct.min())  # negatives\n",
    "\n",
    "def _sharpe(series: pd.Series, rf: float = 0.0) -> float:\n",
    "    x = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if x.empty:\n",
    "        return np.nan\n",
    "    excess = x - rf\n",
    "    std = excess.std(ddof=1)\n",
    "    return float(excess.mean() / (std + 1e-12))\n",
    "\n",
    "# ----------------------------\n",
    "# Core simulator\n",
    "# ----------------------------\n",
    "def simulate_from_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    return_col: str = \"growth_future_30d\",     # realized forward *factor* (1+r), e.g., 1.03 for +3%\n",
    "    prob_cols: Optional[List[str]] = None,     # e.g., [\"rf_prob_30d\", \"xgb_prob_30d\"]\n",
    "    binary_cols: Optional[List[str]] = None,   # e.g., all columns starting with \"pred_\"\n",
    "    k_values: List[int] = (5, 10, 20, 50),     # per-date Top-K by prob\n",
    "    prob_thresholds: List[float] = (0.5,0.55,0.6,0.65,0.7,0.75, 0.8,0.85, 0.9,0.95),  # per-date prob cuts (global thresholds)\n",
    "    hold_days: int = 30,                       # holding window in days\n",
    "    invest_per_trade: float = 1000.0,          # notional per position\n",
    "    fee_rate: float = 0.0005,                  # round-trip fee as fraction of notional (e.g., 5 bps)\n",
    "    split_col: Optional[str] = None,           # if you want to restrict to a split\n",
    "    split_name: Optional[str] = None,          # e.g., \"test\"\n",
    "    risk_free_annual: float = 0.0,             # set to e.g. 0.05 for 5% annual US T-bill equiv\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a summary DataFrame: one row per (strategy, variant) with:\n",
    "      n_trades, avg_return, win_rate, per_trade_sharpe, daily_sharpe,\n",
    "      capital_required, net_pnl, max_dd_abs, max_dd_pct, cagr,\n",
    "      avg_positions/day, p75_positions/day, efficiency, ra_efficiency.\n",
    "\n",
    "    NOTE: `return_col` must be a *factor* (1+r). We convert to simple returns internally (r = factor - 1).\n",
    "    \"\"\"\n",
    "    assert return_col in df.columns, f\"{return_col} not in df\"\n",
    "    assert date_col in df.columns, f\"{date_col} not in df\"\n",
    "\n",
    "    # Filter to split if requested\n",
    "    work = df.copy()\n",
    "    if split_col and split_name:\n",
    "        work = work.loc[work[split_col] == split_name].copy()\n",
    "        if work.empty:\n",
    "            raise ValueError(f\"No rows found for {split_col} == {split_name}\")\n",
    "\n",
    "    # Autodetect columns if not provided\n",
    "    if prob_cols is None:\n",
    "        prob_cols = [c for c in work.columns if c.endswith(\"_prob_30d\") or c.startswith(\"rf_prob\")]\n",
    "    if binary_cols is None:\n",
    "        binary_cols = [c for c in work.columns if c.lower().startswith(\"pred\") and c not in prob_cols]\n",
    "\n",
    "    # Guard\n",
    "    if len(prob_cols) == 0 and len(binary_cols) == 0:\n",
    "        raise ValueError(\"No strategies found. Provide prob_cols and/or binary_cols.\")\n",
    "\n",
    "    # Prepare\n",
    "    work = work.copy()\n",
    "    work[date_col] = _ensure_dt(work[date_col])\n",
    "    work = work.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "    # Sanity: ensure return_col looks like a factor (~1.0 median)\n",
    "    s = pd.to_numeric(work[return_col], errors=\"coerce\").dropna()\n",
    "    if not (0.5 < s.median() < 1.5):\n",
    "        raise ValueError(\n",
    "            f\"{return_col} doesn't look like a factor (1+r). \"\n",
    "            f\"If it's already simple returns, adjust the code to skip '- 1.0'. \"\n",
    "            f\"Median value: {s.median():.4f}\"\n",
    "        )\n",
    "\n",
    "    # Risk-free per trade (approx over hold_days)\n",
    "    rf_per_trade = (1.0 + risk_free_annual)**(hold_days/365.25) - 1.0 if risk_free_annual > 0 else 0.0\n",
    "\n",
    "    summaries: List[Dict] = []\n",
    "\n",
    "    # Helper: compute summary from selected rows (\"trades\")\n",
    "    def summarize_trades(trades: pd.DataFrame, label_strategy: str, label_variant: str) -> Dict:\n",
    "        # --- Realized returns per trade (FACTOR -> SIMPLE) ---\n",
    "        ret_factor = pd.to_numeric(trades[return_col], errors=\"coerce\").dropna()\n",
    "        ret = ret_factor - 1.0                     # simple returns (e.g., 1.03 -> 0.03)\n",
    "        n_trades = int(len(ret))\n",
    "\n",
    "        if n_trades == 0:\n",
    "            return {\n",
    "                \"strategy\": label_strategy, \"variant\": label_variant,\n",
    "                \"n_trades\": 0, \"avg_return\": np.nan, \"win_rate\": np.nan,\n",
    "                \"per_trade_sharpe\": np.nan, \"daily_sharpe\": np.nan,\n",
    "                \"capital_required\": 0.0, \"net_pnl\": 0.0,\n",
    "                \"max_dd_abs\": 0.0, \"max_dd_pct\": 0.0, \"cagr\": 0.0,\n",
    "                \"avg_pos_per_day\": 0.0, \"p75_pos_per_day\": 0.0,\n",
    "                \"efficiency\": np.nan, \"ra_efficiency\": np.nan\n",
    "            }\n",
    "\n",
    "        # --- Per-trade economics (use SIMPLE return) ---\n",
    "        gross = invest_per_trade * ret              # $ P&L per trade before fees\n",
    "        fees  = -invest_per_trade * fee_rate        # constant round-trip fee per trade\n",
    "        net_per_trade = gross + fees\n",
    "\n",
    "        net_pnl = float(net_per_trade.sum())\n",
    "        avg_return = float(ret.mean())\n",
    "        win_rate = float((ret > 0).mean())\n",
    "        per_trade_sharpe = _sharpe(ret, rf=rf_per_trade)\n",
    "\n",
    "        # Build entry/exit dates for concurrency & daily P&L\n",
    "        twd = trades.copy()\n",
    "        twd[\"_entry\"] = pd.to_datetime(twd[date_col])\n",
    "        twd[\"_exit\"]  = twd[\"_entry\"] + pd.Timedelta(days=hold_days - 1)\n",
    "\n",
    "        # --- Daily P&L (book on EXIT day) ---\n",
    "        daily = (\n",
    "            pd.DataFrame({\"date\": twd[\"_exit\"], \"pnl\": net_per_trade.values})\n",
    "            .groupby(\"date\")[\"pnl\"].sum()\n",
    "        )\n",
    "\n",
    "        # --- Active positions per day via difference array (vectorized) ---\n",
    "        start_day = twd[\"_entry\"].min()\n",
    "        end_day   = twd[\"_exit\"].max()\n",
    "        days = pd.date_range(start_day, end_day, freq=\"D\")\n",
    "        idx = pd.Index(days, name=\"date\")\n",
    "\n",
    "        delta = pd.Series(0.0, index=idx)\n",
    "        entry_counts = twd[\"_entry\"].value_counts()\n",
    "        exit_counts  = (twd[\"_exit\"] + pd.Timedelta(days=1)).value_counts()\n",
    "        delta = delta.add(entry_counts, fill_value=0.0)\n",
    "        delta = delta.add(-exit_counts, fill_value=0.0)\n",
    "        active_pos = delta.cumsum()\n",
    "\n",
    "        avg_pos = float(active_pos.mean())\n",
    "        p75_pos = float(active_pos.quantile(0.75))\n",
    "        pos_for_cap = max(avg_pos, p75_pos, 1.0)          # 1 position equivalent\n",
    "        capital_required = float(invest_per_trade * pos_for_cap)\n",
    "\n",
    "        # --- Equity & drawdown ---\n",
    "        daily_all = pd.Series(0.0, index=idx)\n",
    "        daily_all.loc[daily.index] = daily.values\n",
    "        equity = capital_required + daily_all.cumsum()\n",
    "\n",
    "        max_dd_abs, max_dd_pct = _max_drawdown_from_equity(equity)\n",
    "\n",
    "        # --- Daily Sharpe: time-weighted by capital in use ---\n",
    "        cap_in_use = invest_per_trade * active_pos\n",
    "        daily_ret_proxy = (daily_all / cap_in_use.replace(0, np.nan)).fillna(0.0)\n",
    "        daily_sharpe = _sharpe(daily_ret_proxy, rf=0.0)\n",
    "\n",
    "        # --- CAGR over the full backtest interval ---\n",
    "        years = max((idx[-1] - idx[0]).days / 365.25, 0.5)   #  6 months\n",
    "        starting = max(capital_required, invest_per_trade)\n",
    "        ending   = float(equity.iloc[-1])\n",
    "        cagr = float((ending / starting) ** (1.0 / years) - 1.0)\n",
    "\n",
    "        # Custom efficiency metrics\n",
    "        efficiency = float(avg_return * n_trades)\n",
    "        std_ret = float(ret.std(ddof=1)) if n_trades > 1 else np.nan\n",
    "        ra_efficiency = (efficiency / (std_ret + 1e-12)) if std_ret == std_ret else np.nan\n",
    "\n",
    "        return {\n",
    "            \"strategy\": label_strategy, \"variant\": label_variant,\n",
    "            \"n_trades\": n_trades, \"avg_return\": avg_return, \"win_rate\": win_rate,\n",
    "            \"per_trade_sharpe\": per_trade_sharpe, \"daily_sharpe\": daily_sharpe,\n",
    "            \"capital_required\": capital_required, \"net_pnl\": net_pnl,\n",
    "            \"max_dd_abs\": max_dd_abs, \"max_dd_pct\": max_dd_pct, \"cagr\": cagr,\n",
    "            \"avg_pos_per_day\": avg_pos, \"p75_pos_per_day\": p75_pos,\n",
    "            \"efficiency\": efficiency, \"ra_efficiency\": ra_efficiency\n",
    "        }\n",
    "\n",
    "    # ---------------------------------\n",
    "    # A) Binary strategies (per-date)\n",
    "    # ---------------------------------\n",
    "    for col in (binary_cols or []):\n",
    "        if col not in work.columns:\n",
    "            continue\n",
    "        trades = work.loc[work[col].astype(float) == 1.0, [date_col, return_col]].copy()\n",
    "        summaries.append(summarize_trades(trades, col, \"binary@1\"))\n",
    "\n",
    "    # ---------------------------------\n",
    "    # B) Prob strategies: per-date Top-K & thresholds\n",
    "    # ---------------------------------\n",
    "    for col in (prob_cols or []):\n",
    "        if col not in work.columns:\n",
    "            continue\n",
    "        scores = pd.to_numeric(work[col], errors=\"coerce\")\n",
    "\n",
    "        # Top-K per date\n",
    "        tmp = work.assign(_score=scores).dropna(subset=[\"_score\"])\n",
    "        g = tmp.groupby(date_col)\n",
    "        for k in k_values:\n",
    "            picks = (\n",
    "                g.apply(lambda d: d.nlargest(k, columns=\"_score\"))\n",
    "                 .reset_index(level=0, drop=True)\n",
    "                 [[date_col, return_col]]\n",
    "            )\n",
    "            summaries.append(summarize_trades(picks, col, f\"top-{k}\"))\n",
    "\n",
    "        # Threshold (scores >= )\n",
    "        for t in prob_thresholds:\n",
    "            picks = work.loc[scores >= t, [date_col, return_col]]\n",
    "            summaries.append(summarize_trades(picks, col, f\"={t:.2f}\"))\n",
    "\n",
    "    result = pd.DataFrame(summaries)\n",
    "    if not result.empty:\n",
    "        result = result.sort_values([\"strategy\", \"variant\"]).reset_index(drop=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example wiring (tweak names if yours differ)\n",
    "summary = simulate_from_predictions(\n",
    "    df=df,                      # your loaded predictions dataframe\n",
    "    date_col=\"Date\",                  # <- your timestamp column\n",
    "    return_col=\"growth_future_30d\",   # realized forward return\n",
    "    prob_cols=[\"rf_prob_30d\"],        # add more prob columns if you have them\n",
    "    binary_cols=[c for c in df.columns if c.startswith(\"pred\")],\n",
    "    k_values=[5, 10, 20, 50,75,100],\n",
    "    prob_thresholds=[0.6, 0.65,0.7,0.75, 0.8,0.85, 0.9,0.95],\n",
    "    hold_days=30,\n",
    "    invest_per_trade=1000.0,\n",
    "    fee_rate=0.0005,\n",
    "    split_col=\"split\",                   # or \"split\"\n",
    "    split_name=\"test\",                  # or \"test\"\n",
    "    risk_free_annual=0.05              # set to 0.05 to include a 5% annual RF in per-trade Sharpe\n",
    ")\n",
    "\n",
    "\n",
    "hold_days = 30  # or whatever you used\n",
    "summary = summary.copy()\n",
    "\n",
    "# Per-trade annualization (geometric, safer)\n",
    "summary[\"per_trade_annualized\"] = np.exp(\n",
    "    (365/hold_days) * np.log1p(summary[\"avg_return\"].clip(lower=-0.999999)).replace([-np.inf, np.inf], np.nan)\n",
    ") - 1\n",
    "\n",
    "# Pretty %\n",
    "for col in [\"avg_return\", \"per_trade_annualized\", \"cagr\"]:\n",
    "    summary[f\"{col}_%\"] = (summary[col] * 100).round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.loc[:, [\"strategy\",\"variant\",\"n_trades\",\n",
    "                \"avg_return_%\",\"win_rate\",\"per_trade_annualized_%\",\"cagr_%\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(\n",
    "    by=[\"avg_return\", \"n_trades\"],\n",
    "    ascending=[False, True],     # high avg_return first, then fewer trades first\n",
    "    na_position=\"last\"\n",
    ")\n",
    "\n",
    "# 1) avg_return (per-trade, over the hold window) or Average 30-day return per trade: +X%.\n",
    "# 2) Cagr : The annualized growth rate of your portfolio equity over the whole backtest, starting with capital_required and adding daily P&L (we book P&L on exit day) Run as a portfolio with realistic concurrency, the strategy achieved a CAGR of Z% over the backtest period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if your returns are realistic\n",
    "ret_stats = df['growth_future_30d'].describe()\n",
    "print(f\"Return distribution:\\n{ret_stats}\")\n",
    "\n",
    "# Check win rate vs market\n",
    "market_win_rate = (df['growth_future_30d'] > 1.0).mean()\n",
    "print(f\"Market win rate: {market_win_rate:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your target variable timing\n",
    "print(\"Feature date range:\", df['Date'].min(), \"to\", df['Date'].max())\n",
    "print(\"Target date range for growth_future_30d\")\n",
    "\n",
    "# Make sure predictions are made BEFORE target period\n",
    "sample_row = df.iloc[100]\n",
    "print(f\"Prediction made on: {sample_row['Date']}\")\n",
    "print(f\"Return measured from: {sample_row['Date']} to {sample_row['Date'] + pd.Timedelta(days=30)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b202d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for features that might use future data\n",
    "suspicious_features = [col for col in df.columns if 'future' in col.lower()]\n",
    "print(\"Suspicious features:\", suspicious_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure test data comes AFTER training data\n",
    "train_dates = df[df['split'] == 'train']['Date']\n",
    "test_dates = df[df['split'] == 'test']['Date']\n",
    "valid_dates = df[df['split'] == 'validation']['Date']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train period: {train_dates.min()} to {train_dates.max()}\")\n",
    "print(f\"Test period: {test_dates.min()} to {test_dates.max()}\")\n",
    "print(f\"Valid period: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "\n",
    "\n",
    "# This should be True\n",
    "print(f\"Test starts after train ends: {test_dates.min() > train_dates.max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
