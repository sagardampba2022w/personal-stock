{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock Prediction Decision Making Notebook\n",
    "# Interactive notebook for making daily trading decisions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "TOP_N_PICKS = 10\n",
    "INVESTMENT_AMOUNT = 1000\n",
    "\n",
    "# Path configuration - adjust for notebook location\n",
    "# Since notebook is in eda-notebooks/, go up one level to project root\n",
    "PROJECT_ROOT = Path(\"..\").resolve()  # Go up one level from eda-notebooks/\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "APP_DIR = PROJECT_ROOT / \"app\"\n",
    "\n",
    "print(\"üìä Stock Prediction Decision Making Notebook\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup paths\n",
    "if str(APP_DIR) not in sys.path:\n",
    "    sys.path.append(str(APP_DIR))\n",
    "\n",
    "print(f\"‚úÖ Paths configured:\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   App code: {APP_DIR}\")\n",
    "print(f\"   Artifacts: {ARTIFACTS_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08407a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go up one level to the root directory\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "\n",
    "\n",
    "# # add project root (parent of notebooks/) to sys.path\n",
    "# project_root = Path.cwd().parent\n",
    "# sys.path.insert(0, str(project_root))\n",
    "\n",
    "from app.predictions import (\n",
    "    load_latest_data,\n",
    "    load_model_and_features,\n",
    "    PredictionComparator,\n",
    "    _TransformAdapter,\n",
    ")\n",
    "\n",
    "# If you also need TrainModel directly in the notebook:\n",
    "from app.train_model_new import TrainModel   # ‚úÖ absolute package import\n",
    "\n",
    "from app.stock_pipeline import StockDataPipeline\n",
    "\n",
    "\n",
    "# Import modules\n",
    "try:\n",
    "    from app.predictions import load_model_and_features, _TransformAdapter\n",
    "    from app.train_model_new import TrainModel\n",
    "    from app.stock_pipeline import StockDataPipeline\n",
    "    print(\"‚úÖ All modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Make sure your app directory contains all required modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49c429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Tickers\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 1: LOAD TICKERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "tickers = (pd.read_csv(\"/Users/sagardhal/Desktop/Practice/personal-stock/ticker/spx_ndx_liq_top250_latest.csv\")['Ticker']\n",
    "        #.head(5)\n",
    "        .tolist())\n",
    "tickers\n",
    "len(tickers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3379b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check Requirements\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: CHECK REQUIREMENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for trained models\n",
    "artifacts_files = list(ARTIFACTS_DIR.glob(\"*.joblib\")) + list(ARTIFACTS_DIR.glob(\"*.pkl\"))\n",
    "print(f\"ü§ñ Models found: {len(artifacts_files)}\")\n",
    "for f in artifacts_files:\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "# Check for data files\n",
    "data_files = list(DATA_DIR.glob(\"*.parquet\")) if DATA_DIR.exists() else []\n",
    "print(f\"üìä Data files found: {len(data_files)}\")\n",
    "for f in data_files[:3]:\n",
    "    print(f\"   - {f.name}\")\n",
    "if len(data_files) > 3:\n",
    "    print(f\"   ... and {len(data_files) - 3} more files\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4af18aa3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def fetch_fresh_ohlcv_fast(tickers, days_back=30):\n",
    "    \"\"\"\n",
    "    Most efficient approach: one batched yfinance call for all tickers,\n",
    "    then reshape to long format. Prints per-ticker status after download.\n",
    "    \"\"\"\n",
    "    # Clean + de-dupe\n",
    "    tickers = [t.strip().upper() for t in tickers if t and str(t).strip()]\n",
    "    tickers = list(dict.fromkeys(tickers))\n",
    "    if not tickers:\n",
    "        raise ValueError(\"No valid tickers provided\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2 (FAST): FETCH FRESH DATA ‚Äî Single batched call\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Fetching last {days_back} days of data for {len(tickers)} tickers...\")\n",
    "    # Small pad for weekends/holidays; using period is often more robust than start/end\n",
    "    period_days = days_back + 2\n",
    "    print(f\"   Period: {period_days} days\")\n",
    "\n",
    "    # ---- One batched download ----\n",
    "    df = yf.download(\n",
    "        \" \".join(tickers),\n",
    "        period=f\"{period_days}d\",\n",
    "        interval=\"1d\",\n",
    "        auto_adjust=True,\n",
    "        actions=False,          # no dividends/splits to keep it lean\n",
    "        group_by=\"column\",      # flat column groups (field, ticker) MultiIndex\n",
    "        threads=True,\n",
    "        progress=False,\n",
    "        #show_errors=True,\n",
    "    )\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        raise ValueError(\"No data returned by yfinance for the requested tickers/period\")\n",
    "\n",
    "    # Normalize rows\n",
    "    df = df.dropna(how=\"all\")\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"All rows are NaN after cleaning\")\n",
    "\n",
    "    # ---- Reshape to long (Ticker, Date, OHLCV) ----\n",
    "    # yfinance with group_by=\"column\" gives a MultiIndex on columns:\n",
    "    # one level = field names (Open, High, Low, Close, Adj Close, Volume)\n",
    "    # the other level = tickers. We'll detect which is which and reshape.\n",
    "    fields = {\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"}\n",
    "    if not isinstance(df.columns, pd.MultiIndex):\n",
    "        # Single-ticker case: plain columns -> make it look like multi for consistency\n",
    "        df.columns = pd.MultiIndex.from_product([df.columns, [tickers[0]]])\n",
    "\n",
    "    lvl0 = set(map(str, df.columns.get_level_values(0)))\n",
    "    lvl1 = set(map(str, df.columns.get_level_values(1)))\n",
    "\n",
    "    # Determine which level is fields\n",
    "    if fields & lvl0:\n",
    "        field_level, ticker_level = 0, 1\n",
    "    elif fields & lvl1:\n",
    "        field_level, ticker_level = 1, 0\n",
    "        df.columns = df.columns.swaplevel(0, 1)  # put (field, ticker) order\n",
    "    else:\n",
    "        # Fallback: assume first level is field\n",
    "        field_level, ticker_level = 0, 1\n",
    "\n",
    "    # After ensuring (field, ticker), stack tickers to rows\n",
    "    df_long = (\n",
    "        df.stack(level=1)  # stack ticker level to rows\n",
    "          .reset_index()\n",
    "          .rename(columns={\"level_1\": \"Ticker\"})\n",
    "    )\n",
    "\n",
    "    # Clean column names\n",
    "    df_long.columns = [str(c).replace(\" \", \"_\") for c in df_long.columns]\n",
    "    # Ensure standard set exists (some tickers may miss columns on illiquid days)\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj_Close\", \"Volume\"]:\n",
    "        if col not in df_long.columns:\n",
    "            df_long[col] = pd.NA\n",
    "\n",
    "    # ---- Per-ticker status summary ----\n",
    "    successful_tickers = []\n",
    "    empty_tickers = []\n",
    "    for t in tickers:\n",
    "        n = len(df_long[df_long[\"Ticker\"] == t])\n",
    "        if n > 0:\n",
    "            successful_tickers.append(t)\n",
    "            print(f\"‚úÖ {t}: {n} rows\")\n",
    "        else:\n",
    "            empty_tickers.append(t)\n",
    "            print(f\"‚ö†Ô∏è {t}: no rows\")\n",
    "\n",
    "    if not successful_tickers:\n",
    "        raise ValueError(\"No data fetched for any ticker\")\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n‚úÖ Fetched data for {len(successful_tickers)} tickers \"\n",
    "          f\"(empty: {len(empty_tickers)})\")\n",
    "    print(f\"   Total observations: {len(df_long)}\")\n",
    "    try:\n",
    "        min_d = df_long['Date'].min()\n",
    "        max_d = df_long['Date'].max()\n",
    "        print(f\"   Date range in data: {min_d} to {max_d}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df_long, successful_tickers\n",
    "\n",
    "# ---- Example usage ----\n",
    "# raw_data, successful_tickers = fetch_fresh_ohlcv_fast(tickers, days_back=30)\n",
    "# print(f\"\\nüìä Raw data shape: {raw_data.shape}\")\n",
    "# latest_date = raw_data['Date'].max()\n",
    "# latest_sample = raw_data[raw_data['Date'] == latest_date].head(3)\n",
    "# print(f\"\\nüìÖ Latest data sample ({latest_date}):\")\n",
    "# print(latest_sample[['Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']].to_string(index=False))\n",
    "\n",
    "raw_data, successful_tickers = fetch_fresh_ohlcv_fast(tickers, days_back=1)\n",
    "\n",
    "latest_data = raw_data[['Date', 'Ticker', 'Close', 'High', 'Low', 'Open', 'Volume',\n",
    "       'Adj_Close']]\n",
    "\n",
    "latest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301fba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: TRANSFORM DATA USING YOUR STOCKDATAPIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: TRANSFORM DATA USING STOCKDATAPIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_stock_pipeline_for_predictions(tickers):\n",
    "    \"\"\"Run your StockDataPipeline to get fully transformed data\"\"\"\n",
    "    print(\"üöÄ Running StockDataPipeline for complete feature engineering...\")\n",
    "    \n",
    "    # Configuration (same as your run_data_extraction.py)\n",
    "    config = {\n",
    "        \"LOOKBACKS\": [1, 3, 7, 30, 90, 252, 365],\n",
    "        \"HORIZONS\": [30],\n",
    "        \"BINARY_THRESHOLDS\": {30: 1.00},  # 0% gain threshold\n",
    "    }\n",
    "    \n",
    "    print(f\"   Configuration:\")\n",
    "    print(f\"   - Lookbacks: {config['LOOKBACKS']}\")\n",
    "    print(f\"   - Horizons: {config['HORIZONS']} days\")  \n",
    "    print(f\"   - Binary threshold: {config['BINARY_THRESHOLDS'][30]:.0%}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize StockDataPipeline (same as extraction script)\n",
    "        pipeline = StockDataPipeline(\n",
    "            tickers=tickers,\n",
    "            lookbacks=config[\"LOOKBACKS\"],\n",
    "            horizons=config[\"HORIZONS\"],\n",
    "            binarize_thresholds=config[\"BINARY_THRESHOLDS\"],\n",
    "        )\n",
    "        \n",
    "        # Run complete pipeline: stock data + technical indicators + macro data\n",
    "        print(\"   üìä Running complete pipeline...\")\n",
    "        print(\"      - Step 1: Fetching stock data\")\n",
    "        print(\"      - Step 2: Adding technical indicators (TA-Lib)\")  \n",
    "        print(\"      - Step 3: Adding macro indicators\")\n",
    "        print(\"      - Step 4: Final validation and cleanup\")\n",
    "        \n",
    "        transformed_data = pipeline.run_complete_pipeline()\n",
    "        \n",
    "        print(f\"   ‚úÖ StockDataPipeline complete!\")\n",
    "        print(f\"      Final shape: {transformed_data.shape}\")\n",
    "        \n",
    "        # Get latest data for predictions\n",
    "        latest_date = transformed_data['Date'].max()\n",
    "        prediction_data = transformed_data[transformed_data['Date'] == latest_date].copy()\n",
    "        \n",
    "        print(f\"   üìÖ Prediction data ready:\")\n",
    "        print(f\"      Latest date: {latest_date.date()}\")\n",
    "        print(f\"      Stocks: {len(prediction_data)}\")\n",
    "        \n",
    "        # Show feature categories\n",
    "        feature_categories = {\n",
    "            \"Basic OHLCV\": [c for c in transformed_data.columns if c in [\"Date\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]],\n",
    "            \"Growth Features\": [c for c in transformed_data.columns if c.startswith(\"growth_\") and \"future\" not in c],\n",
    "            \"Technical Indicators\": [c for c in transformed_data.columns if any(x in c.lower() for x in [\"rsi\", \"macd\", \"sma\", \"adx\", \"cci\"])],\n",
    "            \"Candlestick Patterns\": [c for c in transformed_data.columns if c.startswith(\"cdl\")],\n",
    "            \"Macro Features\": [c for c in transformed_data.columns if c.endswith((\"_yoy\", \"_qoq\")) or \"btc\" in c.lower() or \"vix\" in c.lower()],\n",
    "            \"Target Variables\": [c for c in transformed_data.columns if \"future\" in c and (\"positive\" in c or \"growth\" in c)],\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   üìà Features created:\")\n",
    "        total_features = 0\n",
    "        for category, features in feature_categories.items():\n",
    "            print(f\"      {category}: {len(features)}\")\n",
    "            total_features += len(features)\n",
    "        print(f\"      Total: {total_features} features\")\n",
    "        \n",
    "        return transformed_data, prediction_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå StockDataPipeline failed: {e}\")\n",
    "        print(f\"   This could be due to:\")\n",
    "        print(f\"   - API rate limits from yfinance/FRED\")\n",
    "        print(f\"   - Missing TA-Lib dependencies\") \n",
    "        print(f\"   - Network connectivity issues\")\n",
    "        print(f\"   - Insufficient historical data\")\n",
    "        raise\n",
    "\n",
    "# Run StockDataPipeline\n",
    "try:\n",
    "    print(\"üîÑ Starting StockDataPipeline transformation...\")\n",
    "    full_data, prediction_data = run_stock_pipeline_for_predictions(tickers)\n",
    "    \n",
    "    print(\"‚úÖ StockDataPipeline transformation complete!\")\n",
    "    print(f\"   Ready for model predictions: {len(prediction_data)} stocks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå StockDataPipeline failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check internet connection\")\n",
    "    print(\"2. Verify TA-Lib is installed: pip install TA-Lib\")  \n",
    "    print(\"3. Try with fewer tickers (reduce TOP_N_PICKS)\")\n",
    "    print(\"4. Check yfinance/FRED API limits\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"growth_btc_1d\",\"growth_btc_7d\",\"growth_btc_30d\",\"growth_btc_90d\",\n",
    "        \"growth_vix_1d\",\"growth_vix_7d\",\"growth_vix_30d\",\"growth_vix_90d\"]\n",
    "\n",
    "full_data.query(\"Date in ['2025-09-08','2025-09-09']\")[[\"Date\"]+cols].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f386739b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# STEP 4: PREPARE FOR MODEL INFERENCE USING TRAINMODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: PREPARE FOR MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def prepare_for_model_inference(pipeline_data,window_days=7):\n",
    "    \"\"\"Use TrainModel to prepare StockDataPipeline output for inference\"\"\"\n",
    "    print(\"ü§ñ Using TrainModel for inference preparation...\")\n",
    "    \n",
    "    try:\n",
    "        # Create adapter and TrainModel (consistent with predictions.py)\n",
    "        adapter = _TransformAdapter(pipeline_data)\n",
    "        tm = TrainModel(adapter)\n",
    "        \n",
    "        # Prepare for inference (creates dummy variables, etc.)\n",
    "        tm.prepare_dataframe(start_date=\"2000-01-01\")\n",
    "        \n",
    "        print(f\"   ‚úÖ TrainModel preparation complete\")\n",
    "        print(f\"      Shape: {tm.df_full.shape}\")\n",
    "        \n",
    "        # Get latest data for predictions\n",
    "        latest_date = tm.df_full['Date'].max()\n",
    "        inference_data = tm.df_full[tm.df_full['Date'] == latest_date].copy()\n",
    "        \n",
    "        print(f\"   üìä Inference ready: {len(inference_data)} stocks\")\n",
    "        \n",
    "        return tm, inference_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TrainModel preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Prepare data for inference\n",
    "    tm, inference_data = prepare_for_model_inference(full_data)\n",
    "    \n",
    "    # Load trained model\n",
    "    print(\"\\nüìÇ Loading trained model...\")\n",
    "    model, feature_cols, target_col = load_model_and_features(str(ARTIFACTS_DIR))\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded:\")\n",
    "    print(f\"   Type: {type(model).__name__}\")\n",
    "    print(f\"   Features expected: {len(feature_cols)}\")\n",
    "    print(f\"   Target: {target_col}\")\n",
    "    \n",
    "    # Set up TrainModel for inference (same as predictions.py)\n",
    "    tm.model = model\n",
    "    tm._inference_feature_columns = feature_cols\n",
    "    if target_col:\n",
    "        tm.target_col = target_col\n",
    "    \n",
    "    print(\"‚úÖ Model setup complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model setup failed: {e}\")\n",
    "    print(\"Make sure you have a trained model in the artifacts directory\")\n",
    "    print(\"Run: python run_model_training.py --mode basic\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddb575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e80a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: PREPARE FOR MODEL INFERENCE USING TRAINMODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: PREPARE FOR MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd  # ensure available for Timedelta/to_datetime\n",
    "\n",
    "def prepare_for_model_inference(pipeline_data, window_days=7):\n",
    "    \"\"\"Use TrainModel to prepare StockDataPipeline output for inference.\"\"\"\n",
    "    print(\"ü§ñ Using TrainModel for inference preparation...\")\n",
    "    try:\n",
    "        # Create adapter and TrainModel (consistent with predictions.py)\n",
    "        adapter = _TransformAdapter(pipeline_data)\n",
    "        tm = TrainModel(adapter)\n",
    "\n",
    "        # Prepare for inference (creates dummy variables, etc.)\n",
    "        tm.prepare_dataframe(start_date=\"2000-01-01\")\n",
    "\n",
    "        print(f\"   ‚úÖ TrainModel preparation complete\")\n",
    "        print(f\"      Shape: {tm.df_full.shape}\")\n",
    "\n",
    "        # ---- Slice to last `window_days` calendar days (with hardening) ----\n",
    "        # Safety: ensure Date is datetime\n",
    "        tm.df_full['Date'] = pd.to_datetime(tm.df_full['Date'])\n",
    "\n",
    "        latest_date = tm.df_full['Date'].max()\n",
    "        week_start = latest_date - pd.Timedelta(days=window_days - 1)\n",
    "\n",
    "        inference_data = tm.df_full[\n",
    "            (tm.df_full['Date'] >= week_start) &\n",
    "            (tm.df_full['Date'] <= latest_date)\n",
    "        ].copy()\n",
    "\n",
    "        if inference_data.empty:\n",
    "            raise ValueError(\n",
    "                f\"No inference rows between {week_start.date()} and {latest_date.date()}. \"\n",
    "                \"Check upstream dates/timezones or reduce window_days.\"\n",
    "            )\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        print(f\"   üìä Inference window: {week_start.date()} ‚Üí {latest_date.date()}  |  Rows: {len(inference_data)}\")\n",
    "\n",
    "        return tm, inference_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå TrainModel preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Prepare data for inference (last 7 calendar days)\n",
    "    tm, inference_data = prepare_for_model_inference(full_data, window_days=7)\n",
    "\n",
    "    # Load trained model\n",
    "    print(\"\\nüìÇ Loading trained model...\")\n",
    "    model, feature_cols, target_col = load_model_and_features(str(ARTIFACTS_DIR))\n",
    "\n",
    "    print(f\"‚úÖ Model loaded:\")\n",
    "    print(f\"   Type: {type(model).__name__}\")\n",
    "    print(f\"   Features expected: {len(feature_cols)}\")\n",
    "    print(f\"   Target: {target_col}\")\n",
    "\n",
    "    # Set up TrainModel for inference (same as predictions.py)\n",
    "    tm.model = model\n",
    "    tm._inference_feature_columns = feature_cols\n",
    "    if target_col:\n",
    "        tm.target_col = target_col\n",
    "\n",
    "    print(\"‚úÖ Model setup complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model setup or prediction failed: {e}\")\n",
    "    print(\"Make sure you have a trained model in the artifacts directory\")\n",
    "    print(\"Run: python run_model_training.py --mode basic\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301771cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Debug info:\")\n",
    "print(f\"   Data shape: {tm.df_full.shape if hasattr(tm, 'df_full') else 'N/A'}\")\n",
    "if 'feature_cols' in locals() and hasattr(tm, 'df_full'):\n",
    "    available = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "    missing = [f for f in feature_cols if f not in tm.df_full.columns]\n",
    "    print(f\"   Available features (sample): {available}\")\n",
    "    print(f\"   Missing features (sample): {missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273132d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE PREDICTION GENERATOR CLASS DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class CompletePredictionGenerator:\n",
    "    \"\"\"\n",
    "    Generates all prediction types used in your simulation analysis:\n",
    "    - ML probabilities and thresholds\n",
    "    - Manual rule-based predictions  \n",
    "    - Ensemble combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, model, feature_cols: List[str]):\n",
    "        self.data = data.copy()\n",
    "        self.model = model\n",
    "        self.feature_cols = feature_cols\n",
    "        self.prediction_cols = []\n",
    "        \n",
    "    def generate_all_predictions(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate complete set of predictions matching your simulation analysis\"\"\"\n",
    "        print(\"Generating complete prediction set...\")\n",
    "        \n",
    "        # 1. Generate ML predictions\n",
    "        self._generate_ml_predictions()\n",
    "        \n",
    "        # 2. Generate manual rule-based predictions\n",
    "        self._generate_manual_predictions()\n",
    "        \n",
    "        # 3. Generate ensemble predictions\n",
    "        self._generate_ensemble_predictions()\n",
    "        \n",
    "        print(f\"Generated {len(self.prediction_cols)} prediction strategies\")\n",
    "        return self.data\n",
    "    \n",
    "    def _generate_ml_predictions(self):\n",
    "        \"\"\"Generate ML-based predictions with multiple thresholds\"\"\"\n",
    "        print(\"  Generating ML predictions...\")\n",
    "        \n",
    "        # Generate base probabilities\n",
    "        X = self._prepare_feature_matrix()\n",
    "        probabilities = self.model.predict_proba(X)[:, 1]\n",
    "        self.data['rf_prob_30d'] = probabilities\n",
    "        \n",
    "        # Fixed threshold strategies\n",
    "        thresholds = [0.20, 0.50,0.55,0.6, 0.65,0.7,0.75 ,0.80, 0.85,0.90]\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            col_name = f'pred{10 + i}_rf_thresh_{int(threshold * 100)}'\n",
    "            self.data[col_name] = (probabilities >= threshold).astype(int)\n",
    "            self.prediction_cols.append(col_name)\n",
    "        \n",
    "        # Dynamic threshold strategies (based on validation quantiles)\n",
    "        self._generate_adaptive_thresholds(probabilities)\n",
    "        \n",
    "        # Daily top-K strategies\n",
    "        self._generate_topk_strategies(probabilities)\n",
    "    \n",
    "    def _generate_adaptive_thresholds(self, probabilities):\n",
    "        \"\"\"Generate adaptive thresholds based on quantiles\"\"\"\n",
    "        target_rates = [0.01, 0.03, 0.05]  # 1%, 3%, 5% selection rates\n",
    "        \n",
    "        for i, rate in enumerate(target_rates):\n",
    "            threshold = np.quantile(probabilities, 1.0 - rate)\n",
    "            col_name = f'pred{15 + i}_rf_auto_rate_{int(rate * 100)}p'\n",
    "            self.data[col_name] = (probabilities >= threshold).astype(int)\n",
    "            self.prediction_cols.append(col_name)\n",
    "    \n",
    "    def _generate_topk_strategies(self, probabilities):\n",
    "        \"\"\"Generate daily top-K strategies\"\"\"\n",
    "        if 'Date' in self.data.columns:\n",
    "            for k in [3, 5, 10]:\n",
    "                col_name = f'pred{30 + k//3}_top{k}_daily'\n",
    "                \n",
    "                # Group by date and select top K per date\n",
    "                top_k_mask = (\n",
    "                    self.data.groupby('Date')['rf_prob_30d']\n",
    "                    .rank(method='first', ascending=False)\n",
    "                    .le(k)\n",
    "                    .astype(int)\n",
    "                )\n",
    "                self.data[col_name] = top_k_mask\n",
    "                self.prediction_cols.append(col_name)\n",
    "    \n",
    "    def _generate_manual_predictions(self):\n",
    "        \"\"\"Generate manual rule-based predictions\"\"\"\n",
    "        print(\"  Generating manual rule-based predictions...\")\n",
    "        \n",
    "        # pred0: CCI momentum breakout\n",
    "        if 'cci' in self.data.columns:\n",
    "            self.data['pred0_manual_cci'] = (\n",
    "                pd.to_numeric(self.data['cci'], errors='coerce').fillna(0) > 200\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred0_manual_cci')\n",
    "        \n",
    "        # pred1: Previous 30d growth momentum\n",
    "        growth_30d_col = self._find_growth_column(['growth_30d'])\n",
    "        if growth_30d_col:\n",
    "            self.data['pred1_manual_prev_g1'] = (\n",
    "                pd.to_numeric(self.data[growth_30d_col], errors='coerce').fillna(0) > 1\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred1_manual_prev_g1')\n",
    "        \n",
    "        # pred2: Combined stock and S&P500 momentum\n",
    "        snp_col = self._find_growth_column(['growth_snp500_30d', 'growth_snp_30d'])\n",
    "        if growth_30d_col and snp_col:\n",
    "            stock_momentum = pd.to_numeric(self.data[growth_30d_col], errors='coerce').fillna(0) > 1\n",
    "            snp_momentum = pd.to_numeric(self.data[snp_col], errors='coerce').fillna(0) > 1\n",
    "            self.data['pred2_manual_prev_g1_and_snp'] = (stock_momentum & snp_momentum).astype(int)\n",
    "            self.prediction_cols.append('pred2_manual_prev_g1_and_snp')\n",
    "        \n",
    "        # pred3: Declining interest rates environment\n",
    "        dgs10_col = self._find_column(['dgs10_yoy'])\n",
    "        dgs5_col = self._find_column(['dgs5_yoy'])\n",
    "        if dgs10_col and dgs5_col:\n",
    "            dgs10_declining = pd.to_numeric(self.data[dgs10_col], errors='coerce').fillna(0) < 0\n",
    "            dgs5_declining = pd.to_numeric(self.data[dgs5_col], errors='coerce').fillna(0) < 0\n",
    "            self.data['pred3_manual_declining_rates'] = (dgs10_declining & dgs5_declining).astype(int)\n",
    "            self.prediction_cols.append('pred3_manual_declining_rates')\n",
    "        \n",
    "        # pred4: Federal Reserve easing cycle\n",
    "        fedfunds_col = self._find_column(['fedfunds_yoy'])\n",
    "        if fedfunds_col:\n",
    "            self.data['pred4_manual_fed_easing'] = (\n",
    "                pd.to_numeric(self.data[fedfunds_col], errors='coerce').fillna(0) < -0.1\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred4_manual_fed_easing')\n",
    "        \n",
    "        # pred5: VIX contrarian signal (fear spike = buy opportunity)\n",
    "        vix_col = self._find_growth_column(['growth_vix_30d'])\n",
    "        if vix_col:\n",
    "            self.data['pred5_manual_vix_contrarian'] = (\n",
    "                pd.to_numeric(self.data[vix_col], errors='coerce').fillna(0) > 0.2\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred5_manual_vix_contrarian')\n",
    "        \n",
    "        # pred6: Combined stock and Bitcoin momentum\n",
    "        btc_col = self._find_growth_column(['growth_btc_30d'])\n",
    "        if growth_30d_col and btc_col:\n",
    "            stock_momentum = pd.to_numeric(self.data[growth_30d_col], errors='coerce').fillna(0) > 1.0\n",
    "            btc_momentum = pd.to_numeric(self.data[btc_col], errors='coerce').fillna(0) > 1.0\n",
    "            self.data['pred6_manual_stock_btc_momentum'] = (stock_momentum & btc_momentum).astype(int)\n",
    "            self.prediction_cols.append('pred6_manual_stock_btc_momentum')\n",
    "    \n",
    "    def _generate_ensemble_predictions(self):\n",
    "        \"\"\"Generate ensemble predictions combining manual and ML\"\"\"\n",
    "        print(\"  Generating ensemble predictions...\")\n",
    "        \n",
    "        manual_preds = [c for c in self.prediction_cols if 'manual' in c]\n",
    "        ml_mid = [c for c in self.prediction_cols if 'rf_thresh_50' in c]\n",
    "        ml_auto = [c for c in self.prediction_cols if 'rf_auto_rate_' in c]\n",
    "        topk_preds = [c for c in self.prediction_cols if 'top' in c and 'daily' in c]\n",
    "        \n",
    "        # Ensemble A: ML confidence + momentum confirmation\n",
    "        if ml_mid and 'pred1_manual_prev_g1' in manual_preds:\n",
    "            ml_signal = self.data[ml_mid[0]] == 1\n",
    "            momentum_signal = self.data['pred1_manual_prev_g1'] == 1\n",
    "            self.data['pred20_ens_ml50_and_momentum'] = (ml_signal & momentum_signal).astype(int)\n",
    "            self.prediction_cols.append('pred20_ens_ml50_and_momentum')\n",
    "        \n",
    "        # Ensemble B: High conviction ML OR daily top picks\n",
    "        auto_1p = [c for c in ml_auto if c.endswith('_1p')]\n",
    "        if auto_1p and topk_preds:\n",
    "            auto_signal = self.data[auto_1p[0]] == 1\n",
    "            top_signal = self.data[topk_preds[0]] == 1  # Use first top-K strategy\n",
    "            self.data['pred21_ens_auto1p_or_top3'] = (auto_signal | top_signal).astype(int)\n",
    "            self.prediction_cols.append('pred21_ens_auto1p_or_top3')\n",
    "        \n",
    "        # Ensemble C: Majority manual rules + ML confirmation\n",
    "        if len(manual_preds) >= 3:\n",
    "            manual_sum = self.data[manual_preds].sum(axis=1)\n",
    "            auto_3p = [c for c in ml_auto if c.endswith('_3p')]\n",
    "            if auto_3p:\n",
    "                manual_majority = manual_sum >= 2\n",
    "                ml_confirmation = self.data[auto_3p[0]] == 1\n",
    "                self.data['pred22_ens_manual2plus_and_auto3p'] = (manual_majority & ml_confirmation).astype(int)\n",
    "                self.prediction_cols.append('pred22_ens_manual2plus_and_auto3p')\n",
    "    \n",
    "    def _prepare_feature_matrix(self):\n",
    "        \"\"\"Prepare feature matrix for ML model\"\"\"\n",
    "        # Build feature matrix matching training features\n",
    "        missing_features = []\n",
    "        X_cols = []\n",
    "        \n",
    "        for col in self.feature_cols:\n",
    "            if col in self.data.columns:\n",
    "                X_cols.append(self.data[col])\n",
    "            else:\n",
    "                missing_features.append(col)\n",
    "                X_cols.append(pd.Series(0, index=self.data.index))\n",
    "        \n",
    "        if missing_features:\n",
    "            print(f\"    Warning: {len(missing_features)} features missing, filled with 0\")\n",
    "        \n",
    "        X = pd.DataFrame(dict(zip(self.feature_cols, X_cols)), index=self.data.index)\n",
    "        \n",
    "        # Clean data\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(X.median(numeric_only=True)).fillna(0)\n",
    "        X = X.astype(np.float32, copy=False)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _find_column(self, candidates: List[str]) -> Optional[str]:\n",
    "        \"\"\"Find first existing column from candidates\"\"\"\n",
    "        for col in candidates:\n",
    "            if col in self.data.columns:\n",
    "                return col\n",
    "        return None\n",
    "    \n",
    "    def _find_growth_column(self, candidates: List[str]) -> Optional[str]:\n",
    "        \"\"\"Find growth column, trying exact match first, then partial\"\"\"\n",
    "        # Try exact matches first\n",
    "        for col in candidates:\n",
    "            if col in self.data.columns:\n",
    "                return col\n",
    "        \n",
    "        # Try partial matches\n",
    "        for candidate in candidates:\n",
    "            for col in self.data.columns:\n",
    "                if candidate.replace('growth_', '') in col and 'growth' in col:\n",
    "                    return col\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Integration function\n",
    "def generate_complete_predictions(prediction_data, model, feature_cols):\n",
    "    \"\"\"\n",
    "    Main function to generate complete prediction set\n",
    "    \"\"\"\n",
    "    print(\"Generating complete prediction set...\")\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = CompletePredictionGenerator(prediction_data, model, feature_cols)\n",
    "    \n",
    "    # Generate all predictions\n",
    "    enhanced_data = generator.generate_all_predictions()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Total strategies generated: {len(generator.prediction_cols)}\")\n",
    "    \n",
    "    return enhanced_data, generator.prediction_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: GENERATE COMPLETE PREDICTIONS (ML + MANUAL + ENSEMBLE)  \n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: GENERATE COMPLETE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we have the date range defined\n",
    "if 'week_start' not in locals():\n",
    "    latest_date = tm.df_full['Date'].max()\n",
    "    week_start = latest_date - pd.Timedelta(days=6)  # Last 7 days\n",
    "    print(f\"Defining date range: {week_start.date()} to {latest_date.date()}\")\n",
    "\n",
    "try:\n",
    "    # Generate complete prediction set matching simulation analysis\n",
    "    enhanced_data, all_prediction_cols = generate_complete_predictions(\n",
    "        prediction_data=tm.df_full[tm.df_full['Date'] >= week_start],\n",
    "        model=model,\n",
    "        feature_cols=feature_cols\n",
    "    )\n",
    "    \n",
    "    # Update prediction data to use enhanced version\n",
    "    latest_date = enhanced_data['Date'].max()\n",
    "    latest_slice = enhanced_data[enhanced_data['Date'] == latest_date].copy()\n",
    "    \n",
    "    print(f\"Enhanced prediction data ready: {len(latest_slice)} stocks\")\n",
    "    print(f\"Available strategies: {len(all_prediction_cols)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Enhanced prediction generation failed: {e}\")\n",
    "    print(\"Using basic ML predictions only\")\n",
    "    \n",
    "    # Fallback - use existing data\n",
    "    enhanced_data = tm.df_full\n",
    "    latest_date = enhanced_data['Date'].max()\n",
    "    latest_slice = enhanced_data[enhanced_data['Date'] == latest_date].copy()\n",
    "    all_prediction_cols = ['rf_prob_30d']  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = [col for col in all_prediction_cols if col.startswith(\"pred\")]\n",
    "pred_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_choose =  ['Date',\n",
    " 'Open',\n",
    " 'High',\n",
    " 'Low',\n",
    " 'Close',\n",
    " 'adj_close',\n",
    " 'Volume',\n",
    " 'Ticker',\n",
    " 'growth_future_30d',\n",
    " 'is_positive_growth_30d_future',\n",
    "  'rf_prob_30d'] + pred_cols\n",
    "\n",
    "\n",
    "new_df = enhanced_data[cols_to_choose]\n",
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa24d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1) Define predictor buckets --------------------------------------------\n",
    "rf_thresh_cols = [\n",
    "    'pred10_rf_thresh_21','pred11_rf_thresh_50','pred12_rf_thresh_65',\n",
    "    'pred13_rf_thresh_80','pred14_rf_thresh_90',\n",
    "    'pred15_rf_auto_rate_1p','pred16_rf_auto_rate_3p','pred17_rf_auto_rate_5p'\n",
    "]\n",
    "\n",
    "topN_cols = ['pred31_top3_daily','pred31_top5_daily','pred33_top10_daily']\n",
    "\n",
    "manual_cols = [\n",
    "    'pred0_manual_cci','pred1_manual_prev_g1','pred2_manual_prev_g1_and_snp',\n",
    "    'pred3_manual_declining_rates','pred4_manual_fed_easing',\n",
    "    'pred5_manual_vix_contrarian','pred6_manual_stock_btc_momentum'\n",
    "]\n",
    "\n",
    "ensemble_cols = ['pred20_ens_ml50_and_momentum','pred21_ens_auto1p_or_top3','pred22_ens_manual2plus_and_auto3p']\n",
    "\n",
    "# Choose which sets to include in each ‚Äúvote family‚Äù\n",
    "vote_sets = {\n",
    "    # everything\n",
    "    'all_pred': rf_thresh_cols + topN_cols + manual_cols + ensemble_cols,\n",
    "    # model-ish (rf thresholds + topN + ensemble)\n",
    "    'modelish': rf_thresh_cols + topN_cols + ensemble_cols,\n",
    "    # models only (no manual and no ensemble built from manual) ‚Äì keep it pure if you want\n",
    "    'models_only': rf_thresh_cols + topN_cols,\n",
    "    # manual only\n",
    "    'manual_only': manual_cols,\n",
    "    # blended: rf med+high thresholds + ensemble + a couple robust manual rules\n",
    "    'blended': ['pred11_rf_thresh_50','pred12_rf_thresh_65','pred13_rf_thresh_80'] + ensemble_cols + [\n",
    "        'pred0_manual_cci','pred2_manual_prev_g1_and_snp'\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- 2) Utility: clean & clip to {0,1} --------------------------------------\n",
    "def _as_binary(df, cols):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    out = df[cols].copy()\n",
    "    # Try to coerce to numeric, then clip\n",
    "    for c in cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "    return out.fillna(0).clip(lower=0, upper=1)\n",
    "\n",
    "# --- 3) Core voting function -------------------------------------------------\n",
    "def build_voting_signals(\n",
    "    df: pd.DataFrame,\n",
    "    vote_sets: dict,\n",
    "    vote_threshold: float = 0.5,\n",
    "    min_votes: int = 3,\n",
    "    tie_breaker_col: str = 'rf_prob_30d',   # optional soft tiebreaker\n",
    "    tie_breaker_cut: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    For each key in vote_sets:\n",
    "      - computes vote_count, possible_count, vote_share\n",
    "      - produces signal_{name} ‚àà {0,1}\n",
    "      - produces strength_{name} ‚àà {'WEAK','MODERATE','STRONG'}\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for name, cols in vote_sets.items():\n",
    "        bin_df = _as_binary(out, cols)\n",
    "        vote_count = bin_df.sum(axis=1)\n",
    "        possible_count = bin_df.shape[1] if bin_df.shape[1] > 0 else 1\n",
    "        vote_share = (vote_count / possible_count)\n",
    "\n",
    "        # base decision\n",
    "        signal = (vote_share >= vote_threshold) & (vote_count >= min_votes)\n",
    "\n",
    "        # optional tiebreaker around the threshold edge\n",
    "        if tie_breaker_col in out.columns:\n",
    "            near_edge = (vote_share.between(vote_threshold - 0.05, vote_threshold + 0.05))\n",
    "            # if near edge, nudge using the probability column\n",
    "            signal = np.where(\n",
    "                near_edge & (out[tie_breaker_col] > tie_breaker_cut),\n",
    "                1,\n",
    "                signal.astype(int)\n",
    "            )\n",
    "\n",
    "        # strength buckets from vote_share (tweak as you like)\n",
    "        strength = pd.cut(\n",
    "            vote_share,\n",
    "            bins=[-np.inf, 0.33, 0.6, 1.01],  # looser\n",
    "            labels=['WEAK','MODERATE','STRONG']\n",
    "        )\n",
    "\n",
    "        out[f'vote_count_{name}'] = vote_count.astype(int)\n",
    "        out[f'vote_possible_{name}'] = possible_count\n",
    "        out[f'vote_share_{name}'] = vote_share\n",
    "        out[f'signal_{name}'] = signal.astype(int)\n",
    "        out[f'strength_{name}'] = strength.astype(str)\n",
    "\n",
    "    return out\n",
    "\n",
    "# # --- 4) Run it ----------------------------------------------------------------\n",
    "# voted_df = build_voting_signals(\n",
    "#     new_df,\n",
    "#     vote_sets=vote_sets,\n",
    "#     vote_threshold=0.55,   # a bit stricter than 0.5\n",
    "#     min_votes=3,           # avoid single-flag triggers\n",
    "#     tie_breaker_col='rf_prob_30d',\n",
    "#     tie_breaker_cut=0.55\n",
    "# )\n",
    "\n",
    "# voted_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: define your top-3 strategies\n",
    "top3_cols = ['pred15_rf_auto_rate_3p','pred15_rf_auto_rate_5p']\n",
    "vote_sets_top3 = {\"top3\": top3_cols}\n",
    "\n",
    "\n",
    "# Make sure Date column is datetime\n",
    "new_df[\"Date\"] = pd.to_datetime(new_df[\"Date\"])\n",
    "\n",
    "# Find the most recent date in your dataset\n",
    "latest_date = new_df[\"Date\"].max()\n",
    "\n",
    "# Filter to that day only (‚âà your daily ‚Äústock universe‚Äù snapshot)\n",
    "latest_df = new_df.loc[new_df[\"Date\"] == latest_date].copy()\n",
    "\n",
    "print(\"Latest date:\", latest_date.date(),\n",
    "      \"| rows:\", len(latest_df),\n",
    "      \"| unique tickers:\", latest_df[\"Ticker\"].nunique())\n",
    "\n",
    "\n",
    "# Step 2: Build signals on the latest snapshot\n",
    "latest_voted_top3 = build_voting_signals(\n",
    "    latest_df,             # latest date df from earlier\n",
    "    vote_sets=vote_sets_top3,\n",
    "    vote_threshold=0.67,   # require 2 out of 3\n",
    "    min_votes=2,           # at least 2 predictors must agree\n",
    "    tie_breaker_col='rf_prob_30d',\n",
    "    tie_breaker_cut=0.55\n",
    ")\n",
    "\n",
    "# Step 3: Rank tickers by signal & conviction\n",
    "ranked_top3 = (\n",
    "    latest_voted_top3\n",
    "      .assign(\n",
    "          _sig = latest_voted_top3[\"signal_top3\"].values,\n",
    "          _strength_ord = latest_voted_top3[\"strength_top3\"].map({\"STRONG\":2,\"MODERATE\":1,\"WEAK\":0}).fillna(0).values,\n",
    "          _share = latest_voted_top3[\"vote_share_top3\"].values,\n",
    "          _prob = latest_voted_top3[\"rf_prob_30d\"].values\n",
    "      )\n",
    "      .sort_values(by=[\"_sig\",\"_strength_ord\",\"_share\",\"_prob\"], ascending=[False,False,False,False])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Step 4: Take top N picks (say 20)\n",
    "TOP_N = 20\n",
    "top_picks= ranked_top3.head(TOP_N)\n",
    "\n",
    "# Final table\n",
    "top_picks[[\"Date\",\"Ticker\",\"Close\",\"rf_prob_30d\",\n",
    "                \"vote_count_top3\",\"vote_share_top3\",\n",
    "                \"signal_top3\",\"strength_top3\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_latest_predictions_simple(latest_df: pd.DataFrame, vote_family: str, out_dir: str = \"pred_logs\"):\n",
    "    \"\"\"\n",
    "    Saves a minimal snapshot (Date, Ticker, Close, probability, signal) for the latest day.\n",
    "    Assumes latest_df already contains signal columns for the chosen vote_family.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fam = vote_family\n",
    "    cols = [c for c in [\n",
    "        \"Date\",\"Ticker\",\"Close\",\"rf_prob_30d\",\n",
    "        f\"signal_{fam}\", f\"vote_share_{fam}\", f\"strength_{fam}\"\n",
    "    ] if c in latest_df.columns]\n",
    "\n",
    "    snap = latest_df[cols].copy().reset_index(drop=True)\n",
    "\n",
    "    asof = pd.to_datetime(snap[\"Date\"].max()).strftime(\"%Y-%m-%d\")\n",
    "    csv_path = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.csv\")\n",
    "    pq_path  = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.parquet\")\n",
    "\n",
    "    snap.to_csv(csv_path, index=False)\n",
    "    try:\n",
    "        snap.to_parquet(pq_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"Parquet save skipped:\", e)\n",
    "\n",
    "    print(f\"Saved:\\n  {csv_path}\\n  {pq_path if os.path.exists(pq_path) else '(parquet skipped)'}\")\n",
    "    return snap\n",
    "\n",
    "# Example:\n",
    "# save_latest_predictions_simple(latest_voted_top3, vote_family=\"top3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def save_latest_predictions_simple_rank(\n",
    "    df: pd.DataFrame,\n",
    "    vote_sets: dict,\n",
    "    vote_family: str,\n",
    "    vote_threshold: float = 0.6,\n",
    "    min_votes: int = 2,\n",
    "    tie_breaker_col: str = \"rf_prob_30d\",\n",
    "    tie_breaker_cut: float = 0.55,\n",
    "    out_dir: str = \"../new_pred_logs\"\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # --- 1) get latest date ---\n",
    "    tmp = df.copy()\n",
    "    if \"Date\" not in tmp.columns:\n",
    "        raise KeyError(\"Input df must contain a 'Date' column.\")\n",
    "\n",
    "    tmp[\"Date\"] = pd.to_datetime(tmp[\"Date\"], errors=\"coerce\")\n",
    "    latest_date = tmp[\"Date\"].max()\n",
    "    if pd.isna(latest_date):\n",
    "        print(\"No valid dates found in df; returning empty result.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    latest_df = tmp.loc[tmp[\"Date\"] == latest_date].copy()\n",
    "    if latest_df.empty:\n",
    "        print(f\"No rows found for latest date {latest_date}; returning empty result.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- 2) build signals ---\n",
    "    voted = build_voting_signals(\n",
    "        latest_df,\n",
    "        vote_sets=vote_sets,\n",
    "        vote_threshold=vote_threshold,\n",
    "        min_votes=min_votes,\n",
    "        tie_breaker_col=tie_breaker_col,\n",
    "        tie_breaker_cut=tie_breaker_cut\n",
    "    )\n",
    "\n",
    "    fam = vote_family\n",
    "    fam_cols = [f\"signal_{fam}\", f\"vote_share_{fam}\", f\"strength_{fam}\"]\n",
    "    missing = [c for c in fam_cols if c not in voted.columns]\n",
    "\n",
    "    if missing:\n",
    "        # help the user see what *is* available\n",
    "        families_available = sorted({\n",
    "            col.split(\"_\", 1)[1]\n",
    "            for col in voted.columns\n",
    "            if col.startswith((\"signal_\", \"vote_share_\", \"strength_\")) and \"_\" in col\n",
    "        })\n",
    "        raise KeyError(\n",
    "            f\"Missing expected columns for family '{fam}': {missing}. \"\n",
    "            f\"Available families detected: {families_available}. \"\n",
    "            f\"Make sure vote_family matches the key in vote_sets and what build_voting_signals outputs.\"\n",
    "        )\n",
    "\n",
    "    # --- columns we want to carry forward ---\n",
    "    base_needed = [\"Date\", \"Ticker\", \"Close\", \"rf_prob_30d\"]\n",
    "    needed = base_needed + fam_cols\n",
    "    available = [c for c in needed if c in voted.columns]\n",
    "    snap = voted[available].copy().reset_index(drop=True)\n",
    "\n",
    "    # --- 3) map strength to numeric order for sorting ---\n",
    "    # normalize case just in case (\"weak\", \"Weak\", etc.)\n",
    "    s_col = f\"strength_{fam}\"\n",
    "    if s_col in snap.columns:\n",
    "        snap[s_col] = snap[s_col].astype(str).str.upper()\n",
    "    strength_map = {\"STRONG\": 2, \"MODERATE\": 1, \"WEAK\": 0}\n",
    "    snap[\"_strength_ord\"] = snap.get(s_col, pd.Series(index=snap.index)).map(strength_map).fillna(0).astype(int)\n",
    "\n",
    "    # --- 4) sort & rank ---\n",
    "    sort_cols = [\"_strength_ord\"]\n",
    "    ascending = [False]\n",
    "\n",
    "    if \"rf_prob_30d\" in snap.columns:\n",
    "        sort_cols.append(\"rf_prob_30d\")\n",
    "        ascending.append(False)\n",
    "    else:\n",
    "        print(\"Warning: 'rf_prob_30d' not found; sorting only by strength.\")\n",
    "\n",
    "    snap_sorted = snap.sort_values(by=sort_cols, ascending=ascending).reset_index(drop=True)\n",
    "    snap_sorted[\"rank_today\"] = np.arange(1, len(snap_sorted) + 1)\n",
    "\n",
    "    # --- 5) reorder nicely (keep rank first; move any missing aside gracefully) ---\n",
    "    ordered_cols = [\"rank_today\",\"Date\",\"Ticker\",\"Close\",\"rf_prob_30d\",\n",
    "                    f\"signal_{fam}\", f\"vote_share_{fam}\", f\"strength_{fam}\"]\n",
    "    snap_sorted = snap_sorted[[c for c in ordered_cols if c in snap_sorted.columns]]\n",
    "\n",
    "    # --- 6) save ---\n",
    "    asof = pd.to_datetime(latest_date).strftime(\"%Y-%m-%d\")\n",
    "    csv_path = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.csv\")\n",
    "    pq_path  = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.parquet\")\n",
    "\n",
    "    snap_sorted.to_csv(csv_path, index=False)\n",
    "    try:\n",
    "        snap_sorted.to_parquet(pq_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"Parquet save skipped:\", e)\n",
    "\n",
    "    print(f\"Saved predictions to:\\n  {csv_path}\\n  {pq_path if os.path.exists(pq_path) else '(parquet skipped)'}\")\n",
    "    return snap_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_sets_top3 = {\"top2\": ['pred16_rf_auto_rate_3p','pred17_rf_auto_rate_5p']}\n",
    "\n",
    "today_ranked = save_latest_predictions_simple_rank(\n",
    "    new_df,\n",
    "    vote_sets=vote_sets_top3,\n",
    "    vote_family=\"top2\",\n",
    "    vote_threshold=0.67,\n",
    "    min_votes=2,\n",
    "    out_dir=\"../new_pred_logs\"\n",
    ")\n",
    "\n",
    "today_ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_ranked.rf_prob_30d.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6cc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713db07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd858cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: GENERATE COMPLETE PREDICTIONS (ML + MANUAL + ENSEMBLE)  - full data\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: GENERATE COMPLETE PREDICTIONS - Full Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we have the date range defined\n",
    "if 'week_start' not in locals():\n",
    "    latest_date = tm.df_full['Date'].max()\n",
    "    week_start = latest_date - pd.Timedelta(days=6)  # Last 7 days\n",
    "    print(f\"Defining date range: {week_start.date()} to {latest_date.date()}\")\n",
    "\n",
    "try:\n",
    "    # Generate complete prediction set matching simulation analysis\n",
    "    enhanced_data, all_prediction_cols = generate_complete_predictions(\n",
    "        prediction_data=tm.df_full.copy(),\n",
    "        model=model,\n",
    "        feature_cols=feature_cols\n",
    "    )\n",
    "    \n",
    "    # Update prediction data to use enhanced version\n",
    "    latest_date = enhanced_data['Date'].max()\n",
    "    latest_slice = enhanced_data[enhanced_data['Date'] == latest_date].copy()\n",
    "    \n",
    "    print(f\"Enhanced prediction data ready: {len(latest_slice)} stocks\")\n",
    "    print(f\"Available strategies: {len(all_prediction_cols)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Enhanced prediction generation failed: {e}\")\n",
    "    print(\"Using basic ML predictions only\")\n",
    "    \n",
    "    # Fallback - use existing data\n",
    "    enhanced_data = tm.df_full\n",
    "    latest_date = enhanced_data['Date'].max()\n",
    "    latest_slice = enhanced_data[enhanced_data['Date'] == latest_date].copy()\n",
    "    all_prediction_cols = ['rf_prob_30d']  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data[\"rf_prob_30d\"].hist(bins=50, alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8335627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data.query('split == \"test\"').rf_prob_30d.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18d40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Get the latest date in your prediction data\n",
    "latest_date = enhanced_data[\"Date\"].max()\n",
    "\n",
    "# Define start of the latest week\n",
    "time_start = latest_date - pd.Timedelta(days=0)\n",
    "\n",
    "# Filter to only the latest week\n",
    "latest_data = enhanced_data.query(\"Date >= @time_start\")\n",
    "\n",
    "# Plot histogram\n",
    "latest_data[\"rf_prob_30d\"].hist(bins=50, alpha=0.7)\n",
    "plt.xlabel(\"rf_prob_30d\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Distribution of rf_prob_30d (last week: {time_start.date()} ‚Üí {latest_date.date()})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb691605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f59be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e6e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080ca41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa703d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = enhanced_data[\"Date\"].max()\n",
    "prev_date = enhanced_data.loc[enhanced_data[\"Date\"] < last_date, \"Date\"].max()\n",
    "\n",
    "print(\"Last date:\", last_date)\n",
    "print(\"Previous date:\", prev_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa5750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_last = enhanced_data.loc[enhanced_data[\"Date\"] == last_date, feature_cols]\n",
    "X_prev = enhanced_data.loc[enhanced_data[\"Date\"] == prev_date, feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaNs on last date:\\n\", X_last.isna().sum().sort_values(ascending=False).head(20))\n",
    "print(\"\\nColumns with only one unique value (constant):\\n\", X_last.nunique().loc[lambda x: x <= 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = enhanced_data.loc[enhanced_data[\"split\"]==\"train\", feature_cols]\n",
    "\n",
    "train_stats = train.describe().T[[\"mean\",\"std\"]]\n",
    "last_stats  = X_last.describe().T[[\"mean\",\"std\"]]\n",
    "\n",
    "compare = train_stats.join(last_stats, lsuffix=\"_train\", rsuffix=\"_last\")\n",
    "print(compare.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ea6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dup_ratio = 1 - np.unique(pd.util.hash_pandas_object(X_last.fillna(-999))).shape[0] / X_last.shape[0]\n",
    "print(f\"Share of duplicate rows on last date: {dup_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21008203",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_day = (\n",
    "    enhanced_data.groupby(\"Date\")[\"rf_prob_30d\"]\n",
    "    .agg(mean=\"mean\", std=\"std\", p10=lambda s: s.quantile(0.1), p90=lambda s: s.quantile(0.9))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(by_day.tail(10))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(by_day[\"Date\"], by_day[\"std\"], marker=\"o\")\n",
    "plt.title(\"Std of rf_prob_30d per day\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = enhanced_data[\"Date\"].max()\n",
    "prev_date = enhanced_data.loc[enhanced_data[\"Date\"] < last_date, \"Date\"].max()\n",
    "\n",
    "X_last = enhanced_data.loc[enhanced_data[\"Date\"] == last_date, feature_cols]\n",
    "X_prev = enhanced_data.loc[enhanced_data[\"Date\"] == prev_date, feature_cols]\n",
    "\n",
    "# Compare variance\n",
    "var_last = X_last.var(numeric_only=True)\n",
    "var_prev = X_prev.var(numeric_only=True)\n",
    "\n",
    "collapsed = var_last[var_last < 1e-8].index.tolist()\n",
    "print(\"Collapsed features on last date:\", collapsed[:20], \"‚Ä¶ total:\", len(collapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_date = pd.to_datetime(\"2025-09-09\")\n",
    "\n",
    "X_check = enhanced_data.loc[enhanced_data[\"Date\"] == check_date, feature_cols]\n",
    "print(f\"Shape for {check_date.date()}:\", X_check.shape)\n",
    "\n",
    "\n",
    "collapsed = X_check.nunique().loc[lambda x: x <= 1]\n",
    "print(f\"Collapsed features on {check_date.date()} (first 20 shown):\")\n",
    "print(collapsed.head(20))\n",
    "print(f\"Total collapsed features: {len(collapsed)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cols  = [c for c in X_check.columns if \"btc\" in c]\n",
    "vix_cols  = [c for c in X_check.columns if \"vix\" in c]\n",
    "dax_cols  = [c for c in X_check.columns if \"dax\" in c]\n",
    "\n",
    "print(\"BTC growth features on\", check_date.date(), \"\\n\", X_check[btc_cols].head())\n",
    "print(\"VIX growth features on\", check_date.date(), \"\\n\", X_check[vix_cols].head())\n",
    "print(\"DAX growth features on\", check_date.date(), \"\\n\", X_check[dax_cols].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
