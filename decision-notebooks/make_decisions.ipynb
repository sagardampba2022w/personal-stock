{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ef7bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Stock Prediction Decision Making Notebook\n",
      "==================================================\n",
      "✅ Paths configured:\n",
      "   Project root: /Users/sagardhal/Desktop/Practice/personal-stock\n",
      "   App code: /Users/sagardhal/Desktop/Practice/personal-stock/app\n",
      "   Artifacts: /Users/sagardhal/Desktop/Practice/personal-stock/artifacts\n",
      "   Data: /Users/sagardhal/Desktop/Practice/personal-stock/data\n"
     ]
    }
   ],
   "source": [
    "# Stock Prediction Decision Making Notebook\n",
    "# Interactive notebook for making daily trading decisions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "TOP_N_PICKS = 10\n",
    "INVESTMENT_AMOUNT = 1000\n",
    "\n",
    "# Path configuration - adjust for notebook location\n",
    "# Since notebook is in eda-notebooks/, go up one level to project root\n",
    "PROJECT_ROOT = Path(\"..\").resolve()  # Go up one level from eda-notebooks/\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "APP_DIR = PROJECT_ROOT / \"app\"\n",
    "\n",
    "print(\"📊 Stock Prediction Decision Making Notebook\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup paths\n",
    "if str(APP_DIR) not in sys.path:\n",
    "    sys.path.append(str(APP_DIR))\n",
    "\n",
    "print(f\"✅ Paths configured:\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   App code: {APP_DIR}\")\n",
    "print(f\"   Artifacts: {ARTIFACTS_DIR}\")\n",
    "print(f\"   Data: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08407a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# go up one level to the root directory\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "\n",
    "\n",
    "# # add project root (parent of notebooks/) to sys.path\n",
    "# project_root = Path.cwd().parent\n",
    "# sys.path.insert(0, str(project_root))\n",
    "\n",
    "from app.predictions import (\n",
    "    load_latest_data,\n",
    "    load_model_and_features,\n",
    "    PredictionComparator,\n",
    "    _TransformAdapter,\n",
    ")\n",
    "\n",
    "# If you also need TrainModel directly in the notebook:\n",
    "from app.train_model_new import TrainModel   # ✅ absolute package import\n",
    "\n",
    "from app.stock_pipeline import StockDataPipeline\n",
    "\n",
    "\n",
    "# Import modules\n",
    "try:\n",
    "    from app.predictions import load_model_and_features, _TransformAdapter\n",
    "    from app.train_model_new import TrainModel\n",
    "    from app.stock_pipeline import StockDataPipeline\n",
    "    print(\"✅ All modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Make sure your app directory contains all required modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49c429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87c1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 1: LOAD TICKERS\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load Tickers\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 1: LOAD TICKERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "tickers = (pd.read_csv(\"/Users/sagardhal/Desktop/Practice/personal-stock/ticker/spx_ndx_liq_top250_latest.csv\")['Ticker']\n",
    "        #.head(5)\n",
    "        .tolist())\n",
    "tickers\n",
    "len(tickers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3379b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STEP 2: CHECK REQUIREMENTS\n",
      "==================================================\n",
      "🤖 Models found: 5\n",
      "   - best_rf_model.joblib\n",
      "   - random_forest_train_valid_20250904_112953.joblib\n",
      "   - random_forest_train_only_20250904_000839.joblib\n",
      "   - random_forest_train_only_20250904_112906.joblib\n",
      "   - random_forest_train_valid_20250904_000856.joblib\n",
      "📊 Data files found: 1\n",
      "   - stock_data_combined_20250904_071145.parquet\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check Requirements\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STEP 2: CHECK REQUIREMENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for trained models\n",
    "artifacts_files = list(ARTIFACTS_DIR.glob(\"*.joblib\")) + list(ARTIFACTS_DIR.glob(\"*.pkl\"))\n",
    "print(f\"🤖 Models found: {len(artifacts_files)}\")\n",
    "for f in artifacts_files:\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "# Check for data files\n",
    "data_files = list(DATA_DIR.glob(\"*.parquet\")) if DATA_DIR.exists() else []\n",
    "print(f\"📊 Data files found: {len(data_files)}\")\n",
    "for f in data_files[:3]:\n",
    "    print(f\"   - {f.name}\")\n",
    "if len(data_files) > 3:\n",
    "    print(f\"   ... and {len(data_files) - 3} more files\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4af18aa3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def fetch_fresh_ohlcv_fast(tickers, days_back=30):\n",
    "    \"\"\"\n",
    "    Most efficient approach: one batched yfinance call for all tickers,\n",
    "    then reshape to long format. Prints per-ticker status after download.\n",
    "    \"\"\"\n",
    "    # Clean + de-dupe\n",
    "    tickers = [t.strip().upper() for t in tickers if t and str(t).strip()]\n",
    "    tickers = list(dict.fromkeys(tickers))\n",
    "    if not tickers:\n",
    "        raise ValueError(\"No valid tickers provided\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2 (FAST): FETCH FRESH DATA — Single batched call\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📊 Fetching last {days_back} days of data for {len(tickers)} tickers...\")\n",
    "    # Small pad for weekends/holidays; using period is often more robust than start/end\n",
    "    period_days = days_back + 2\n",
    "    print(f\"   Period: {period_days} days\")\n",
    "\n",
    "    # ---- One batched download ----\n",
    "    df = yf.download(\n",
    "        \" \".join(tickers),\n",
    "        period=f\"{period_days}d\",\n",
    "        interval=\"1d\",\n",
    "        auto_adjust=True,\n",
    "        actions=False,          # no dividends/splits to keep it lean\n",
    "        group_by=\"column\",      # flat column groups (field, ticker) MultiIndex\n",
    "        threads=True,\n",
    "        progress=False,\n",
    "        #show_errors=True,\n",
    "    )\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        raise ValueError(\"No data returned by yfinance for the requested tickers/period\")\n",
    "\n",
    "    # Normalize rows\n",
    "    df = df.dropna(how=\"all\")\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"All rows are NaN after cleaning\")\n",
    "\n",
    "    # ---- Reshape to long (Ticker, Date, OHLCV) ----\n",
    "    # yfinance with group_by=\"column\" gives a MultiIndex on columns:\n",
    "    # one level = field names (Open, High, Low, Close, Adj Close, Volume)\n",
    "    # the other level = tickers. We'll detect which is which and reshape.\n",
    "    fields = {\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"}\n",
    "    if not isinstance(df.columns, pd.MultiIndex):\n",
    "        # Single-ticker case: plain columns -> make it look like multi for consistency\n",
    "        df.columns = pd.MultiIndex.from_product([df.columns, [tickers[0]]])\n",
    "\n",
    "    lvl0 = set(map(str, df.columns.get_level_values(0)))\n",
    "    lvl1 = set(map(str, df.columns.get_level_values(1)))\n",
    "\n",
    "    # Determine which level is fields\n",
    "    if fields & lvl0:\n",
    "        field_level, ticker_level = 0, 1\n",
    "    elif fields & lvl1:\n",
    "        field_level, ticker_level = 1, 0\n",
    "        df.columns = df.columns.swaplevel(0, 1)  # put (field, ticker) order\n",
    "    else:\n",
    "        # Fallback: assume first level is field\n",
    "        field_level, ticker_level = 0, 1\n",
    "\n",
    "    # After ensuring (field, ticker), stack tickers to rows\n",
    "    df_long = (\n",
    "        df.stack(level=1)  # stack ticker level to rows\n",
    "          .reset_index()\n",
    "          .rename(columns={\"level_1\": \"Ticker\"})\n",
    "    )\n",
    "\n",
    "    # Clean column names\n",
    "    df_long.columns = [str(c).replace(\" \", \"_\") for c in df_long.columns]\n",
    "    # Ensure standard set exists (some tickers may miss columns on illiquid days)\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj_Close\", \"Volume\"]:\n",
    "        if col not in df_long.columns:\n",
    "            df_long[col] = pd.NA\n",
    "\n",
    "    # ---- Per-ticker status summary ----\n",
    "    successful_tickers = []\n",
    "    empty_tickers = []\n",
    "    for t in tickers:\n",
    "        n = len(df_long[df_long[\"Ticker\"] == t])\n",
    "        if n > 0:\n",
    "            successful_tickers.append(t)\n",
    "            print(f\"✅ {t}: {n} rows\")\n",
    "        else:\n",
    "            empty_tickers.append(t)\n",
    "            print(f\"⚠️ {t}: no rows\")\n",
    "\n",
    "    if not successful_tickers:\n",
    "        raise ValueError(\"No data fetched for any ticker\")\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n✅ Fetched data for {len(successful_tickers)} tickers \"\n",
    "          f\"(empty: {len(empty_tickers)})\")\n",
    "    print(f\"   Total observations: {len(df_long)}\")\n",
    "    try:\n",
    "        min_d = df_long['Date'].min()\n",
    "        max_d = df_long['Date'].max()\n",
    "        print(f\"   Date range in data: {min_d} to {max_d}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df_long, successful_tickers\n",
    "\n",
    "# ---- Example usage ----\n",
    "# raw_data, successful_tickers = fetch_fresh_ohlcv_fast(tickers, days_back=30)\n",
    "# print(f\"\\n📊 Raw data shape: {raw_data.shape}\")\n",
    "# latest_date = raw_data['Date'].max()\n",
    "# latest_sample = raw_data[raw_data['Date'] == latest_date].head(3)\n",
    "# print(f\"\\n📅 Latest data sample ({latest_date}):\")\n",
    "# print(latest_sample[['Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']].to_string(index=False))\n",
    "\n",
    "raw_data, successful_tickers = fetch_fresh_ohlcv_fast(tickers, days_back=1)\n",
    "\n",
    "latest_data = raw_data[['Date', 'Ticker', 'Close', 'High', 'Low', 'Open', 'Volume',\n",
    "       'Adj_Close']]\n",
    "\n",
    "latest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301fba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: TRANSFORM DATA USING STOCKDATAPIPELINE\n",
      "============================================================\n",
      "🔄 Starting StockDataPipeline transformation...\n",
      "🚀 Running StockDataPipeline for complete feature engineering...\n",
      "   Configuration:\n",
      "   - Lookbacks: [1, 3, 7, 30, 90, 252, 365]\n",
      "   - Horizons: [30] days\n",
      "   - Binary threshold: 100%\n",
      "   📊 Running complete pipeline...\n",
      "      - Step 1: Fetching stock data\n",
      "      - Step 2: Adding technical indicators (TA-Lib)\n",
      "      - Step 3: Adding macro indicators\n",
      "      - Step 4: Final validation and cleanup\n",
      "STOCK MARKET DATA PIPELINE\n",
      "======================================================================\n",
      "Processing 250 tickers: NVDA, TSLA, AAPL, PLTR, MSFT, AMD, AMZN, META, GOOGL, UNH, AVGO, GOOG, MSTR, LLY, COIN, NFLX, INTC, APP, ORCL, BRK-B, JPM, COST, V, MU, PANW, CRM, SMCI, GEV, WMT, NOW, BAC, XOM, UBER, BA, MA, INTU, MRVL, JNJ, IBM, CSCO, CRWD, TXN, HD, C, CVX, AMAT, CAT, GS, QCOM, ADBE, PEP, TMO, LRCX, ANET, UNP, GE, ASML, SHOP, WFC, ACN, BKNG, PG, ISRG, MCD, PFE, TMUS, PDD, KO, MRK, PM, SBUX, TTD, ABBV, ADI, DIS, CMG, APH, T, KLAC, VST, DASH, VRTX, ETN, CSX, MELI, COF, PYPL, WDAY, FI, UPS, HON, NKE, DHR, NSC, LMT, SNPS, SCHW, DDOG, CMCSA, CEG, AXP, TGT, PGR, FTNT, LIN, ELV, VZ, DELL, ABT, LOW, RCL, TJX, WBD, GILD, CHTR, SHW, BSX, NEE, LULU, DE, NEM, MS, F, ABNB, ARM, SPGI, MDT, MCHP, RTX, XYZ, COP, EBAY, BX, CDNS, DHI, MCK, FCX, FSLR, AMGN, JCI, NXPI, REGN, TT, CME, WDC, CI, BMY, KEY, AXON, ICE, BLK, PH, CVS, CB, STX, MO, CCL, ON, IBKR, AMT, URI, EA, FICO, MDLZ, AON, MMM, TDG, HCA, TPR, KDP, MPWR, SYK, HBAN, MMC, UAL, AJG, EMR, HWM, ADSK, CNC, TER, MSI, ZTS, KR, ZS, TEAM, BDX, ORLY, ADP, SLB, APO, KKR, HUM, SO, PWR, NRG, ROST, AZO, HLT, MSCI, RF, IDXX, GM, NOC, FDX, OXY, KMI, EQT, CL, DAL, DOW, ULTA, WELL, GLW, DLTR, WM, TTWO, TRI, IT, USB, TEL, DUK, CARR, WMB, AEP, AZN, DECK, MNST, PCG, EXPE, EL, ROP, COR, CAH, LEN, OTIS, HPE, EW, VLO, PLD, ALB, MAR, KHC, KVUE, CTAS, CCI, NCLH, MCO, LHX, EQIX\n",
      "Lookback periods: [1, 3, 7, 30, 90, 252, 365]\n",
      "Prediction horizons: [30]\n",
      "\n",
      "Step 1: Fetching stock data and basic features...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  250 of 250 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔄 Starting StockDataPipeline transformation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     full_data, prediction_data = \u001b[43mrun_stock_pipeline_for_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ StockDataPipeline transformation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Ready for model predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prediction_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m stocks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrun_stock_pipeline_for_predictions\u001b[39m\u001b[34m(tickers)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m      - Step 3: Adding macro indicators\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m      - Step 4: Final validation and cleanup\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m transformed_data = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ✅ StockDataPipeline complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      Final shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformed_data.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Practice/personal-stock/app/stock_pipeline.py:281\u001b[39m, in \u001b[36mStockDataPipeline.run_complete_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    277\u001b[39m start_time = datetime.now()\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# Run all pipeline steps\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_1_fetch_stock_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m.step_2_add_technical_indicators()\n\u001b[32m    283\u001b[39m     \u001b[38;5;28mself\u001b[39m.step_3_add_macro_indicators()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Practice/personal-stock/app/stock_pipeline.py:87\u001b[39m, in \u001b[36mStockDataPipeline.step_1_fetch_stock_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# This would call your stock_data.build_stock_dataframe function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28mself\u001b[39m.raw_stock_df = \u001b[43mbuild_stock_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlookbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlookbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbinarize_thresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbinarize_thresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mma_windows\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mma_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvol_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvol_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrisk_free\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrisk_free\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Normalize column names to handle case sensitivity\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.raw_stock_df.columns = (\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mself\u001b[39m.raw_stock_df.columns\n\u001b[32m    100\u001b[39m     .str.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    101\u001b[39m     .str.lower()\n\u001b[32m    102\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Practice/personal-stock/app/extract.py:199\u001b[39m, in \u001b[36mbuild_stock_dataframe\u001b[39m\u001b[34m(tickers, lookbacks, horizons, binarize_thresholds, ma_windows, vol_window, risk_free, pause)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_stock_dataframe\u001b[39m(\n\u001b[32m    186\u001b[39m     tickers: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    187\u001b[39m     lookbacks: List[\u001b[38;5;28mint\u001b[39m] = [\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m30\u001b[39m, \u001b[32m90\u001b[39m, \u001b[32m252\u001b[39m, \u001b[32m365\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     pause: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1.0\u001b[39m\n\u001b[32m    194\u001b[39m ) -> pd.DataFrame:\n\u001b[32m    195\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m    Fetches history for `tickers`, applies all feature pipelines,\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[33;03m    and returns a cleaned, sorted DataFrame.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     df_all = \u001b[43mfetch_history_bulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpause\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     df_all = (\n\u001b[32m    202\u001b[39m         df_all\n\u001b[32m    203\u001b[39m         .pipe(add_time_features)\n\u001b[32m   (...)\u001b[39m\u001b[32m    209\u001b[39m \n\u001b[32m    210\u001b[39m     )\n\u001b[32m    212\u001b[39m     \u001b[38;5;66;03m# ADD MISSING FEATURES FOR TRAINING COMPATIBILITY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Practice/personal-stock/app/extract.py:42\u001b[39m, in \u001b[36mfetch_history_bulk\u001b[39m\u001b[34m(tickers, pause)\u001b[39m\n\u001b[32m     39\u001b[39m     df.loc[:, \u001b[33m\"\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m\"\u001b[39m] = ticker\n\u001b[32m     40\u001b[39m     frames.append(df)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to be gentle on the API\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.concat(frames, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: TRANSFORM DATA USING YOUR STOCKDATAPIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: TRANSFORM DATA USING STOCKDATAPIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_stock_pipeline_for_predictions(tickers):\n",
    "    \"\"\"Run your StockDataPipeline to get fully transformed data\"\"\"\n",
    "    print(\"🚀 Running StockDataPipeline for complete feature engineering...\")\n",
    "    \n",
    "    # Configuration (same as your run_data_extraction.py)\n",
    "    config = {\n",
    "        \"LOOKBACKS\": [1, 3, 7, 30, 90, 252, 365],\n",
    "        \"HORIZONS\": [30],\n",
    "        \"BINARY_THRESHOLDS\": {30: 1.00},  # 0% gain threshold\n",
    "    }\n",
    "    \n",
    "    print(f\"   Configuration:\")\n",
    "    print(f\"   - Lookbacks: {config['LOOKBACKS']}\")\n",
    "    print(f\"   - Horizons: {config['HORIZONS']} days\")  \n",
    "    print(f\"   - Binary threshold: {config['BINARY_THRESHOLDS'][30]:.0%}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize StockDataPipeline (same as extraction script)\n",
    "        pipeline = StockDataPipeline(\n",
    "            tickers=tickers,\n",
    "            lookbacks=config[\"LOOKBACKS\"],\n",
    "            horizons=config[\"HORIZONS\"],\n",
    "            binarize_thresholds=config[\"BINARY_THRESHOLDS\"],\n",
    "        )\n",
    "        \n",
    "        # Run complete pipeline: stock data + technical indicators + macro data\n",
    "        print(\"   📊 Running complete pipeline...\")\n",
    "        print(\"      - Step 1: Fetching stock data\")\n",
    "        print(\"      - Step 2: Adding technical indicators (TA-Lib)\")  \n",
    "        print(\"      - Step 3: Adding macro indicators\")\n",
    "        print(\"      - Step 4: Final validation and cleanup\")\n",
    "        \n",
    "        transformed_data = pipeline.run_complete_pipeline()\n",
    "        \n",
    "        print(f\"   ✅ StockDataPipeline complete!\")\n",
    "        print(f\"      Final shape: {transformed_data.shape}\")\n",
    "        \n",
    "        # Get latest data for predictions\n",
    "        latest_date = transformed_data['Date'].max()\n",
    "        prediction_data = transformed_data[transformed_data['Date'] == latest_date].copy()\n",
    "        \n",
    "        print(f\"   📅 Prediction data ready:\")\n",
    "        print(f\"      Latest date: {latest_date.date()}\")\n",
    "        print(f\"      Stocks: {len(prediction_data)}\")\n",
    "        \n",
    "        # Show feature categories\n",
    "        feature_categories = {\n",
    "            \"Basic OHLCV\": [c for c in transformed_data.columns if c in [\"Date\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]],\n",
    "            \"Growth Features\": [c for c in transformed_data.columns if c.startswith(\"growth_\") and \"future\" not in c],\n",
    "            \"Technical Indicators\": [c for c in transformed_data.columns if any(x in c.lower() for x in [\"rsi\", \"macd\", \"sma\", \"adx\", \"cci\"])],\n",
    "            \"Candlestick Patterns\": [c for c in transformed_data.columns if c.startswith(\"cdl\")],\n",
    "            \"Macro Features\": [c for c in transformed_data.columns if c.endswith((\"_yoy\", \"_qoq\")) or \"btc\" in c.lower() or \"vix\" in c.lower()],\n",
    "            \"Target Variables\": [c for c in transformed_data.columns if \"future\" in c and (\"positive\" in c or \"growth\" in c)],\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   📈 Features created:\")\n",
    "        total_features = 0\n",
    "        for category, features in feature_categories.items():\n",
    "            print(f\"      {category}: {len(features)}\")\n",
    "            total_features += len(features)\n",
    "        print(f\"      Total: {total_features} features\")\n",
    "        \n",
    "        return transformed_data, prediction_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ StockDataPipeline failed: {e}\")\n",
    "        print(f\"   This could be due to:\")\n",
    "        print(f\"   - API rate limits from yfinance/FRED\")\n",
    "        print(f\"   - Missing TA-Lib dependencies\") \n",
    "        print(f\"   - Network connectivity issues\")\n",
    "        print(f\"   - Insufficient historical data\")\n",
    "        raise\n",
    "\n",
    "# Run StockDataPipeline\n",
    "try:\n",
    "    print(\"🔄 Starting StockDataPipeline transformation...\")\n",
    "    full_data, prediction_data = run_stock_pipeline_for_predictions(tickers)\n",
    "    \n",
    "    print(\"✅ StockDataPipeline transformation complete!\")\n",
    "    print(f\"   Ready for model predictions: {len(prediction_data)} stocks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ StockDataPipeline failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check internet connection\")\n",
    "    print(\"2. Verify TA-Lib is installed: pip install TA-Lib\")  \n",
    "    print(\"3. Try with fewer tickers (reduce TOP_N_PICKS)\")\n",
    "    print(\"4. Check yfinance/FRED API limits\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f386739b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# STEP 4: PREPARE FOR MODEL INFERENCE USING TRAINMODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: PREPARE FOR MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def prepare_for_model_inference(pipeline_data,window_days=7):\n",
    "    \"\"\"Use TrainModel to prepare StockDataPipeline output for inference\"\"\"\n",
    "    print(\"🤖 Using TrainModel for inference preparation...\")\n",
    "    \n",
    "    try:\n",
    "        # Create adapter and TrainModel (consistent with predictions.py)\n",
    "        adapter = _TransformAdapter(pipeline_data)\n",
    "        tm = TrainModel(adapter)\n",
    "        \n",
    "        # Prepare for inference (creates dummy variables, etc.)\n",
    "        tm.prepare_dataframe(start_date=\"2000-01-01\")\n",
    "        \n",
    "        print(f\"   ✅ TrainModel preparation complete\")\n",
    "        print(f\"      Shape: {tm.df_full.shape}\")\n",
    "        \n",
    "        # Get latest data for predictions\n",
    "        latest_date = tm.df_full['Date'].max()\n",
    "        inference_data = tm.df_full[tm.df_full['Date'] == latest_date].copy()\n",
    "        \n",
    "        print(f\"   📊 Inference ready: {len(inference_data)} stocks\")\n",
    "        \n",
    "        return tm, inference_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ TrainModel preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Prepare data for inference\n",
    "    tm, inference_data = prepare_for_model_inference(full_data)\n",
    "    \n",
    "    # Load trained model\n",
    "    print(\"\\n📂 Loading trained model...\")\n",
    "    model, feature_cols, target_col = load_model_and_features(str(ARTIFACTS_DIR))\n",
    "    \n",
    "    print(f\"✅ Model loaded:\")\n",
    "    print(f\"   Type: {type(model).__name__}\")\n",
    "    print(f\"   Features expected: {len(feature_cols)}\")\n",
    "    print(f\"   Target: {target_col}\")\n",
    "    \n",
    "    # Set up TrainModel for inference (same as predictions.py)\n",
    "    tm.model = model\n",
    "    tm._inference_feature_columns = feature_cols\n",
    "    if target_col:\n",
    "        tm.target_col = target_col\n",
    "    \n",
    "    print(\"✅ Model setup complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Model setup failed: {e}\")\n",
    "    print(\"Make sure you have a trained model in the artifacts directory\")\n",
    "    print(\"Run: python run_model_training.py --mode basic\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddb575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e80a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: PREPARE FOR MODEL INFERENCE\n",
      "============================================================\n",
      "🤖 Using TrainModel for inference preparation...\n",
      "Preparing dataframe for modeling...\n",
      "Defining feature sets...\n",
      "Feature Set Summary:\n",
      "  Growth features: 70\n",
      "  Technical indicators: 56\n",
      "  Technical patterns: 61\n",
      "  Custom numerical: 7\n",
      "  Macro features: 75\n",
      "  Categorical (for dummies): 7\n",
      "  Target columns: 2\n",
      "  Total numerical features: 206\n",
      "  Unused columns: 0\n",
      "Creating dummy variables...\n",
      "Created 397 dummy variables\n",
      "Sample dummies: ['month_1', 'month_10', 'month_11', 'month_12', 'month_2']\n",
      "Filtered data from 2000-01-01\n",
      "Date range: 2000-01-03 00:00:00 to 2025-09-12 00:00:00\n",
      "Temporal split created:\n",
      "  train: 909,539 samples\n",
      "  validation: 232,029 samples\n",
      "  test: 238,866 samples\n",
      "Creating ML datasets...\n",
      "Total features before filtering: 603\n",
      "  - Numerical: 206\n",
      "  - Dummies: 397\n",
      "Features after removing 'future': 603\n",
      "Selected target: is_positive_growth_30d_future\n",
      "ML Dataset Summary:\n",
      "  Features used: 603\n",
      "  Train: 909,539 samples\n",
      "  Validation: 232,029 samples\n",
      "  Test: 238,866 samples\n",
      "  Train+Valid: 1,141,568 samples\n",
      "  Target: is_positive_growth_30d_future\n",
      "  Target distribution (train): {1: 532731, 0: 376808}\n",
      "   ✅ TrainModel preparation complete\n",
      "      Shape: (1380434, 620)\n",
      "   📊 Inference window: 2025-09-06 → 2025-09-12  |  Rows: 1250\n",
      "\n",
      "📂 Loading trained model...\n",
      "[load_model_and_features] Using model file: best_rf_model.joblib\n",
      "[load_model_and_features] feature_names_in_: 603 features\n",
      "✅ Model loaded:\n",
      "   Type: RandomForestClassifier\n",
      "   Features expected: 603\n",
      "   Target: None\n",
      "✅ Model setup complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: PREPARE FOR MODEL INFERENCE USING TRAINMODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: PREPARE FOR MODEL INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd  # ensure available for Timedelta/to_datetime\n",
    "\n",
    "def prepare_for_model_inference(pipeline_data, window_days=7):\n",
    "    \"\"\"Use TrainModel to prepare StockDataPipeline output for inference.\"\"\"\n",
    "    print(\"🤖 Using TrainModel for inference preparation...\")\n",
    "    try:\n",
    "        # Create adapter and TrainModel (consistent with predictions.py)\n",
    "        adapter = _TransformAdapter(pipeline_data)\n",
    "        tm = TrainModel(adapter)\n",
    "\n",
    "        # Prepare for inference (creates dummy variables, etc.)\n",
    "        tm.prepare_dataframe(start_date=\"2000-01-01\")\n",
    "\n",
    "        print(f\"   ✅ TrainModel preparation complete\")\n",
    "        print(f\"      Shape: {tm.df_full.shape}\")\n",
    "\n",
    "        # ---- Slice to last `window_days` calendar days (with hardening) ----\n",
    "        # Safety: ensure Date is datetime\n",
    "        tm.df_full['Date'] = pd.to_datetime(tm.df_full['Date'])\n",
    "\n",
    "        latest_date = tm.df_full['Date'].max()\n",
    "        week_start = latest_date - pd.Timedelta(days=window_days - 1)\n",
    "\n",
    "        inference_data = tm.df_full[\n",
    "            (tm.df_full['Date'] >= week_start) &\n",
    "            (tm.df_full['Date'] <= latest_date)\n",
    "        ].copy()\n",
    "\n",
    "        if inference_data.empty:\n",
    "            raise ValueError(\n",
    "                f\"No inference rows between {week_start.date()} and {latest_date.date()}. \"\n",
    "                \"Check upstream dates/timezones or reduce window_days.\"\n",
    "            )\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        print(f\"   📊 Inference window: {week_start.date()} → {latest_date.date()}  |  Rows: {len(inference_data)}\")\n",
    "\n",
    "        return tm, inference_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ TrainModel preparation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Prepare data for inference (last 7 calendar days)\n",
    "    tm, inference_data = prepare_for_model_inference(full_data, window_days=7)\n",
    "\n",
    "    # Load trained model\n",
    "    print(\"\\n📂 Loading trained model...\")\n",
    "    model, feature_cols, target_col = load_model_and_features(str(ARTIFACTS_DIR))\n",
    "\n",
    "    print(f\"✅ Model loaded:\")\n",
    "    print(f\"   Type: {type(model).__name__}\")\n",
    "    print(f\"   Features expected: {len(feature_cols)}\")\n",
    "    print(f\"   Target: {target_col}\")\n",
    "\n",
    "    # Set up TrainModel for inference (same as predictions.py)\n",
    "    tm.model = model\n",
    "    tm._inference_feature_columns = feature_cols\n",
    "    if target_col:\n",
    "        tm.target_col = target_col\n",
    "\n",
    "    print(\"✅ Model setup complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model setup or prediction failed: {e}\")\n",
    "    print(\"Make sure you have a trained model in the artifacts directory\")\n",
    "    print(\"Run: python run_model_training.py --mode basic\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301771cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug info:\n",
      "   Data shape: (1380434, 620)\n",
      "   Available features (sample): ['growth_1d', 'growth_3d', 'growth_7d', 'growth_30d', 'growth_90d', 'growth_252d', 'growth_365d', 'growth_btc_1d', 'growth_btc_3d', 'growth_btc_7d', 'growth_btc_30d', 'growth_btc_90d', 'growth_btc_252d', 'growth_btc_365d', 'growth_vix_1d', 'growth_vix_3d', 'growth_vix_7d', 'growth_vix_30d', 'growth_vix_90d', 'growth_vix_252d', 'growth_vix_365d', 'growth_dax_1d', 'growth_dax_3d', 'growth_dax_7d', 'growth_dax_30d', 'growth_dax_90d', 'growth_dax_252d', 'growth_dax_365d', 'growth_snp500_1d', 'growth_snp500_3d', 'growth_snp500_7d', 'growth_snp500_30d', 'growth_snp500_90d', 'growth_snp500_252d', 'growth_snp500_365d', 'growth_dji_1d', 'growth_dji_3d', 'growth_dji_7d', 'growth_dji_30d', 'growth_dji_90d', 'growth_dji_252d', 'growth_dji_365d', 'growth_epi_1d', 'growth_epi_3d', 'growth_epi_7d', 'growth_epi_30d', 'growth_epi_90d', 'growth_epi_252d', 'growth_epi_365d', 'growth_gold_1d', 'growth_gold_3d', 'growth_gold_7d', 'growth_gold_30d', 'growth_gold_90d', 'growth_gold_252d', 'growth_gold_365d', 'growth_brent_oil_1d', 'growth_brent_oil_3d', 'growth_brent_oil_7d', 'growth_brent_oil_30d', 'growth_brent_oil_90d', 'growth_brent_oil_252d', 'growth_brent_oil_365d', 'growth_crude_oil_1d', 'growth_crude_oil_3d', 'growth_crude_oil_7d', 'growth_crude_oil_30d', 'growth_crude_oil_90d', 'growth_crude_oil_252d', 'growth_crude_oil_365d', 'adx', 'adxr', 'apo', 'bop', 'cci', 'cmo', 'dx', 'mfi', 'mom', 'ppo', 'roc', 'rocp', 'rocr', 'rocr100', 'rsi', 'trix', 'ultosc', 'willr', 'macd', 'macd_signal', 'macd_hist', 'macd_ext', 'macd_signal_ext', 'macd_hist_ext', 'macd_fix', 'macd_signal_fix', 'macd_hist_fix', 'aroon_up', 'aroon_down', 'aroonosc', 'stoch_slowk', 'stoch_slowd', 'stoch_fastk', 'stoch_fastd', 'stochrsi_fastk', 'stochrsi_fastd', 'ad', 'adosc', 'obv', 'atr', 'natr', 'trange', 'plus_di', 'minus_di', 'plus_dm', 'avgprice', 'medprice', 'typprice', 'wclprice', 'ht_dcperiod', 'ht_dcphase', 'ht_phasor_inphase', 'ht_phasor_quadrature', 'ht_sine_sine', 'ht_sine_leadsine', 'ht_trendmode', 'cdl2crows', 'cdl3blackcrows', 'cdl3inside', 'cdl3linestrike', 'cdl3outside', 'cdl3starsinsouth', 'cdl3whitesoldiers', 'cdlabandonedbaby', 'cdladvanceblock', 'cdlbelthold', 'cdlbreakaway', 'cdlclosingmarubozu', 'cdlconcealbabyswall', 'cdlcounterattack', 'cdldarkcloudcover', 'cdldoji', 'cdldojistar', 'cdldragonflydoji', 'cdlengulfing', 'cdleveningdojistar', 'cdleveningstar', 'cdlgapsidesidewhite', 'cdlgravestonedoji', 'cdlhammer', 'cdlhangingman', 'cdlharami', 'cdlharamicross', 'cdlhighwave', 'cdlhikkake', 'cdlhikkakemod', 'cdlhomingpigeon', 'cdlidentical3crows', 'cdlinneck', 'cdlinvertedhammer', 'cdlkicking', 'cdlkickingbylength', 'cdlladderbottom', 'cdllongleggeddoji', 'cdllongline', 'cdlmarubozu', 'cdlmatchinglow', 'cdlmathold', 'cdlmorningdojistar', 'cdlmorningstar', 'cdlonneck', 'cdlpiercing', 'cdlrickshawman', 'cdlrisefall3methods', 'cdlseparatinglines', 'cdlshootingstar', 'cdlshortline', 'cdlspinningtop', 'cdlstalledpattern', 'cdlsticksandwich', 'cdltakuri', 'cdltasukigap', 'cdlthrusting', 'cdltristar', 'cdlunique3river', 'cdlupsidegap2crows', 'cdlxsidegap3methods', 'sma10', 'sma20', 'growing_moving_average', 'volatility', 'sharpe', 'high_minus_low_relative', 'ln_volume', 'gdppot_yoy', 'gdppot_qoq', 'cpilfesl_yoy', 'cpilfesl_qoq', 'fedfunds_yoy', 'fedfunds_qoq', 'dgs1_yoy', 'dgs1_qoq', 'dgs5_yoy', 'dgs5_qoq', 'dgs10_yoy', 'dgs10_qoq', 'month_1', 'month_10', 'month_11', 'month_12', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'Ticker_AAPL', 'Ticker_ABBV', 'Ticker_ABNB', 'Ticker_ABT', 'Ticker_ACN', 'Ticker_ADBE', 'Ticker_ADI', 'Ticker_ADP', 'Ticker_ADSK', 'Ticker_AEP', 'Ticker_AJG', 'Ticker_ALB', 'Ticker_AMAT', 'Ticker_AMD', 'Ticker_AMGN', 'Ticker_AMT', 'Ticker_AMZN', 'Ticker_ANET', 'Ticker_AON', 'Ticker_APH', 'Ticker_APO', 'Ticker_APP', 'Ticker_ARM', 'Ticker_ASML', 'Ticker_AVGO', 'Ticker_AXON', 'Ticker_AXP', 'Ticker_AZN', 'Ticker_AZO', 'Ticker_BA', 'Ticker_BAC', 'Ticker_BDX', 'Ticker_BKNG', 'Ticker_BLK', 'Ticker_BMY', 'Ticker_BRK-B', 'Ticker_BSX', 'Ticker_BX', 'Ticker_C', 'Ticker_CAH', 'Ticker_CARR', 'Ticker_CAT', 'Ticker_CB', 'Ticker_CCI', 'Ticker_CCL', 'Ticker_CDNS', 'Ticker_CEG', 'Ticker_CHTR', 'Ticker_CI', 'Ticker_CL', 'Ticker_CMCSA', 'Ticker_CME', 'Ticker_CMG', 'Ticker_CNC', 'Ticker_COF', 'Ticker_COIN', 'Ticker_COP', 'Ticker_COR', 'Ticker_COST', 'Ticker_CRM', 'Ticker_CRWD', 'Ticker_CSCO', 'Ticker_CSX', 'Ticker_CTAS', 'Ticker_CVS', 'Ticker_CVX', 'Ticker_DAL', 'Ticker_DASH', 'Ticker_DDOG', 'Ticker_DE', 'Ticker_DECK', 'Ticker_DELL', 'Ticker_DHI', 'Ticker_DHR', 'Ticker_DIS', 'Ticker_DLTR', 'Ticker_DOW', 'Ticker_DUK', 'Ticker_EA', 'Ticker_EBAY', 'Ticker_EL', 'Ticker_ELV', 'Ticker_EMR', 'Ticker_EQIX', 'Ticker_EQT', 'Ticker_ETN', 'Ticker_EW', 'Ticker_EXPE', 'Ticker_F', 'Ticker_FCX', 'Ticker_FDX', 'Ticker_FI', 'Ticker_FICO', 'Ticker_FSLR', 'Ticker_FTNT', 'Ticker_GE', 'Ticker_GEV', 'Ticker_GILD', 'Ticker_GLW', 'Ticker_GM', 'Ticker_GOOG', 'Ticker_GOOGL', 'Ticker_GS', 'Ticker_HBAN', 'Ticker_HCA', 'Ticker_HD', 'Ticker_HLT', 'Ticker_HON', 'Ticker_HPE', 'Ticker_HUM', 'Ticker_HWM', 'Ticker_IBKR', 'Ticker_IBM', 'Ticker_ICE', 'Ticker_IDXX', 'Ticker_INTC', 'Ticker_INTU', 'Ticker_ISRG', 'Ticker_IT', 'Ticker_JCI', 'Ticker_JNJ', 'Ticker_JPM', 'Ticker_KDP', 'Ticker_KEY', 'Ticker_KHC', 'Ticker_KKR', 'Ticker_KLAC', 'Ticker_KMI', 'Ticker_KO', 'Ticker_KR', 'Ticker_KVUE', 'Ticker_LEN', 'Ticker_LHX', 'Ticker_LIN', 'Ticker_LLY', 'Ticker_LMT', 'Ticker_LOW', 'Ticker_LRCX', 'Ticker_LULU', 'Ticker_MA', 'Ticker_MAR', 'Ticker_MCD', 'Ticker_MCHP', 'Ticker_MCK', 'Ticker_MCO', 'Ticker_MDLZ', 'Ticker_MDT', 'Ticker_MELI', 'Ticker_META', 'Ticker_MMC', 'Ticker_MMM', 'Ticker_MNST', 'Ticker_MO', 'Ticker_MPWR', 'Ticker_MRK', 'Ticker_MRVL', 'Ticker_MS', 'Ticker_MSCI', 'Ticker_MSFT', 'Ticker_MSI', 'Ticker_MSTR', 'Ticker_MU', 'Ticker_NCLH', 'Ticker_NEE', 'Ticker_NEM', 'Ticker_NFLX', 'Ticker_NKE', 'Ticker_NOC', 'Ticker_NOW', 'Ticker_NRG', 'Ticker_NSC', 'Ticker_NVDA', 'Ticker_NXPI', 'Ticker_ON', 'Ticker_ORCL', 'Ticker_ORLY', 'Ticker_OTIS', 'Ticker_OXY', 'Ticker_PANW', 'Ticker_PCG', 'Ticker_PDD', 'Ticker_PEP', 'Ticker_PFE', 'Ticker_PG', 'Ticker_PGR', 'Ticker_PH', 'Ticker_PLD', 'Ticker_PLTR', 'Ticker_PM', 'Ticker_PWR', 'Ticker_PYPL', 'Ticker_QCOM', 'Ticker_RCL', 'Ticker_REGN', 'Ticker_RF', 'Ticker_ROP', 'Ticker_ROST', 'Ticker_RTX', 'Ticker_SBUX', 'Ticker_SCHW', 'Ticker_SHOP', 'Ticker_SHW', 'Ticker_SLB', 'Ticker_SMCI', 'Ticker_SNPS', 'Ticker_SO', 'Ticker_SPGI', 'Ticker_STX', 'Ticker_SYK', 'Ticker_T', 'Ticker_TDG', 'Ticker_TEAM', 'Ticker_TEL', 'Ticker_TER', 'Ticker_TGT', 'Ticker_TJX', 'Ticker_TMO', 'Ticker_TMUS', 'Ticker_TPR', 'Ticker_TRI', 'Ticker_TSLA', 'Ticker_TT', 'Ticker_TTD', 'Ticker_TTWO', 'Ticker_TXN', 'Ticker_UAL', 'Ticker_UBER', 'Ticker_ULTA', 'Ticker_UNH', 'Ticker_UNP', 'Ticker_UPS', 'Ticker_URI', 'Ticker_USB', 'Ticker_V', 'Ticker_VLO', 'Ticker_VRTX', 'Ticker_VST', 'Ticker_VZ', 'Ticker_WBD', 'Ticker_WDAY', 'Ticker_WDC', 'Ticker_WELL', 'Ticker_WFC', 'Ticker_WM', 'Ticker_WMB', 'Ticker_WMT', 'Ticker_XOM', 'Ticker_XYZ', 'Ticker_ZS', 'Ticker_ZTS', 'ticker_type_US', 'year_1962', 'year_1963', 'year_1964', 'year_1965', 'year_1966', 'year_1967', 'year_1968', 'year_1969', 'year_1970', 'year_1971', 'year_1972', 'year_1973', 'year_1974', 'year_1975', 'year_1976', 'year_1977', 'year_1978', 'year_1979', 'year_1980', 'year_1981', 'year_1982', 'year_1983', 'year_1984', 'year_1985', 'year_1986', 'year_1987', 'year_1988', 'year_1989', 'year_1990', 'year_1991', 'year_1992', 'year_1993', 'year_1994', 'year_1995', 'year_1996', 'year_1997', 'year_1998', 'year_1999', 'year_2000', 'year_2001', 'year_2002', 'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007', 'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016', 'year_2017', 'year_2018', 'year_2019', 'year_2020', 'year_2021', 'year_2022', 'year_2023', 'year_2024', 'year_2025', 'wom_1', 'wom_2', 'wom_3', 'wom_4', 'wom_5', 'month_wom_April_w1', 'month_wom_April_w2', 'month_wom_April_w3', 'month_wom_April_w4', 'month_wom_April_w5', 'month_wom_August_w1', 'month_wom_August_w2', 'month_wom_August_w3', 'month_wom_August_w4', 'month_wom_August_w5', 'month_wom_December_w1', 'month_wom_December_w2', 'month_wom_December_w3', 'month_wom_December_w4', 'month_wom_December_w5', 'month_wom_February_w1', 'month_wom_February_w2', 'month_wom_February_w3', 'month_wom_February_w4', 'month_wom_February_w5', 'month_wom_January_w1', 'month_wom_January_w2', 'month_wom_January_w3', 'month_wom_January_w4', 'month_wom_January_w5', 'month_wom_July_w1', 'month_wom_July_w2', 'month_wom_July_w3', 'month_wom_July_w4', 'month_wom_July_w5', 'month_wom_June_w1', 'month_wom_June_w2', 'month_wom_June_w3', 'month_wom_June_w4', 'month_wom_June_w5', 'month_wom_March_w1', 'month_wom_March_w2', 'month_wom_March_w3', 'month_wom_March_w4', 'month_wom_March_w5', 'month_wom_May_w1', 'month_wom_May_w2', 'month_wom_May_w3', 'month_wom_May_w4', 'month_wom_May_w5', 'month_wom_November_w1', 'month_wom_November_w2', 'month_wom_November_w3', 'month_wom_November_w4', 'month_wom_November_w5', 'month_wom_October_w1', 'month_wom_October_w2', 'month_wom_October_w3', 'month_wom_October_w4', 'month_wom_October_w5', 'month_wom_September_w1', 'month_wom_September_w2', 'month_wom_September_w3', 'month_wom_September_w4', 'month_wom_September_w5']\n",
      "   Missing features (sample): []\n"
     ]
    }
   ],
   "source": [
    "print(f\"Debug info:\")\n",
    "print(f\"   Data shape: {tm.df_full.shape if hasattr(tm, 'df_full') else 'N/A'}\")\n",
    "if 'feature_cols' in locals() and hasattr(tm, 'df_full'):\n",
    "    available = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "    missing = [f for f in feature_cols if f not in tm.df_full.columns]\n",
    "    print(f\"   Available features (sample): {available}\")\n",
    "    print(f\"   Missing features (sample): {missing}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9463fa95",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# STEP 5: GENERATE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: GENERATE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Generate predictions using TrainModel.make_inference (same as predictions.py)\n",
    "    print(\"🎯 Generating predictions using TrainModel.make_inference()...\")\n",
    "    \n",
    "    prediction_results = tm.make_inference(\"realtime_probs\")\n",
    "    \n",
    "    print(f\"✅ Predictions generated!\")\n",
    "    \n",
    "    # Get prediction data with probabilities\n",
    "    latest_date = tm.df_full['Date'].max()\n",
    "    prediction_data = tm.df_full[tm.df_full['Date'] == latest_date].copy()\n",
    "    \n",
    "    if 'realtime_probs' in prediction_data.columns:\n",
    "        probabilities = prediction_data['realtime_probs'].values\n",
    "        \n",
    "        print(f\"   📊 Prediction summary:\")\n",
    "        print(f\"      Probability range: {probabilities.min():.3f} to {probabilities.max():.3f}\")\n",
    "        print(f\"      Mean probability: {probabilities.mean():.3f}\")\n",
    "        print(f\"      Std deviation: {probabilities.std():.3f}\")\n",
    "        \n",
    "        # Add additional columns\n",
    "        prediction_data['probability'] = probabilities\n",
    "        prediction_data['prediction'] = (probabilities >= 0.5).astype(int)\n",
    "        prediction_data['rank'] = prediction_data['probability'].rank(ascending=False)\n",
    "        prediction_data['percentile'] = prediction_data['probability'].rank(pct=True)\n",
    "        \n",
    "        positive_preds = prediction_data['prediction'].sum()\n",
    "        print(f\"      Positive predictions (>50%): {positive_preds}/{len(prediction_data)}\")\n",
    "        \n",
    "        # Feature alignment info\n",
    "        available_features = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "        print(f\"      Feature alignment: {len(available_features)}/{len(feature_cols)} ({len(available_features)/len(feature_cols):.1%})\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Prediction column 'realtime_probs' not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Prediction generation failed: {e}\")\n",
    "    \n",
    "    # Debug info\n",
    "    if 'tm' in locals():\n",
    "        print(f\"Debug info:\")\n",
    "        print(f\"   Data shape: {tm.df_full.shape if hasattr(tm, 'df_full') else 'N/A'}\")\n",
    "        if 'feature_cols' in locals() and hasattr(tm, 'df_full'):\n",
    "            available = [f for f in feature_cols[:10] if f in tm.df_full.columns]\n",
    "            missing = [f for f in feature_cols[:10] if f not in tm.df_full.columns]\n",
    "            print(f\"   Available features (sample): {available}\")\n",
    "            print(f\"   Missing features (sample): {missing}\")\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f68d50",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# STEP 5: GENERATE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: GENERATE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Generate predictions using TrainModel.make_inference (same as predictions.py)\n",
    "    print(\"🎯 Generating predictions using TrainModel.make_inference()...\")\n",
    "    prediction_results = tm.make_inference(\"realtime_probs\")\n",
    "    print(f\"✅ Predictions generated!\")\n",
    "\n",
    "    # Use the same 7-day inference window prepared in STEP 4\n",
    "    # (tm.df_full already contains the column added by make_inference)\n",
    "    if 'realtime_probs' not in tm.df_full.columns:\n",
    "        raise ValueError(\"Prediction column 'realtime_probs' not found after make_inference().\")\n",
    "\n",
    "    # Limit to the window you prepared earlier\n",
    "    latest_date = inference_data['Date'].max()\n",
    "    week_start = inference_data['Date'].min()\n",
    "    prediction_data = tm.df_full[\n",
    "        (tm.df_full['Date'] >= week_start) &\n",
    "        (tm.df_full['Date'] <= latest_date)\n",
    "    ].copy()\n",
    "\n",
    "    # Keep only rows that have probabilities\n",
    "    prediction_data = prediction_data.loc[prediction_data['realtime_probs'].notna()].copy()\n",
    "\n",
    "    if prediction_data.empty:\n",
    "        raise ValueError(\n",
    "            f\"No rows with 'realtime_probs' between {week_start.date()} and {latest_date.date()}.\"\n",
    "        )\n",
    "\n",
    "    # Add derived columns\n",
    "    probs = prediction_data['realtime_probs'].astype(float)\n",
    "    prediction_data['probability'] = probs\n",
    "    prediction_data['prediction'] = (probs >= 0.5).astype(int)\n",
    "\n",
    "    # Rank within each Date (highest prob = rank 1)\n",
    "    prediction_data['rank'] = prediction_data.groupby('Date')['probability'].rank(ascending=False, method='first')\n",
    "    prediction_data['percentile'] = prediction_data.groupby('Date')['probability'].rank(pct=True)\n",
    "\n",
    "    # --- Summary prints ---\n",
    "    print(\"   📊 Window summary:\")\n",
    "    print(f\"      Dates: {week_start.date()} → {latest_date.date()} \"\n",
    "          f\"({prediction_data['Date'].nunique()} trading day(s))\")\n",
    "    print(f\"      Rows with probs: {len(prediction_data)}\")\n",
    "    print(f\"      Probability range: {probs.min():.3f} to {probs.max():.3f}\")\n",
    "    print(f\"      Mean probability: {probs.mean():.3f}\")\n",
    "    print(f\"      Std deviation: {probs.std():.3f}\")\n",
    "\n",
    "    # Latest-day quick view (preserves your old behavior)\n",
    "    latest_slice = prediction_data[prediction_data['Date'] == latest_date].copy()\n",
    "    positive_preds = latest_slice['prediction'].sum()\n",
    "    print(f\"   📅 Latest day: {latest_date.date()} | rows: {len(latest_slice)}\")\n",
    "    print(f\"      Positive predictions (>50%): {positive_preds}/{len(latest_slice)}\")\n",
    "\n",
    "    # Feature alignment info (same as before)\n",
    "    available_features = [f for f in feature_cols if f in tm.df_full.columns]\n",
    "    print(f\"      Feature alignment: {len(available_features)}/{len(feature_cols)} \"\n",
    "          f\"({len(available_features)/len(feature_cols):.1%})\")\n",
    "\n",
    "    # If you need these dataframes elsewhere, they are:\n",
    "    # - prediction_data: full 7-day window with probs/preds/ranks\n",
    "    # - latest_slice   : latest day only\n",
    "    # (Return or save as needed in your workflow.)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Prediction generation failed: {e}\")\n",
    "    # Debug info\n",
    "    if 'tm' in locals():\n",
    "        print(\"Debug info:\")\n",
    "        print(f\"   Data shape: {tm.df_full.shape if hasattr(tm, 'df_full') else 'N/A'}\")\n",
    "        if 'feature_cols' in locals() and hasattr(tm, 'df_full'):\n",
    "            available = [f for f in feature_cols[:10] if f in tm.df_full.columns]\n",
    "            missing = [f for f in feature_cols[:10] if f not in tm.df_full.columns]\n",
    "            print(f\"   Available features (sample): {available}\")\n",
    "            print(f\"   Missing features (sample): {missing}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bfee61a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "latest_slice"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3421204",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "prediction_data.head()\n",
    "latest_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21975ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7c37ae1a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# STEP 6: CREATE TRADING SIGNALS AND DECISIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: TRADING SIGNALS & DECISIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_trading_signals(data):\n",
    "    \"\"\"Create trading signals based on prediction probabilities\"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate dynamic thresholds\n",
    "    high_conf = df['probability'].quantile(0.90)  # Top 10%\n",
    "    med_conf = df['probability'].quantile(0.75)   # Top 25%\n",
    "    low_conf = df['probability'].quantile(0.60)   # Top 40%\n",
    "    \n",
    "    print(f\"📊 Dynamic thresholds:\")\n",
    "    print(f\"   High confidence (top 10%): {high_conf:.3f}\")\n",
    "    print(f\"   Medium confidence (top 25%): {med_conf:.3f}\")\n",
    "    print(f\"   Low confidence (top 40%): {low_conf:.3f}\")\n",
    "    \n",
    "    # Create signals\n",
    "    conditions = [\n",
    "        (df['probability'] >= high_conf),\n",
    "        (df['probability'] >= med_conf),\n",
    "        (df['probability'] >= low_conf),\n",
    "        (df['probability'] >= 0.5)\n",
    "    ]\n",
    "    \n",
    "    choices = ['🟢 STRONG BUY', '🟡 BUY', '🟠 CONSIDER', '🔵 WEAK BUY']\n",
    "    df['signal'] = np.select(conditions, choices, default='🔴 PASS')\n",
    "    \n",
    "    # Investment recommendations\n",
    "    df['investment'] = 0\n",
    "    df.loc[df['signal'].str.contains('STRONG'), 'investment'] = INVESTMENT_AMOUNT * 1.5\n",
    "    df.loc[df['signal'] == '🟡 BUY', 'investment'] = INVESTMENT_AMOUNT\n",
    "    df.loc[df['signal'].str.contains('CONSIDER'), 'investment'] = INVESTMENT_AMOUNT * 0.5\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_top_picks_analysis(data, top_n=TOP_N_PICKS):\n",
    "    \"\"\"Analyze top picks and create actionable recommendations\"\"\"\n",
    "    \n",
    "    # Sort by probability\n",
    "    top_picks = data.nlargest(top_n, 'probability')\n",
    "    \n",
    "    print(f\"\\n🎯 TOP {top_n} PICKS FOR {data['Date'].iloc[0].strftime('%Y-%m-%d')}:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create display\n",
    "    display_data = []\n",
    "    for i, (_, stock) in enumerate(top_picks.iterrows(), 1):\n",
    "        \n",
    "        display_data.append({\n",
    "            'Rank': i,\n",
    "            'Ticker': stock['Ticker'],\n",
    "            'Probability': f\"{stock['probability']:.1%}\",\n",
    "            'Signal': stock['signal'],\n",
    "            'Investment': f\"${int(stock['investment']):,}\" if stock['investment'] > 0 else \"-\",\n",
    "            'Price': f\"${stock.get('Close', 0):.2f}\" if 'Close' in stock and stock.get('Close', 0) > 0 else \"N/A\"\n",
    "        })\n",
    "    \n",
    "    display_df = pd.DataFrame(display_data)\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    return top_picks\n",
    "\n",
    "def create_action_plan(top_picks):\n",
    "    \"\"\"Create executable trading action plan\"\"\"\n",
    "    \n",
    "    print(f\"\\n📋 ACTION PLAN:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Group by signal type\n",
    "    strong_buys = top_picks[top_picks['signal'].str.contains('STRONG')]\n",
    "    buys = top_picks[top_picks['signal'] == '🟡 BUY']\n",
    "    considers = top_picks[top_picks['signal'].str.contains('CONSIDER')]\n",
    "    \n",
    "    total_investment = 0\n",
    "    \n",
    "    if len(strong_buys) > 0:\n",
    "        print(f\"🟢 IMMEDIATE ACTION ({len(strong_buys)} stocks):\")\n",
    "        for _, stock in strong_buys.iterrows():\n",
    "            investment = int(stock['investment'])\n",
    "            total_investment += investment\n",
    "            print(f\"   • {stock['Ticker']:6s} - ${investment:,} (prob: {stock['probability']:.1%})\")\n",
    "    \n",
    "    if len(buys) > 0:\n",
    "        print(f\"\\n🟡 SECONDARY TARGETS ({len(buys)} stocks):\")\n",
    "        for _, stock in buys.iterrows():\n",
    "            investment = int(stock['investment'])\n",
    "            total_investment += investment\n",
    "            print(f\"   • {stock['Ticker']:6s} - ${investment:,} (prob: {stock['probability']:.1%})\")\n",
    "    \n",
    "    if len(considers) > 0:\n",
    "        print(f\"\\n🟠 WATCH LIST ({len(considers)} stocks):\")\n",
    "        for _, stock in considers.iterrows():\n",
    "            print(f\"   • {stock['Ticker']:6s} - Monitor (prob: {stock['probability']:.1%})\")\n",
    "    \n",
    "    print(f\"\\n💰 TOTAL INVESTMENT RECOMMENDED: ${total_investment:,}\")\n",
    "    \n",
    "    return {\n",
    "        'strong_buys': strong_buys,\n",
    "        'buys': buys, \n",
    "        'considers': considers,\n",
    "        'total_investment': total_investment\n",
    "    }\n",
    "\n",
    "# Generate trading signals and analysis\n",
    "try:\n",
    "    # Create signals\n",
    "    prediction_data = create_trading_signals(latest_slice)\n",
    "    \n",
    "    # Analyze top picks\n",
    "    top_picks = create_top_picks_analysis(prediction_data, TOP_N_PICKS)\n",
    "    \n",
    "    # Create action plan\n",
    "    action_plan = create_action_plan(top_picks)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📊 SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"   Analysis date: {prediction_data['Date'].iloc[0].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Stocks analyzed: {len(prediction_data)}\")\n",
    "    print(f\"   Strong buy signals: {len(action_plan['strong_buys'])}\")\n",
    "    print(f\"   Buy signals: {len(action_plan['buys'])}\")\n",
    "    print(f\"   Watch list: {len(action_plan['considers'])}\")\n",
    "    print(f\"   Total capital needed: ${action_plan['total_investment']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Signal generation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56d613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fca01a82",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# SAVE RESULTS AND FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create results directory\n",
    "    RESULTS_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Timestamp for files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions_file = RESULTS_DIR / f\"realtime_predictions_{timestamp}.csv\"\n",
    "    save_cols = ['Date', 'Ticker', 'probability', 'signal', 'investment']\n",
    "    if 'Close' in prediction_data.columns:\n",
    "        save_cols.append('Close')\n",
    "    \n",
    "    prediction_data[save_cols].to_csv(predictions_file, index=False)\n",
    "    \n",
    "    # Save action plan\n",
    "    action_file = RESULTS_DIR / f\"action_plan_{timestamp}.txt\"\n",
    "    with open(action_file, 'w') as f:\n",
    "        f.write(f\"Trading Action Plan - {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"STRONG BUY:\\n\")\n",
    "        for _, stock in action_plan['strong_buys'].iterrows():\n",
    "            f.write(f\"  {stock['Ticker']} - ${int(stock['investment']):,} ({stock['probability']:.1%})\\n\")\n",
    "        \n",
    "        f.write(\"\\nBUY:\\n\")\n",
    "        for _, stock in action_plan['buys'].iterrows():\n",
    "            f.write(f\"  {stock['Ticker']} - ${int(stock['investment']):,} ({stock['probability']:.1%})\\n\")\n",
    "        \n",
    "        f.write(f\"\\nTotal Investment: ${action_plan['total_investment']:,}\\n\")\n",
    "    \n",
    "    print(f\"💾 Results saved:\")\n",
    "    print(f\"   Predictions: {predictions_file}\")\n",
    "    print(f\"   Action plan: {action_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save results: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🚀 REAL-TIME ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'action_plan' in locals():\n",
    "    print(f\"✅ Analysis successful!\")\n",
    "    print(f\"📅 Data date: {prediction_data['Date'].iloc[0].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"🎯 Stocks analyzed: {len(prediction_data)}\")\n",
    "    print(f\"💰 Total recommendations: ${action_plan['total_investment']:,}\")\n",
    "    \n",
    "    if len(action_plan['strong_buys']) > 0:\n",
    "        best_pick = action_plan['strong_buys'].iloc[0]\n",
    "        print(f\"\\n🏆 TOP RECOMMENDATION:\")\n",
    "        print(f\"   {best_pick['Ticker']} - {best_pick['probability']:.1%} confidence\")\n",
    "        print(f\"   Investment: ${int(best_pick['investment']):,}\")\n",
    "    \n",
    "    print(f\"\\n⚠️ IMPORTANT REMINDERS:\")\n",
    "    print(\"• Set stop losses at -15% to -20%\")\n",
    "    print(\"• Don't invest more than 5-10% per position\")\n",
    "    print(\"• Monitor positions daily\")\n",
    "    print(\"• This is based on historical patterns only\")\n",
    "    \n",
    "    print(f\"\\n🔄 NEXT STEPS:\")\n",
    "    print(\"1. Execute strong buy signals\")\n",
    "    print(\"2. Set stop loss orders\")\n",
    "    print(\"3. Monitor throughout trading day\")\n",
    "    print(\"4. Re-run notebook daily for fresh signals\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Analysis incomplete - check errors above\")\n",
    "\n",
    "print(f\"\\n📊 Performance tracking:\")\n",
    "print(f\"   Results saved in: {RESULTS_DIR}\")\n",
    "print(f\"   Track actual vs predicted outcomes\")\n",
    "print(f\"   Adjust model/thresholds based on results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273132d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE PREDICTION GENERATOR CLASS DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class CompletePredictionGenerator:\n",
    "    \"\"\"\n",
    "    Generates all prediction types used in your simulation analysis:\n",
    "    - ML probabilities and thresholds\n",
    "    - Manual rule-based predictions  \n",
    "    - Ensemble combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, model, feature_cols: List[str]):\n",
    "        self.data = data.copy()\n",
    "        self.model = model\n",
    "        self.feature_cols = feature_cols\n",
    "        self.prediction_cols = []\n",
    "        \n",
    "    def generate_all_predictions(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate complete set of predictions matching your simulation analysis\"\"\"\n",
    "        print(\"Generating complete prediction set...\")\n",
    "        \n",
    "        # 1. Generate ML predictions\n",
    "        self._generate_ml_predictions()\n",
    "        \n",
    "        # 2. Generate manual rule-based predictions\n",
    "        self._generate_manual_predictions()\n",
    "        \n",
    "        # 3. Generate ensemble predictions\n",
    "        self._generate_ensemble_predictions()\n",
    "        \n",
    "        print(f\"Generated {len(self.prediction_cols)} prediction strategies\")\n",
    "        return self.data\n",
    "    \n",
    "    def _generate_ml_predictions(self):\n",
    "        \"\"\"Generate ML-based predictions with multiple thresholds\"\"\"\n",
    "        print(\"  Generating ML predictions...\")\n",
    "        \n",
    "        # Generate base probabilities\n",
    "        X = self._prepare_feature_matrix()\n",
    "        probabilities = self.model.predict_proba(X)[:, 1]\n",
    "        self.data['rf_prob_30d'] = probabilities\n",
    "        \n",
    "        # Fixed threshold strategies\n",
    "        thresholds = [0.21, 0.50, 0.65, 0.80, 0.90]\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            col_name = f'pred{10 + i}_rf_thresh_{int(threshold * 100)}'\n",
    "            self.data[col_name] = (probabilities >= threshold).astype(int)\n",
    "            self.prediction_cols.append(col_name)\n",
    "        \n",
    "        # Dynamic threshold strategies (based on validation quantiles)\n",
    "        self._generate_adaptive_thresholds(probabilities)\n",
    "        \n",
    "        # Daily top-K strategies\n",
    "        self._generate_topk_strategies(probabilities)\n",
    "    \n",
    "    def _generate_adaptive_thresholds(self, probabilities):\n",
    "        \"\"\"Generate adaptive thresholds based on quantiles\"\"\"\n",
    "        target_rates = [0.01, 0.03, 0.05]  # 1%, 3%, 5% selection rates\n",
    "        \n",
    "        for i, rate in enumerate(target_rates):\n",
    "            threshold = np.quantile(probabilities, 1.0 - rate)\n",
    "            col_name = f'pred{15 + i}_rf_auto_rate_{int(rate * 100)}p'\n",
    "            self.data[col_name] = (probabilities >= threshold).astype(int)\n",
    "            self.prediction_cols.append(col_name)\n",
    "    \n",
    "    def _generate_topk_strategies(self, probabilities):\n",
    "        \"\"\"Generate daily top-K strategies\"\"\"\n",
    "        if 'Date' in self.data.columns:\n",
    "            for k in [3, 5, 10]:\n",
    "                col_name = f'pred{30 + k//3}_top{k}_daily'\n",
    "                \n",
    "                # Group by date and select top K per date\n",
    "                top_k_mask = (\n",
    "                    self.data.groupby('Date')['rf_prob_30d']\n",
    "                    .rank(method='first', ascending=False)\n",
    "                    .le(k)\n",
    "                    .astype(int)\n",
    "                )\n",
    "                self.data[col_name] = top_k_mask\n",
    "                self.prediction_cols.append(col_name)\n",
    "    \n",
    "    def _generate_manual_predictions(self):\n",
    "        \"\"\"Generate manual rule-based predictions\"\"\"\n",
    "        print(\"  Generating manual rule-based predictions...\")\n",
    "        \n",
    "        # pred0: CCI momentum breakout\n",
    "        if 'cci' in self.data.columns:\n",
    "            self.data['pred0_manual_cci'] = (\n",
    "                pd.to_numeric(self.data['cci'], errors='coerce').fillna(0) > 200\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred0_manual_cci')\n",
    "        \n",
    "        # pred1: Previous 30d growth momentum\n",
    "        growth_30d_col = self._find_growth_column(['growth_30d'])\n",
    "        if growth_30d_col:\n",
    "            self.data['pred1_manual_prev_g1'] = (\n",
    "                pd.to_numeric(self.data[growth_30d_col], errors='coerce').fillna(0) > 1\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred1_manual_prev_g1')\n",
    "        \n",
    "        # pred2: Combined stock and S&P500 momentum\n",
    "        snp_col = self._find_growth_column(['growth_snp500_30d', 'growth_snp_30d'])\n",
    "        if growth_30d_col and snp_col:\n",
    "            stock_momentum = pd.to_numeric(self.data[growth_30d_col], errors='coerce').fillna(0) > 1\n",
    "            snp_momentum = pd.to_numeric(self.data[snp_col], errors='coerce').fillna(0) > 1\n",
    "            self.data['pred2_manual_prev_g1_and_snp'] = (stock_momentum & snp_momentum).astype(int)\n",
    "            self.prediction_cols.append('pred2_manual_prev_g1_and_snp')\n",
    "        \n",
    "        # pred3: Declining interest rates environment\n",
    "        dgs10_col = self._find_column(['dgs10_yoy'])\n",
    "        dgs5_col = self._find_column(['dgs5_yoy'])\n",
    "        if dgs10_col and dgs5_col:\n",
    "            dgs10_declining = pd.to_numeric(self.data[dgs10_col], errors='coerce').fillna(0) < 0\n",
    "            dgs5_declining = pd.to_numeric(self.data[dgs5_col], errors='coerce').fillna(0) < 0\n",
    "            self.data['pred3_manual_declining_rates'] = (dgs10_declining & dgs5_declining).astype(int)\n",
    "            self.prediction_cols.append('pred3_manual_declining_rates')\n",
    "        \n",
    "        # pred4: Federal Reserve easing cycle\n",
    "        fedfunds_col = self._find_column(['fedfunds_yoy'])\n",
    "        if fedfunds_col:\n",
    "            self.data['pred4_manual_fed_easing'] = (\n",
    "                pd.to_numeric(self.data[fedfunds_col], errors='coerce').fillna(0) < -0.1\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred4_manual_fed_easing')\n",
    "        \n",
    "        # pred5: VIX contrarian signal (fear spike = buy opportunity)\n",
    "        vix_col = self._find_growth_column(['growth_vix_30d'])\n",
    "        if vix_col:\n",
    "            self.data['pred5_manual_vix_contrarian'] = (\n",
    "                pd.to_numeric(self.data[vix_col], errors='coerce').fillna(0) > 0.2\n",
    "            ).astype(int)\n",
    "            self.prediction_cols.append('pred5_manual_vix_contrarian')\n",
    "        \n",
    "        # pred6: Combined stock and Bitcoin momentum\n",
    "        btc_col = self._find_growth_column(['growth_btc_30d'])\n",
    "        if growth_30d_col and btc_col:\n",
    "            stock_momentum = pd.to_numeric(self.data[growth_30d_col], errors='coerce').fillna(0) > 1.0\n",
    "            btc_momentum = pd.to_numeric(self.data[btc_col], errors='coerce').fillna(0) > 1.0\n",
    "            self.data['pred6_manual_stock_btc_momentum'] = (stock_momentum & btc_momentum).astype(int)\n",
    "            self.prediction_cols.append('pred6_manual_stock_btc_momentum')\n",
    "    \n",
    "    def _generate_ensemble_predictions(self):\n",
    "        \"\"\"Generate ensemble predictions combining manual and ML\"\"\"\n",
    "        print(\"  Generating ensemble predictions...\")\n",
    "        \n",
    "        manual_preds = [c for c in self.prediction_cols if 'manual' in c]\n",
    "        ml_mid = [c for c in self.prediction_cols if 'rf_thresh_50' in c]\n",
    "        ml_auto = [c for c in self.prediction_cols if 'rf_auto_rate_' in c]\n",
    "        topk_preds = [c for c in self.prediction_cols if 'top' in c and 'daily' in c]\n",
    "        \n",
    "        # Ensemble A: ML confidence + momentum confirmation\n",
    "        if ml_mid and 'pred1_manual_prev_g1' in manual_preds:\n",
    "            ml_signal = self.data[ml_mid[0]] == 1\n",
    "            momentum_signal = self.data['pred1_manual_prev_g1'] == 1\n",
    "            self.data['pred20_ens_ml50_and_momentum'] = (ml_signal & momentum_signal).astype(int)\n",
    "            self.prediction_cols.append('pred20_ens_ml50_and_momentum')\n",
    "        \n",
    "        # Ensemble B: High conviction ML OR daily top picks\n",
    "        auto_1p = [c for c in ml_auto if c.endswith('_1p')]\n",
    "        if auto_1p and topk_preds:\n",
    "            auto_signal = self.data[auto_1p[0]] == 1\n",
    "            top_signal = self.data[topk_preds[0]] == 1  # Use first top-K strategy\n",
    "            self.data['pred21_ens_auto1p_or_top3'] = (auto_signal | top_signal).astype(int)\n",
    "            self.prediction_cols.append('pred21_ens_auto1p_or_top3')\n",
    "        \n",
    "        # Ensemble C: Majority manual rules + ML confirmation\n",
    "        if len(manual_preds) >= 3:\n",
    "            manual_sum = self.data[manual_preds].sum(axis=1)\n",
    "            auto_3p = [c for c in ml_auto if c.endswith('_3p')]\n",
    "            if auto_3p:\n",
    "                manual_majority = manual_sum >= 2\n",
    "                ml_confirmation = self.data[auto_3p[0]] == 1\n",
    "                self.data['pred22_ens_manual2plus_and_auto3p'] = (manual_majority & ml_confirmation).astype(int)\n",
    "                self.prediction_cols.append('pred22_ens_manual2plus_and_auto3p')\n",
    "    \n",
    "    def _prepare_feature_matrix(self):\n",
    "        \"\"\"Prepare feature matrix for ML model\"\"\"\n",
    "        # Build feature matrix matching training features\n",
    "        missing_features = []\n",
    "        X_cols = []\n",
    "        \n",
    "        for col in self.feature_cols:\n",
    "            if col in self.data.columns:\n",
    "                X_cols.append(self.data[col])\n",
    "            else:\n",
    "                missing_features.append(col)\n",
    "                X_cols.append(pd.Series(0, index=self.data.index))\n",
    "        \n",
    "        if missing_features:\n",
    "            print(f\"    Warning: {len(missing_features)} features missing, filled with 0\")\n",
    "        \n",
    "        X = pd.DataFrame(dict(zip(self.feature_cols, X_cols)), index=self.data.index)\n",
    "        \n",
    "        # Clean data\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(X.median(numeric_only=True)).fillna(0)\n",
    "        X = X.astype(np.float32, copy=False)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _find_column(self, candidates: List[str]) -> Optional[str]:\n",
    "        \"\"\"Find first existing column from candidates\"\"\"\n",
    "        for col in candidates:\n",
    "            if col in self.data.columns:\n",
    "                return col\n",
    "        return None\n",
    "    \n",
    "    def _find_growth_column(self, candidates: List[str]) -> Optional[str]:\n",
    "        \"\"\"Find growth column, trying exact match first, then partial\"\"\"\n",
    "        # Try exact matches first\n",
    "        for col in candidates:\n",
    "            if col in self.data.columns:\n",
    "                return col\n",
    "        \n",
    "        # Try partial matches\n",
    "        for candidate in candidates:\n",
    "            for col in self.data.columns:\n",
    "                if candidate.replace('growth_', '') in col and 'growth' in col:\n",
    "                    return col\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Integration function\n",
    "def generate_complete_predictions(prediction_data, model, feature_cols):\n",
    "    \"\"\"\n",
    "    Main function to generate complete prediction set\n",
    "    \"\"\"\n",
    "    print(\"Generating complete prediction set...\")\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = CompletePredictionGenerator(prediction_data, model, feature_cols)\n",
    "    \n",
    "    # Generate all predictions\n",
    "    enhanced_data = generator.generate_all_predictions()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Total strategies generated: {len(generator.prediction_cols)}\")\n",
    "    \n",
    "    return enhanced_data, generator.prediction_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0ea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: GENERATE COMPLETE PREDICTIONS\n",
      "============================================================\n",
      "Defining date range: 2025-09-06 to 2025-09-12\n",
      "Generating complete prediction set...\n",
      "Generating complete prediction set...\n",
      "  Generating ML predictions...\n",
      "  Generating manual rule-based predictions...\n",
      "  Generating ensemble predictions...\n",
      "Generated 21 prediction strategies\n",
      "Total strategies generated: 21\n",
      "Enhanced prediction data ready: 250 stocks\n",
      "Available strategies: 21\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: GENERATE COMPLETE PREDICTIONS (ML + MANUAL + ENSEMBLE)  \n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: GENERATE COMPLETE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we have the date range defined\n",
    "if 'week_start' not in locals():\n",
    "    latest_date = tm.df_full['Date'].max()\n",
    "    week_start = latest_date - pd.Timedelta(days=6)  # Last 7 days\n",
    "    print(f\"Defining date range: {week_start.date()} to {latest_date.date()}\")\n",
    "\n",
    "try:\n",
    "    # Generate complete prediction set matching simulation analysis\n",
    "    enhanced_data, all_prediction_cols = generate_complete_predictions(\n",
    "        prediction_data=tm.df_full[tm.df_full['Date'] >= week_start],\n",
    "        model=model,\n",
    "        feature_cols=feature_cols\n",
    "    )\n",
    "    \n",
    "    # Update prediction data to use enhanced version\n",
    "    latest_date = enhanced_data['Date'].max()\n",
    "    latest_slice = enhanced_data[enhanced_data['Date'] == latest_date].copy()\n",
    "    \n",
    "    print(f\"Enhanced prediction data ready: {len(latest_slice)} stocks\")\n",
    "    print(f\"Available strategies: {len(all_prediction_cols)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Enhanced prediction generation failed: {e}\")\n",
    "    print(\"Using basic ML predictions only\")\n",
    "    \n",
    "    # Fallback - use existing data\n",
    "    enhanced_data = tm.df_full\n",
    "    latest_date = enhanced_data['Date'].max()\n",
    "    latest_slice = enhanced_data[enhanced_data['Date'] == latest_date].copy()\n",
    "    all_prediction_cols = ['rf_prob_30d']  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2d8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893c0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>growth_future_30d</th>\n",
       "      <th>is_positive_growth_30d_future</th>\n",
       "      <th>...</th>\n",
       "      <th>pred0_manual_cci</th>\n",
       "      <th>pred1_manual_prev_g1</th>\n",
       "      <th>pred2_manual_prev_g1_and_snp</th>\n",
       "      <th>pred3_manual_declining_rates</th>\n",
       "      <th>pred4_manual_fed_easing</th>\n",
       "      <th>pred5_manual_vix_contrarian</th>\n",
       "      <th>pred6_manual_stock_btc_momentum</th>\n",
       "      <th>pred20_ens_ml50_and_momentum</th>\n",
       "      <th>pred21_ens_auto1p_or_top3</th>\n",
       "      <th>pred22_ens_manual2plus_and_auto3p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2109520</th>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>152.970001</td>\n",
       "      <td>146.490005</td>\n",
       "      <td>151.750000</td>\n",
       "      <td>151.750000</td>\n",
       "      <td>3313000.0</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109521</th>\n",
       "      <td>2025-09-09</td>\n",
       "      <td>151.410004</td>\n",
       "      <td>152.119995</td>\n",
       "      <td>149.639999</td>\n",
       "      <td>149.789993</td>\n",
       "      <td>149.789993</td>\n",
       "      <td>1952600.0</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109522</th>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>150.580002</td>\n",
       "      <td>148.380005</td>\n",
       "      <td>149.419998</td>\n",
       "      <td>149.419998</td>\n",
       "      <td>2734300.0</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109523</th>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>149.779999</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>149.419998</td>\n",
       "      <td>150.610001</td>\n",
       "      <td>150.610001</td>\n",
       "      <td>3194000.0</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109524</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>149.649994</td>\n",
       "      <td>150.050003</td>\n",
       "      <td>146.539993</td>\n",
       "      <td>148.199997</td>\n",
       "      <td>148.199997</td>\n",
       "      <td>3111200.0</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date        Open        High         Low       Close  \\\n",
       "2109520 2025-09-08  152.500000  152.970001  146.490005  151.750000   \n",
       "2109521 2025-09-09  151.410004  152.119995  149.639999  149.789993   \n",
       "2109522 2025-09-10  149.000000  150.580002  148.380005  149.419998   \n",
       "2109523 2025-09-11  149.779999  152.000000  149.419998  150.610001   \n",
       "2109524 2025-09-12  149.649994  150.050003  146.539993  148.199997   \n",
       "\n",
       "          adj_close     Volume Ticker  growth_future_30d  \\\n",
       "2109520  151.750000  3313000.0    ZTS           0.005998   \n",
       "2109521  149.789993  1952600.0    ZTS           0.005972   \n",
       "2109522  149.419998  2734300.0    ZTS           0.005961   \n",
       "2109523  150.610001  3194000.0    ZTS           0.005836   \n",
       "2109524  148.199997  3111200.0    ZTS           0.005904   \n",
       "\n",
       "         is_positive_growth_30d_future  ...  pred0_manual_cci  \\\n",
       "2109520                              0  ...                 0   \n",
       "2109521                              0  ...                 0   \n",
       "2109522                              0  ...                 0   \n",
       "2109523                              0  ...                 0   \n",
       "2109524                              0  ...                 0   \n",
       "\n",
       "         pred1_manual_prev_g1  pred2_manual_prev_g1_and_snp  \\\n",
       "2109520                     0                             0   \n",
       "2109521                     0                             0   \n",
       "2109522                     0                             0   \n",
       "2109523                     1                             0   \n",
       "2109524                     1                             0   \n",
       "\n",
       "         pred3_manual_declining_rates  pred4_manual_fed_easing  \\\n",
       "2109520                             0                        1   \n",
       "2109521                             0                        1   \n",
       "2109522                             0                        1   \n",
       "2109523                             0                        1   \n",
       "2109524                             0                        1   \n",
       "\n",
       "         pred5_manual_vix_contrarian  pred6_manual_stock_btc_momentum  \\\n",
       "2109520                            0                                0   \n",
       "2109521                            0                                0   \n",
       "2109522                            0                                0   \n",
       "2109523                            0                                0   \n",
       "2109524                            0                                0   \n",
       "\n",
       "         pred20_ens_ml50_and_momentum  pred21_ens_auto1p_or_top3  \\\n",
       "2109520                             0                          0   \n",
       "2109521                             0                          0   \n",
       "2109522                             0                          0   \n",
       "2109523                             0                          0   \n",
       "2109524                             1                          0   \n",
       "\n",
       "         pred22_ens_manual2plus_and_auto3p  \n",
       "2109520                                  0  \n",
       "2109521                                  0  \n",
       "2109522                                  0  \n",
       "2109523                                  0  \n",
       "2109524                                  0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_choose =  ['Date',\n",
    " 'Open',\n",
    " 'High',\n",
    " 'Low',\n",
    " 'Close',\n",
    " 'adj_close',\n",
    " 'Volume',\n",
    " 'Ticker',\n",
    " 'growth_future_30d',\n",
    " 'is_positive_growth_30d_future',\n",
    "  'rf_prob_30d',\n",
    " 'pred10_rf_thresh_21',\n",
    " 'pred11_rf_thresh_50',\n",
    " 'pred12_rf_thresh_65',\n",
    " 'pred13_rf_thresh_80',\n",
    " 'pred14_rf_thresh_90',\n",
    " 'pred15_rf_auto_rate_1p',\n",
    " 'pred16_rf_auto_rate_3p',\n",
    " 'pred17_rf_auto_rate_5p',\n",
    " 'pred31_top3_daily',\n",
    " 'pred31_top5_daily',\n",
    " 'pred33_top10_daily',\n",
    " 'pred0_manual_cci',\n",
    " 'pred1_manual_prev_g1',\n",
    " 'pred2_manual_prev_g1_and_snp',\n",
    " 'pred3_manual_declining_rates',\n",
    " 'pred4_manual_fed_easing',\n",
    " 'pred5_manual_vix_contrarian',\n",
    " 'pred6_manual_stock_btc_momentum',\n",
    " 'pred20_ens_ml50_and_momentum',\n",
    " 'pred21_ens_auto1p_or_top3',\n",
    " 'pred22_ens_manual2plus_and_auto3p']\n",
    "\n",
    "\n",
    "new_df = enhanced_data[cols_to_choose]\n",
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa24d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1250 entries, 11268 to 2109524\n",
      "Data columns (total 32 columns):\n",
      " #   Column                             Non-Null Count  Dtype         \n",
      "---  ------                             --------------  -----         \n",
      " 0   Date                               1250 non-null   datetime64[ns]\n",
      " 1   Open                               1250 non-null   float64       \n",
      " 2   High                               1250 non-null   float64       \n",
      " 3   Low                                1250 non-null   float64       \n",
      " 4   Close                              1250 non-null   float64       \n",
      " 5   adj_close                          1250 non-null   float64       \n",
      " 6   Volume                             1250 non-null   float64       \n",
      " 7   Ticker                             1250 non-null   object        \n",
      " 8   growth_future_30d                  1250 non-null   float64       \n",
      " 9   is_positive_growth_30d_future      1250 non-null   int64         \n",
      " 10  rf_prob_30d                        1250 non-null   float64       \n",
      " 11  pred10_rf_thresh_21                1250 non-null   int64         \n",
      " 12  pred11_rf_thresh_50                1250 non-null   int64         \n",
      " 13  pred12_rf_thresh_65                1250 non-null   int64         \n",
      " 14  pred13_rf_thresh_80                1250 non-null   int64         \n",
      " 15  pred14_rf_thresh_90                1250 non-null   int64         \n",
      " 16  pred15_rf_auto_rate_1p             1250 non-null   int64         \n",
      " 17  pred16_rf_auto_rate_3p             1250 non-null   int64         \n",
      " 18  pred17_rf_auto_rate_5p             1250 non-null   int64         \n",
      " 19  pred31_top3_daily                  1250 non-null   int64         \n",
      " 20  pred31_top5_daily                  1250 non-null   int64         \n",
      " 21  pred33_top10_daily                 1250 non-null   int64         \n",
      " 22  pred0_manual_cci                   1250 non-null   int64         \n",
      " 23  pred1_manual_prev_g1               1250 non-null   int64         \n",
      " 24  pred2_manual_prev_g1_and_snp       1250 non-null   int64         \n",
      " 25  pred3_manual_declining_rates       1250 non-null   int64         \n",
      " 26  pred4_manual_fed_easing            1250 non-null   int64         \n",
      " 27  pred5_manual_vix_contrarian        1250 non-null   int64         \n",
      " 28  pred6_manual_stock_btc_momentum    1250 non-null   int64         \n",
      " 29  pred20_ens_ml50_and_momentum       1250 non-null   int64         \n",
      " 30  pred21_ens_auto1p_or_top3          1250 non-null   int64         \n",
      " 31  pred22_ens_manual2plus_and_auto3p  1250 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(8), int64(22), object(1)\n",
      "memory usage: 322.3+ KB\n"
     ]
    }
   ],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1) Define predictor buckets --------------------------------------------\n",
    "rf_thresh_cols = [\n",
    "    'pred10_rf_thresh_21','pred11_rf_thresh_50','pred12_rf_thresh_65',\n",
    "    'pred13_rf_thresh_80','pred14_rf_thresh_90',\n",
    "    'pred15_rf_auto_rate_1p','pred16_rf_auto_rate_3p','pred17_rf_auto_rate_5p'\n",
    "]\n",
    "\n",
    "topN_cols = ['pred31_top3_daily','pred31_top5_daily','pred33_top10_daily']\n",
    "\n",
    "manual_cols = [\n",
    "    'pred0_manual_cci','pred1_manual_prev_g1','pred2_manual_prev_g1_and_snp',\n",
    "    'pred3_manual_declining_rates','pred4_manual_fed_easing',\n",
    "    'pred5_manual_vix_contrarian','pred6_manual_stock_btc_momentum'\n",
    "]\n",
    "\n",
    "ensemble_cols = ['pred20_ens_ml50_and_momentum','pred21_ens_auto1p_or_top3','pred22_ens_manual2plus_and_auto3p']\n",
    "\n",
    "# Choose which sets to include in each “vote family”\n",
    "vote_sets = {\n",
    "    # everything\n",
    "    'all_pred': rf_thresh_cols + topN_cols + manual_cols + ensemble_cols,\n",
    "    # model-ish (rf thresholds + topN + ensemble)\n",
    "    'modelish': rf_thresh_cols + topN_cols + ensemble_cols,\n",
    "    # models only (no manual and no ensemble built from manual) – keep it pure if you want\n",
    "    'models_only': rf_thresh_cols + topN_cols,\n",
    "    # manual only\n",
    "    'manual_only': manual_cols,\n",
    "    # blended: rf med+high thresholds + ensemble + a couple robust manual rules\n",
    "    'blended': ['pred11_rf_thresh_50','pred12_rf_thresh_65','pred13_rf_thresh_80'] + ensemble_cols + [\n",
    "        'pred0_manual_cci','pred2_manual_prev_g1_and_snp'\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- 2) Utility: clean & clip to {0,1} --------------------------------------\n",
    "def _as_binary(df, cols):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    out = df[cols].copy()\n",
    "    # Try to coerce to numeric, then clip\n",
    "    for c in cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "    return out.fillna(0).clip(lower=0, upper=1)\n",
    "\n",
    "# --- 3) Core voting function -------------------------------------------------\n",
    "def build_voting_signals(\n",
    "    df: pd.DataFrame,\n",
    "    vote_sets: dict,\n",
    "    vote_threshold: float = 0.5,\n",
    "    min_votes: int = 3,\n",
    "    tie_breaker_col: str = 'rf_prob_30d',   # optional soft tiebreaker\n",
    "    tie_breaker_cut: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    For each key in vote_sets:\n",
    "      - computes vote_count, possible_count, vote_share\n",
    "      - produces signal_{name} ∈ {0,1}\n",
    "      - produces strength_{name} ∈ {'WEAK','MODERATE','STRONG'}\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for name, cols in vote_sets.items():\n",
    "        bin_df = _as_binary(out, cols)\n",
    "        vote_count = bin_df.sum(axis=1)\n",
    "        possible_count = bin_df.shape[1] if bin_df.shape[1] > 0 else 1\n",
    "        vote_share = (vote_count / possible_count)\n",
    "\n",
    "        # base decision\n",
    "        signal = (vote_share >= vote_threshold) & (vote_count >= min_votes)\n",
    "\n",
    "        # optional tiebreaker around the threshold edge\n",
    "        if tie_breaker_col in out.columns:\n",
    "            near_edge = (vote_share.between(vote_threshold - 0.05, vote_threshold + 0.05))\n",
    "            # if near edge, nudge using the probability column\n",
    "            signal = np.where(\n",
    "                near_edge & (out[tie_breaker_col] > tie_breaker_cut),\n",
    "                1,\n",
    "                signal.astype(int)\n",
    "            )\n",
    "\n",
    "        # strength buckets from vote_share (tweak as you like)\n",
    "        strength = pd.cut(\n",
    "            vote_share,\n",
    "            bins=[-np.inf, 0.33, 0.6, 1.01],  # looser\n",
    "            labels=['WEAK','MODERATE','STRONG']\n",
    "        )\n",
    "\n",
    "        out[f'vote_count_{name}'] = vote_count.astype(int)\n",
    "        out[f'vote_possible_{name}'] = possible_count\n",
    "        out[f'vote_share_{name}'] = vote_share\n",
    "        out[f'signal_{name}'] = signal.astype(int)\n",
    "        out[f'strength_{name}'] = strength.astype(str)\n",
    "\n",
    "    return out\n",
    "\n",
    "# # --- 4) Run it ----------------------------------------------------------------\n",
    "# voted_df = build_voting_signals(\n",
    "#     new_df,\n",
    "#     vote_sets=vote_sets,\n",
    "#     vote_threshold=0.55,   # a bit stricter than 0.5\n",
    "#     min_votes=3,           # avoid single-flag triggers\n",
    "#     tie_breaker_col='rf_prob_30d',\n",
    "#     tie_breaker_cut=0.55\n",
    "# )\n",
    "\n",
    "# voted_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c7ed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ranked_top3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mranked_top3\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvote_share_top3\u001b[39m\u001b[33m\"\u001b[39m].describe())\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(ranked_top3[\u001b[33m\"\u001b[39m\u001b[33mvote_count_top3\u001b[39m\u001b[33m\"\u001b[39m].value_counts())\n",
      "\u001b[31mNameError\u001b[39m: name 'ranked_top3' is not defined"
     ]
    }
   ],
   "source": [
    "print(ranked_top3[\"vote_share_top3\"].describe())\n",
    "print(ranked_top3[\"vote_count_top3\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest date: 2025-09-12 | rows: 250 | unique tickers: 250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Close</th>\n",
       "      <th>rf_prob_30d</th>\n",
       "      <th>vote_count_top3</th>\n",
       "      <th>vote_share_top3</th>\n",
       "      <th>signal_top3</th>\n",
       "      <th>strength_top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>KVUE</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>0.522442</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>AMT</td>\n",
       "      <td>195.080002</td>\n",
       "      <td>0.520626</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>KDP</td>\n",
       "      <td>27.230000</td>\n",
       "      <td>0.520329</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CNC</td>\n",
       "      <td>33.259998</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>BMY</td>\n",
       "      <td>46.200001</td>\n",
       "      <td>0.520231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>LULU</td>\n",
       "      <td>159.869995</td>\n",
       "      <td>0.519825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>INTC</td>\n",
       "      <td>24.080000</td>\n",
       "      <td>0.519625</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>PFE</td>\n",
       "      <td>23.870001</td>\n",
       "      <td>0.519591</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CCI</td>\n",
       "      <td>95.389999</td>\n",
       "      <td>0.519373</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CARR</td>\n",
       "      <td>61.669998</td>\n",
       "      <td>0.519225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>TTD</td>\n",
       "      <td>45.150002</td>\n",
       "      <td>0.519190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>TDG</td>\n",
       "      <td>1271.449951</td>\n",
       "      <td>0.519168</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CMG</td>\n",
       "      <td>38.630001</td>\n",
       "      <td>0.519090</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>FTNT</td>\n",
       "      <td>79.680000</td>\n",
       "      <td>0.518975</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>ON</td>\n",
       "      <td>48.259998</td>\n",
       "      <td>0.518724</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>TEAM</td>\n",
       "      <td>174.869995</td>\n",
       "      <td>0.518674</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>148.199997</td>\n",
       "      <td>0.518282</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>DELL</td>\n",
       "      <td>125.040001</td>\n",
       "      <td>0.518252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>COP</td>\n",
       "      <td>92.430000</td>\n",
       "      <td>0.518250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>DOW</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>0.518157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Ticker        Close  rf_prob_30d  vote_count_top3  \\\n",
       "0  2025-09-12   KVUE    18.990000     0.522442                0   \n",
       "1  2025-09-12    AMT   195.080002     0.520626                0   \n",
       "2  2025-09-12    KDP    27.230000     0.520329                0   \n",
       "3  2025-09-12    CNC    33.259998     0.520249                0   \n",
       "4  2025-09-12    BMY    46.200001     0.520231                0   \n",
       "5  2025-09-12   LULU   159.869995     0.519825                0   \n",
       "6  2025-09-12   INTC    24.080000     0.519625                0   \n",
       "7  2025-09-12    PFE    23.870001     0.519591                0   \n",
       "8  2025-09-12    CCI    95.389999     0.519373                0   \n",
       "9  2025-09-12   CARR    61.669998     0.519225                0   \n",
       "10 2025-09-12    TTD    45.150002     0.519190                0   \n",
       "11 2025-09-12    TDG  1271.449951     0.519168                0   \n",
       "12 2025-09-12    CMG    38.630001     0.519090                0   \n",
       "13 2025-09-12   FTNT    79.680000     0.518975                0   \n",
       "14 2025-09-12     ON    48.259998     0.518724                0   \n",
       "15 2025-09-12   TEAM   174.869995     0.518674                0   \n",
       "16 2025-09-12    ZTS   148.199997     0.518282                0   \n",
       "17 2025-09-12   DELL   125.040001     0.518252                0   \n",
       "18 2025-09-12    COP    92.430000     0.518250                0   \n",
       "19 2025-09-12    DOW    25.040001     0.518157                0   \n",
       "\n",
       "    vote_share_top3  signal_top3 strength_top3  \n",
       "0               0.0            0          WEAK  \n",
       "1               0.0            0          WEAK  \n",
       "2               0.0            0          WEAK  \n",
       "3               0.0            0          WEAK  \n",
       "4               0.0            0          WEAK  \n",
       "5               0.0            0          WEAK  \n",
       "6               0.0            0          WEAK  \n",
       "7               0.0            0          WEAK  \n",
       "8               0.0            0          WEAK  \n",
       "9               0.0            0          WEAK  \n",
       "10              0.0            0          WEAK  \n",
       "11              0.0            0          WEAK  \n",
       "12              0.0            0          WEAK  \n",
       "13              0.0            0          WEAK  \n",
       "14              0.0            0          WEAK  \n",
       "15              0.0            0          WEAK  \n",
       "16              0.0            0          WEAK  \n",
       "17              0.0            0          WEAK  \n",
       "18              0.0            0          WEAK  \n",
       "19              0.0            0          WEAK  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: define your top-3 strategies\n",
    "top3_cols = ['pred15_rf_auto_rate_3p','pred13_rf_thresh_80','pred15_rf_auto_rate_5p']\n",
    "vote_sets_top3 = {\"top3\": top3_cols}\n",
    "\n",
    "\n",
    "# Make sure Date column is datetime\n",
    "new_df[\"Date\"] = pd.to_datetime(new_df[\"Date\"])\n",
    "\n",
    "# Find the most recent date in your dataset\n",
    "latest_date = new_df[\"Date\"].max()\n",
    "\n",
    "# Filter to that day only (≈ your daily “stock universe” snapshot)\n",
    "latest_df = new_df.loc[new_df[\"Date\"] == latest_date].copy()\n",
    "\n",
    "print(\"Latest date:\", latest_date.date(),\n",
    "      \"| rows:\", len(latest_df),\n",
    "      \"| unique tickers:\", latest_df[\"Ticker\"].nunique())\n",
    "\n",
    "\n",
    "# Step 2: Build signals on the latest snapshot\n",
    "latest_voted_top3 = build_voting_signals(\n",
    "    latest_df,             # latest date df from earlier\n",
    "    vote_sets=vote_sets_top3,\n",
    "    vote_threshold=0.67,   # require 2 out of 3\n",
    "    min_votes=2,           # at least 2 predictors must agree\n",
    "    tie_breaker_col='rf_prob_30d',\n",
    "    tie_breaker_cut=0.55\n",
    ")\n",
    "\n",
    "# Step 3: Rank tickers by signal & conviction\n",
    "ranked_top3 = (\n",
    "    latest_voted_top3\n",
    "      .assign(\n",
    "          _sig = latest_voted_top3[\"signal_top3\"].values,\n",
    "          _strength_ord = latest_voted_top3[\"strength_top3\"].map({\"STRONG\":2,\"MODERATE\":1,\"WEAK\":0}).fillna(0).values,\n",
    "          _share = latest_voted_top3[\"vote_share_top3\"].values,\n",
    "          _prob = latest_voted_top3[\"rf_prob_30d\"].values\n",
    "      )\n",
    "      .sort_values(by=[\"_sig\",\"_strength_ord\",\"_share\",\"_prob\"], ascending=[False,False,False,False])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Step 4: Take top N picks (say 20)\n",
    "TOP_N = 20\n",
    "top_picks_top3 = ranked_top3.head(TOP_N)\n",
    "\n",
    "# Final table\n",
    "top_picks_top3[[\"Date\",\"Ticker\",\"Close\",\"rf_prob_30d\",\n",
    "                \"vote_count_top3\",\"vote_share_top3\",\n",
    "                \"signal_top3\",\"strength_top3\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_latest_predictions_simple(latest_df: pd.DataFrame, vote_family: str, out_dir: str = \"pred_logs\"):\n",
    "    \"\"\"\n",
    "    Saves a minimal snapshot (Date, Ticker, Close, probability, signal) for the latest day.\n",
    "    Assumes latest_df already contains signal columns for the chosen vote_family.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    fam = vote_family\n",
    "    cols = [c for c in [\n",
    "        \"Date\",\"Ticker\",\"Close\",\"rf_prob_30d\",\n",
    "        f\"signal_{fam}\", f\"vote_share_{fam}\", f\"strength_{fam}\"\n",
    "    ] if c in latest_df.columns]\n",
    "\n",
    "    snap = latest_df[cols].copy().reset_index(drop=True)\n",
    "\n",
    "    asof = pd.to_datetime(snap[\"Date\"].max()).strftime(\"%Y-%m-%d\")\n",
    "    csv_path = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.csv\")\n",
    "    pq_path  = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.parquet\")\n",
    "\n",
    "    snap.to_csv(csv_path, index=False)\n",
    "    try:\n",
    "        snap.to_parquet(pq_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"Parquet save skipped:\", e)\n",
    "\n",
    "    print(f\"Saved:\\n  {csv_path}\\n  {pq_path if os.path.exists(pq_path) else '(parquet skipped)'}\")\n",
    "    return snap\n",
    "\n",
    "# Example:\n",
    "# save_latest_predictions_simple(latest_voted_top3, vote_family=\"top3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def save_latest_predictions_simple_rank(\n",
    "    df: pd.DataFrame,\n",
    "    vote_sets: dict,\n",
    "    vote_family: str,\n",
    "    vote_threshold: float = 0.6,\n",
    "    min_votes: int = 2,\n",
    "    tie_breaker_col: str = \"rf_prob_30d\",\n",
    "    tie_breaker_cut: float = 0.55,\n",
    "    out_dir: str = \"../new_pred_logs\"\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # --- 1) get latest date ---\n",
    "    tmp = df.copy()\n",
    "    tmp[\"Date\"] = pd.to_datetime(tmp[\"Date\"])\n",
    "    latest_date = tmp[\"Date\"].max()\n",
    "    latest_df = tmp.loc[tmp[\"Date\"] == latest_date].copy()\n",
    "\n",
    "    # --- 2) build signals ---\n",
    "    voted = build_voting_signals(\n",
    "        latest_df,\n",
    "        vote_sets=vote_sets,\n",
    "        vote_threshold=vote_threshold,\n",
    "        min_votes=min_votes,\n",
    "        tie_breaker_col=tie_breaker_col,\n",
    "        tie_breaker_cut=tie_breaker_cut\n",
    "    )\n",
    "\n",
    "    fam = vote_family\n",
    "    needed = [\"Date\",\"Ticker\",\"Close\",\"rf_prob_30d\",\n",
    "              f\"signal_{fam}\", f\"vote_share_{fam}\", f\"strength_{fam}\"]\n",
    "    snap = voted[[c for c in needed if c in voted.columns]].copy().reset_index(drop=True)\n",
    "\n",
    "    # --- 3) map strength to numeric order for sorting ---\n",
    "    strength_map = {\"STRONG\": 2, \"MODERATE\": 1, \"WEAK\": 0}\n",
    "    snap[\"_strength_ord\"] = snap[f\"strength_{fam}\"].map(strength_map).fillna(0).astype(int)\n",
    "\n",
    "    # --- 4) sort & rank ---\n",
    "    snap_sorted = snap.sort_values(\n",
    "        by=[\"_strength_ord\",\"rf_prob_30d\"], \n",
    "        ascending=[False, False]\n",
    "    ).reset_index(drop=True)\n",
    "    snap_sorted[\"rank_today\"] = np.arange(1, len(snap_sorted)+1)\n",
    "\n",
    "    # --- 5) reorder nicely ---\n",
    "    ordered_cols = [\"rank_today\",\"Date\",\"Ticker\",\"Close\",\"rf_prob_30d\",\n",
    "                    f\"signal_{fam}\", f\"vote_share_{fam}\", f\"strength_{fam}\"]\n",
    "    snap_sorted = snap_sorted[ordered_cols]\n",
    "\n",
    "    # --- 6) save ---\n",
    "    asof = pd.to_datetime(latest_date).strftime(\"%Y-%m-%d\")\n",
    "    csv_path = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.csv\")\n",
    "    pq_path  = os.path.join(out_dir, f\"predictions_{fam}_asof_{asof}.parquet\")\n",
    "\n",
    "    snap_sorted.to_csv(csv_path, index=False)\n",
    "    try:\n",
    "        snap_sorted.to_parquet(pq_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"Parquet save skipped:\", e)\n",
    "\n",
    "    print(f\"Saved predictions to:\\n  {csv_path}\\n  {pq_path if os.path.exists(pq_path) else '(parquet skipped)'}\")\n",
    "    return snap_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to:\n",
      "  ../new_pred_logs/predictions_top3_asof_2025-09-12.csv\n",
      "  ../new_pred_logs/predictions_top3_asof_2025-09-12.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_today</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Close</th>\n",
       "      <th>rf_prob_30d</th>\n",
       "      <th>signal_top3</th>\n",
       "      <th>vote_share_top3</th>\n",
       "      <th>strength_top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>KVUE</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>0.522442</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>AMT</td>\n",
       "      <td>195.080002</td>\n",
       "      <td>0.520626</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>KDP</td>\n",
       "      <td>27.230000</td>\n",
       "      <td>0.520329</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CNC</td>\n",
       "      <td>33.259998</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>BMY</td>\n",
       "      <td>46.200001</td>\n",
       "      <td>0.520231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>LULU</td>\n",
       "      <td>159.869995</td>\n",
       "      <td>0.519825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>INTC</td>\n",
       "      <td>24.080000</td>\n",
       "      <td>0.519625</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>PFE</td>\n",
       "      <td>23.870001</td>\n",
       "      <td>0.519591</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CCI</td>\n",
       "      <td>95.389999</td>\n",
       "      <td>0.519373</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CARR</td>\n",
       "      <td>61.669998</td>\n",
       "      <td>0.519225</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_today       Date Ticker       Close  rf_prob_30d  signal_top3  \\\n",
       "0           1 2025-09-12   KVUE   18.990000     0.522442            0   \n",
       "1           2 2025-09-12    AMT  195.080002     0.520626            0   \n",
       "2           3 2025-09-12    KDP   27.230000     0.520329            0   \n",
       "3           4 2025-09-12    CNC   33.259998     0.520249            0   \n",
       "4           5 2025-09-12    BMY   46.200001     0.520231            0   \n",
       "5           6 2025-09-12   LULU  159.869995     0.519825            0   \n",
       "6           7 2025-09-12   INTC   24.080000     0.519625            0   \n",
       "7           8 2025-09-12    PFE   23.870001     0.519591            0   \n",
       "8           9 2025-09-12    CCI   95.389999     0.519373            0   \n",
       "9          10 2025-09-12   CARR   61.669998     0.519225            0   \n",
       "\n",
       "   vote_share_top3 strength_top3  \n",
       "0              0.0          WEAK  \n",
       "1              0.0          WEAK  \n",
       "2              0.0          WEAK  \n",
       "3              0.0          WEAK  \n",
       "4              0.0          WEAK  \n",
       "5              0.0          WEAK  \n",
       "6              0.0          WEAK  \n",
       "7              0.0          WEAK  \n",
       "8              0.0          WEAK  \n",
       "9              0.0          WEAK  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_sets_top3 = {\"top3\": ['pred16_rf_auto_rate_3p','pred13_rf_thresh_80','pred17_rf_auto_rate_5p']}\n",
    "\n",
    "today_ranked = save_latest_predictions_simple_rank(\n",
    "    new_df,\n",
    "    vote_sets=vote_sets_top3,\n",
    "    vote_family=\"top3\",\n",
    "    vote_threshold=0.67,\n",
    "    min_votes=2,\n",
    "    out_dir=\"../new_pred_logs\"\n",
    ")\n",
    "\n",
    "today_ranked.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f638b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_today</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Close</th>\n",
       "      <th>rf_prob_30d</th>\n",
       "      <th>signal_top3</th>\n",
       "      <th>vote_share_top3</th>\n",
       "      <th>strength_top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>KVUE</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>0.522442</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>AMT</td>\n",
       "      <td>195.080002</td>\n",
       "      <td>0.520626</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>KDP</td>\n",
       "      <td>27.230000</td>\n",
       "      <td>0.520329</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>CNC</td>\n",
       "      <td>33.259998</td>\n",
       "      <td>0.520249</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>BMY</td>\n",
       "      <td>46.200001</td>\n",
       "      <td>0.520231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>246</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>395.940002</td>\n",
       "      <td>0.509169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>247</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>BA</td>\n",
       "      <td>215.940002</td>\n",
       "      <td>0.509158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>248</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>HCA</td>\n",
       "      <td>405.130005</td>\n",
       "      <td>0.508725</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>249</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>AVGO</td>\n",
       "      <td>359.869995</td>\n",
       "      <td>0.508489</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>250</td>\n",
       "      <td>2025-09-12</td>\n",
       "      <td>META</td>\n",
       "      <td>755.590027</td>\n",
       "      <td>0.508222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WEAK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank_today       Date Ticker       Close  rf_prob_30d  signal_top3  \\\n",
       "0             1 2025-09-12   KVUE   18.990000     0.522442            0   \n",
       "1             2 2025-09-12    AMT  195.080002     0.520626            0   \n",
       "2             3 2025-09-12    KDP   27.230000     0.520329            0   \n",
       "3             4 2025-09-12    CNC   33.259998     0.520249            0   \n",
       "4             5 2025-09-12    BMY   46.200001     0.520231            0   \n",
       "..          ...        ...    ...         ...          ...          ...   \n",
       "245         246 2025-09-12   TSLA  395.940002     0.509169            0   \n",
       "246         247 2025-09-12     BA  215.940002     0.509158            0   \n",
       "247         248 2025-09-12    HCA  405.130005     0.508725            0   \n",
       "248         249 2025-09-12   AVGO  359.869995     0.508489            0   \n",
       "249         250 2025-09-12   META  755.590027     0.508222            0   \n",
       "\n",
       "     vote_share_top3 strength_top3  \n",
       "0                0.0          WEAK  \n",
       "1                0.0          WEAK  \n",
       "2                0.0          WEAK  \n",
       "3                0.0          WEAK  \n",
       "4                0.0          WEAK  \n",
       "..               ...           ...  \n",
       "245              0.0          WEAK  \n",
       "246              0.0          WEAK  \n",
       "247              0.0          WEAK  \n",
       "248              0.0          WEAK  \n",
       "249              0.0          WEAK  \n",
       "\n",
       "[250 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db09d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'today_ranked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtoday_ranked\u001b[49m.rf_prob_30d.hist()\n",
      "\u001b[31mNameError\u001b[39m: name 'today_ranked' is not defined"
     ]
    }
   ],
   "source": [
    "today_ranked.rf_prob_30d.hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
